{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Semester Project : Implementation of a privacy preserving Spiking Neural Network through MPC\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "Yavuz AKIN, Nicolas SERVOT\n",
        "\n",
        "last modification : 15 February \n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "Semester project at EURECOM (September 2021 -\n",
        "February 2022)\n",
        "\n",
        "Supervisors: Massimiliano TODISCO, Melek ÖNEN,\n",
        "Oubaida CHOUCHA\n",
        "\n",
        "Tasks accomplished:\n",
        "Building SNN for Classification on MNIST\n",
        "\n",
        "*   Building SNN for Classification on MNIST\n",
        "*   Privacy-preserving testing using MPC (with Crypten)\n",
        "\n",
        "INFO : To run the the Crypten testing part on GPU (with world_size=2 go check the modified code on the GitLab page, It may not work with this configuration on Colab)\n"
      ],
      "metadata": {
        "id": "DtxxgrMhjiAt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading libraries and initializing global variables"
      ],
      "metadata": {
        "id": "MsPrkBZ_k-st"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dMcEQ3NF08qN"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.gridspec import GridSpec\n",
        "import seaborn as sns\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6tzKRCt21DL0"
      },
      "outputs": [],
      "source": [
        "# The coarse network structure is dicated by the Fashion MNIST dataset. \n",
        "nb_inputs  = 28*28\n",
        "nb_hidden  = 100\n",
        "nb_outputs = 10\n",
        "\n",
        "time_step = 1e-3\n",
        "nb_steps  = 100\n",
        "\n",
        "batch_size = 256"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s6XA8DgK1GOe",
        "outputId": "61592173-13e6-4464-a4f8-c7f6e090c5b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "dtype = torch.float\n",
        "\n",
        "# Check whether a GPU is available\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(\"cuda\")     \n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset Construction"
      ],
      "metadata": {
        "id": "cxWQflxTlHfd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zL8FHp-C1JJY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451,
          "referenced_widgets": [
            "c1af491a50af444e990136a7d433e299",
            "ef169c8205ca43f7a980740aaeaec91f",
            "88f360063c054ce0a18b52756a7f043f",
            "50a306e6c6f54ad6b3f84720c87b4acb",
            "0938757ea56245578e57596f302c987c",
            "f8749b243d1444098b07a3808077d330",
            "310506f8301445f5a210b0f424c6edf5",
            "b535410024c44ce2b8a2ece3d438ce1a",
            "1efe7c0083c8404fae7e8cfa7c2bc91a",
            "77152e81d840428fbced1f06201e31d3",
            "ab84025db8dc4a3782ef17d08837db69",
            "8ec76c02c4c541ad926284b242cb1f53",
            "c9abdb45182a4e4b80863063184d8af8",
            "0b527fd41a8c4616abc45b7c74e8f254",
            "78000ba7458d48feb0ae10c14538c254",
            "d2df82e0e9214eb8b9c04638c4646056",
            "0199c48fa1664b0cb6fa2b26f98b69a0",
            "f2e1bf8a83e44c8d99c3735352b8fbb2",
            "4e778bcfa5864afea26d47d9ca5bf04e",
            "945ce3154bd34af9960dd45497a24fbf",
            "34fc6d19dc074279999661c6b4591550",
            "97c352b2fc9e4c639b8d78ffe9562be9",
            "451bfe0cecaa4a6e8ad23703c822da1e",
            "fc4a095441244221abed03f62caae2e9",
            "1916c590ac4b47a6919da7f1a7ccfafe",
            "ef849420fd3f41dbaf590b3b896972a6",
            "eb20a00931ac44c09cfd8c5d397acc98",
            "32d71e3b76d549acbcb133052e3b6be6",
            "c70689e31d2f46b5a919593f40a241d4",
            "8eac527f5aa046cda47a015defbe013f",
            "53fe5ba4dcaf4e049bacb00c45d81755",
            "06c88ed49f7c4b93a2cdde8276b6bfb5",
            "d56c6b41342443f0afc7b51958da6517",
            "d0d3323e671b476ea508b4d222e01525",
            "050ce66cf5a04cf19fbbd36a1896ef60",
            "b0aa144ec4b34c6782fa0b6e1899a3a9",
            "4f3385bf7971455a8ac06da2fc2b34b2",
            "b868784422434bdda11cff15f6e8488f",
            "d5640285a5c94de8ad2939efa862b194",
            "17a69ec2609f467996c2ed322fda5f3e",
            "2782948a188e47d9a514ec8b543c6913",
            "f577aa0130bb476d85933223c36c40ff",
            "8d85e4992a9c4e218f481b93555fe125",
            "6c2fe6e05c1c4433a3c7b7904e18606f"
          ]
        },
        "outputId": "258ccc79-3674-4f62-899a-72220470532f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to /root/data/datasets/torch/fashion-mnist/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c1af491a50af444e990136a7d433e299",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/26421880 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /root/data/datasets/torch/fashion-mnist/FashionMNIST/raw/train-images-idx3-ubyte.gz to /root/data/datasets/torch/fashion-mnist/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to /root/data/datasets/torch/fashion-mnist/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8ec76c02c4c541ad926284b242cb1f53",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/29515 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /root/data/datasets/torch/fashion-mnist/FashionMNIST/raw/train-labels-idx1-ubyte.gz to /root/data/datasets/torch/fashion-mnist/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to /root/data/datasets/torch/fashion-mnist/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "451bfe0cecaa4a6e8ad23703c822da1e",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/4422102 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /root/data/datasets/torch/fashion-mnist/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to /root/data/datasets/torch/fashion-mnist/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to /root/data/datasets/torch/fashion-mnist/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d0d3323e671b476ea508b4d222e01525",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/5148 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /root/data/datasets/torch/fashion-mnist/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to /root/data/datasets/torch/fashion-mnist/FashionMNIST/raw\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Here we load the Dataset\n",
        "root = os.path.expanduser(\"~/data/datasets/torch/fashion-mnist\")\n",
        "train_dataset = torchvision.datasets.FashionMNIST(root, train=True, transform=None, target_transform=None, download=True)\n",
        "test_dataset = torchvision.datasets.FashionMNIST(root, train=False, transform=None, target_transform=None, download=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vfn9Tmw61L59",
        "outputId": "0540f1ee-c015-41c7-e47a-9e47292c34c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  \n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:12: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  if sys.path[0] == '':\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000,)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "# Standardize data\n",
        "#x_train = torch.tensor(train_dataset.train_data, device=device, dtype=dtype)\n",
        "x_train = np.array(train_dataset.data, dtype=np.float)\n",
        "x_train = x_train.reshape(x_train.shape[0],-1)/255\n",
        "#x_test = torch.tensor(test_dataset.test_data, device=device, dtype=dtype)\n",
        "x_test = np.array(test_dataset.data, dtype=np.float)\n",
        "x_test = x_test.reshape(x_test.shape[0],-1)/255\n",
        "\n",
        "#y_train = torch.tensor(train_dataset.train_labels, device=device, dtype=dtype)\n",
        "#y_test  = torch.tensor(test_dataset.test_labels, device=device, dtype=dtype)\n",
        "y_train = np.array(train_dataset.targets, dtype=np.int)\n",
        "y_test  = np.array(test_dataset.targets, dtype=np.int)\n",
        "\n",
        "y_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "id": "M-cJMgtZ1TeW",
        "outputId": "c079a9a4-baf5-494a-f3bf-1c49e97afc58"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-0.5, 27.5, 27.5, -0.5)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# Here we plot one of the raw data points as an example\n",
        "data_id = 1\n",
        "plt.imshow(x_train[data_id].reshape(28,28), cmap=plt.cm.gray_r)\n",
        "plt.axis(\"off\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqy5M9qO5Ydf"
      },
      "source": [
        "REshaping the datatsets => shape of train is (60000,784),ie flatening the images matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "55v4daf51ced"
      },
      "outputs": [],
      "source": [
        "def current2firing_time(x, tau=20, thr=0.2, tmax=1.0, epsilon=1e-7):\n",
        "    \"\"\" Computes first firing time latency for a current input x assuming the charge time of a current based LIF neuron.\n",
        "\n",
        "    Args:\n",
        "    x -- The \"current\" values\n",
        "\n",
        "    Keyword args:\n",
        "    tau -- The membrane time constant of the LIF neuron to be charged\n",
        "    thr -- The firing threshold value \n",
        "    tmax -- The maximum time returned \n",
        "    epsilon -- A generic (small) epsilon > 0\n",
        "\n",
        "    Returns:\n",
        "    Time to first spike for each \"current\" x\n",
        "    \"\"\"\n",
        "    idx = x<thr\n",
        "    x = np.clip(x,thr+epsilon,1e9)\n",
        "    T = tau*np.log(x/(x-thr))\n",
        "    # T in [1.5,109]\n",
        "    T[idx] = tmax\n",
        "\n",
        "    return T\n",
        " \n",
        "\n",
        "def sparse_data_generator(X, y, batch_size, nb_steps, nb_units, shuffle=True ):\n",
        "    \"\"\" This generator takes datasets in analog format and generates spiking network input as sparse tensors. \n",
        "\n",
        "    Args:\n",
        "        X: The data ( sample x event x 2 ) the last dim holds (time,neuron) tuples\n",
        "        y: The labels\n",
        "        nb_steps: number of time steps\n",
        "        nb_units: 784 (28x28)\n",
        "    \"\"\"\n",
        "\n",
        "    labels_ = np.array(y,dtype=np.int)\n",
        "    number_of_batches = len(X)//batch_size\n",
        "    sample_index = np.arange(len(X))\n",
        "\n",
        "    # compute discrete firing times\n",
        "    tau_eff = 20e-3/time_step\n",
        "    firing_times = np.array(current2firing_time(X, tau=tau_eff, tmax=nb_steps), dtype=np.int)\n",
        "    # [0...783]\n",
        "    unit_numbers = np.arange(nb_units)\n",
        "\n",
        "    if shuffle:\n",
        "        np.random.shuffle(sample_index)\n",
        "\n",
        "    total_batch_count = 0\n",
        "    counter = 0\n",
        "    while counter<number_of_batches:\n",
        "        # index of the entry in the current batch\n",
        "        batch_index = sample_index[batch_size*counter:batch_size*(counter+1)]\n",
        "\n",
        "        coo = [ [] for i in range(3) ]\n",
        "        # batch index, absolute index in X\n",
        "        for bc,idx in enumerate(batch_index):\n",
        "            c = firing_times[idx]<nb_steps\n",
        "            # keeps the entries such that time<100 \n",
        "            times, units = firing_times[idx][c], unit_numbers[c]\n",
        "\n",
        "            batch = [bc for _ in range(len(times))]\n",
        "            coo[0].extend(batch)\n",
        "            coo[1].extend(times)\n",
        "            coo[2].extend(units)\n",
        "\n",
        "        i = torch.LongTensor(coo).to(device)\n",
        "        v = torch.FloatTensor(np.ones(len(coo[0]))).to(device)\n",
        "    \n",
        "        X_batch = torch.sparse.FloatTensor(i, v, torch.Size([batch_size,nb_steps,nb_units])).to(device)\n",
        "        y_batch = torch.tensor(labels_[batch_index],device=device)\n",
        "\n",
        "        yield X_batch.to(device=device), y_batch.to(device=device)\n",
        "\n",
        "        counter += 1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "T=current2firing_time(x_test)\n",
        "df = pd.DataFrame(T)\n",
        "df.describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        },
        "id": "48SXsiiv9WmR",
        "outputId": "a629275a-fb80-4aa1-a982-a2a9eda05d8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-4d947ad0-78d5-4cdd-b98a-35768bbd011d\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>744</th>\n",
              "      <th>745</th>\n",
              "      <th>746</th>\n",
              "      <th>747</th>\n",
              "      <th>748</th>\n",
              "      <th>749</th>\n",
              "      <th>750</th>\n",
              "      <th>751</th>\n",
              "      <th>752</th>\n",
              "      <th>753</th>\n",
              "      <th>754</th>\n",
              "      <th>755</th>\n",
              "      <th>756</th>\n",
              "      <th>757</th>\n",
              "      <th>758</th>\n",
              "      <th>759</th>\n",
              "      <th>760</th>\n",
              "      <th>761</th>\n",
              "      <th>762</th>\n",
              "      <th>763</th>\n",
              "      <th>764</th>\n",
              "      <th>765</th>\n",
              "      <th>766</th>\n",
              "      <th>767</th>\n",
              "      <th>768</th>\n",
              "      <th>769</th>\n",
              "      <th>770</th>\n",
              "      <th>771</th>\n",
              "      <th>772</th>\n",
              "      <th>773</th>\n",
              "      <th>774</th>\n",
              "      <th>775</th>\n",
              "      <th>776</th>\n",
              "      <th>777</th>\n",
              "      <th>778</th>\n",
              "      <th>779</th>\n",
              "      <th>780</th>\n",
              "      <th>781</th>\n",
              "      <th>782</th>\n",
              "      <th>783</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>10000.0</td>\n",
              "      <td>10000.0</td>\n",
              "      <td>10000.000000</td>\n",
              "      <td>10000.000000</td>\n",
              "      <td>10000.000000</td>\n",
              "      <td>10000.000000</td>\n",
              "      <td>10000.000000</td>\n",
              "      <td>10000.000000</td>\n",
              "      <td>10000.000000</td>\n",
              "      <td>10000.000000</td>\n",
              "      <td>10000.000000</td>\n",
              "      <td>10000.000000</td>\n",
              "      <td>10000.000000</td>\n",
              "      <td>10000.000000</td>\n",
              "      <td>10000.000000</td>\n",
              "      <td>10000.000000</td>\n",
              "      <td>10000.000000</td>\n",
              "      <td>10000.000000</td>\n",
              "      <td>10000.000000</td>\n",
              "      <td>10000.000000</td>\n",
              "      <td>10000.000000</td>\n",
              "      <td>10000.000000</td>\n",
              "      <td>10000.000000</td>\n",
              "      <td>10000.000000</td>\n",
              "      <td>10000.000000</td>\n",
              "      <td>10000.000000</td>\n",
              "      <td>10000.000000</td>\n",
              "      <td>10000.000000</td>\n",
              "      <td>10000.0</td>\n",
              "      <td>10000.000000</td>\n",
              "      <td>10000.000000</td>\n",
              "      <td>10000.000000</td>\n",
              "      <td>10000.000000</td>\n",
              "      <td>10000.000000</td>\n",
              "      <td>10000.000000</td>\n",
              "      <td>10000.000000</td>\n",
              "      <td>10000.000000</td>\n",
              "      <td>10000.000000</td>\n",
              "      <td>10000.000000</td>\n",
              "      <td>10000.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>10000.000000</td>\n",
              "      <td>10000.000000</td>\n",
              "      <td>10000.000000</td>\n",
              "      <td>10000.000000</td>\n",
              "      <td>10000.000000</td>\n",
              "      <td>10000.000000</td>\n",
              "      <td>10000.000000</td>\n",
              "      <td>10000.000000</td>\n",
              "      <td>10000.000000</td>\n",
              "      <td>10000.000000</td>\n",
              "      <td>10000.000000</td>\n",
              "      <td>10000.000000</td>\n",
              "      <td>10000.000000</td>\n",
              "      <td>10000.000000</td>\n",
              "      <td>10000.000000</td>\n",
              "      <td>10000.000000</td>\n",
              "      <td>10000.000000</td>\n",
              "      <td>10000.000000</td>\n",
              "      <td>10000.000000</td>\n",
              "      <td>10000.000000</td>\n",
              "      <td>10000.000000</td>\n",
              "      <td>10000.000000</td>\n",
              "      <td>10000.000000</td>\n",
              "      <td>10000.000000</td>\n",
              "      <td>10000.000000</td>\n",
              "      <td>10000.000000</td>\n",
              "      <td>10000.000000</td>\n",
              "      <td>10000.000000</td>\n",
              "      <td>10000.000000</td>\n",
              "      <td>10000.000000</td>\n",
              "      <td>10000.000000</td>\n",
              "      <td>10000.000000</td>\n",
              "      <td>10000.000000</td>\n",
              "      <td>10000.000000</td>\n",
              "      <td>10000.000000</td>\n",
              "      <td>10000.000000</td>\n",
              "      <td>10000.000000</td>\n",
              "      <td>10000.000000</td>\n",
              "      <td>10000.000000</td>\n",
              "      <td>10000.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.003667</td>\n",
              "      <td>1.002390</td>\n",
              "      <td>1.023809</td>\n",
              "      <td>1.035480</td>\n",
              "      <td>1.117476</td>\n",
              "      <td>1.486990</td>\n",
              "      <td>2.017968</td>\n",
              "      <td>3.257011</td>\n",
              "      <td>4.920690</td>\n",
              "      <td>6.255656</td>\n",
              "      <td>6.451172</td>\n",
              "      <td>6.333493</td>\n",
              "      <td>6.323815</td>\n",
              "      <td>6.861029</td>\n",
              "      <td>6.910054</td>\n",
              "      <td>5.687315</td>\n",
              "      <td>4.564518</td>\n",
              "      <td>2.827696</td>\n",
              "      <td>1.576791</td>\n",
              "      <td>1.310749</td>\n",
              "      <td>1.193173</td>\n",
              "      <td>1.187933</td>\n",
              "      <td>1.062696</td>\n",
              "      <td>1.024864</td>\n",
              "      <td>1.007970</td>\n",
              "      <td>1.002406</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.039307</td>\n",
              "      <td>1.017749</td>\n",
              "      <td>1.021856</td>\n",
              "      <td>1.078252</td>\n",
              "      <td>1.291572</td>\n",
              "      <td>1.927289</td>\n",
              "      <td>2.821164</td>\n",
              "      <td>3.559097</td>\n",
              "      <td>4.845284</td>\n",
              "      <td>5.064893</td>\n",
              "      <td>5.206644</td>\n",
              "      <td>...</td>\n",
              "      <td>4.915390</td>\n",
              "      <td>4.976235</td>\n",
              "      <td>5.054475</td>\n",
              "      <td>4.549793</td>\n",
              "      <td>4.216509</td>\n",
              "      <td>4.274646</td>\n",
              "      <td>3.427936</td>\n",
              "      <td>3.591838</td>\n",
              "      <td>2.999967</td>\n",
              "      <td>1.981783</td>\n",
              "      <td>1.421501</td>\n",
              "      <td>1.181930</td>\n",
              "      <td>1.011452</td>\n",
              "      <td>1.069164</td>\n",
              "      <td>1.225421</td>\n",
              "      <td>1.899921</td>\n",
              "      <td>3.089523</td>\n",
              "      <td>3.588672</td>\n",
              "      <td>3.862318</td>\n",
              "      <td>3.625073</td>\n",
              "      <td>4.319631</td>\n",
              "      <td>5.208095</td>\n",
              "      <td>6.444857</td>\n",
              "      <td>6.424866</td>\n",
              "      <td>6.411244</td>\n",
              "      <td>6.078417</td>\n",
              "      <td>5.303341</td>\n",
              "      <td>5.822237</td>\n",
              "      <td>6.240994</td>\n",
              "      <td>6.083010</td>\n",
              "      <td>5.780717</td>\n",
              "      <td>4.357482</td>\n",
              "      <td>3.592173</td>\n",
              "      <td>3.741058</td>\n",
              "      <td>3.297925</td>\n",
              "      <td>3.544148</td>\n",
              "      <td>2.579955</td>\n",
              "      <td>1.512261</td>\n",
              "      <td>1.232128</td>\n",
              "      <td>1.032410</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.239659</td>\n",
              "      <td>0.192421</td>\n",
              "      <td>0.783123</td>\n",
              "      <td>1.117769</td>\n",
              "      <td>3.483882</td>\n",
              "      <td>8.606353</td>\n",
              "      <td>9.221409</td>\n",
              "      <td>12.334875</td>\n",
              "      <td>14.346869</td>\n",
              "      <td>16.596867</td>\n",
              "      <td>15.906245</td>\n",
              "      <td>15.093312</td>\n",
              "      <td>14.197825</td>\n",
              "      <td>17.674953</td>\n",
              "      <td>18.667207</td>\n",
              "      <td>16.316309</td>\n",
              "      <td>15.957358</td>\n",
              "      <td>13.482974</td>\n",
              "      <td>5.054859</td>\n",
              "      <td>5.624342</td>\n",
              "      <td>3.600184</td>\n",
              "      <td>4.564974</td>\n",
              "      <td>1.413354</td>\n",
              "      <td>0.865160</td>\n",
              "      <td>0.654949</td>\n",
              "      <td>0.174908</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.006195</td>\n",
              "      <td>0.882254</td>\n",
              "      <td>0.655004</td>\n",
              "      <td>1.702460</td>\n",
              "      <td>4.876280</td>\n",
              "      <td>8.861921</td>\n",
              "      <td>10.490556</td>\n",
              "      <td>10.605189</td>\n",
              "      <td>14.836421</td>\n",
              "      <td>12.256836</td>\n",
              "      <td>9.363574</td>\n",
              "      <td>...</td>\n",
              "      <td>10.419820</td>\n",
              "      <td>10.816898</td>\n",
              "      <td>12.126249</td>\n",
              "      <td>11.117241</td>\n",
              "      <td>12.810451</td>\n",
              "      <td>13.433953</td>\n",
              "      <td>9.132231</td>\n",
              "      <td>11.471066</td>\n",
              "      <td>10.161380</td>\n",
              "      <td>8.288347</td>\n",
              "      <td>5.669281</td>\n",
              "      <td>5.285206</td>\n",
              "      <td>0.630222</td>\n",
              "      <td>3.077068</td>\n",
              "      <td>4.698259</td>\n",
              "      <td>8.938931</td>\n",
              "      <td>12.110862</td>\n",
              "      <td>12.534261</td>\n",
              "      <td>13.942601</td>\n",
              "      <td>13.186384</td>\n",
              "      <td>15.111311</td>\n",
              "      <td>15.127639</td>\n",
              "      <td>17.530865</td>\n",
              "      <td>14.419132</td>\n",
              "      <td>15.884367</td>\n",
              "      <td>17.014682</td>\n",
              "      <td>15.099516</td>\n",
              "      <td>15.688436</td>\n",
              "      <td>16.012658</td>\n",
              "      <td>15.550018</td>\n",
              "      <td>17.686318</td>\n",
              "      <td>14.651478</td>\n",
              "      <td>14.419675</td>\n",
              "      <td>15.499743</td>\n",
              "      <td>9.508204</td>\n",
              "      <td>13.038408</td>\n",
              "      <td>10.587615</td>\n",
              "      <td>5.421710</td>\n",
              "      <td>4.847282</td>\n",
              "      <td>2.899793</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>4.502436</td>\n",
              "      <td>...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>7.618480</td>\n",
              "      <td>8.310309</td>\n",
              "      <td>8.109302</td>\n",
              "      <td>8.310309</td>\n",
              "      <td>8.450026</td>\n",
              "      <td>8.379577</td>\n",
              "      <td>6.662889</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>5.220277</td>\n",
              "      <td>5.853392</td>\n",
              "      <td>6.063725</td>\n",
              "      <td>...</td>\n",
              "      <td>5.753641</td>\n",
              "      <td>5.887421</td>\n",
              "      <td>5.887421</td>\n",
              "      <td>5.386659</td>\n",
              "      <td>4.755457</td>\n",
              "      <td>4.522484</td>\n",
              "      <td>4.482566</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>7.735460</td>\n",
              "      <td>8.379577</td>\n",
              "      <td>8.109302</td>\n",
              "      <td>7.341113</td>\n",
              "      <td>6.379165</td>\n",
              "      <td>7.235801</td>\n",
              "      <td>7.980684</td>\n",
              "      <td>7.735460</td>\n",
              "      <td>5.887421</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>21.217439</td>\n",
              "      <td>19.454641</td>\n",
              "      <td>42.290657</td>\n",
              "      <td>79.024874</td>\n",
              "      <td>290.173165</td>\n",
              "      <td>290.173165</td>\n",
              "      <td>290.173165</td>\n",
              "      <td>290.173165</td>\n",
              "      <td>290.173165</td>\n",
              "      <td>290.173165</td>\n",
              "      <td>290.173165</td>\n",
              "      <td>290.173165</td>\n",
              "      <td>290.173165</td>\n",
              "      <td>290.173165</td>\n",
              "      <td>290.173165</td>\n",
              "      <td>290.173165</td>\n",
              "      <td>290.173165</td>\n",
              "      <td>290.173165</td>\n",
              "      <td>290.173165</td>\n",
              "      <td>290.173165</td>\n",
              "      <td>290.173165</td>\n",
              "      <td>290.173165</td>\n",
              "      <td>79.024874</td>\n",
              "      <td>65.542895</td>\n",
              "      <td>65.542895</td>\n",
              "      <td>15.898597</td>\n",
              "      <td>1.0</td>\n",
              "      <td>290.173165</td>\n",
              "      <td>79.024874</td>\n",
              "      <td>36.165775</td>\n",
              "      <td>79.024874</td>\n",
              "      <td>290.173165</td>\n",
              "      <td>290.173165</td>\n",
              "      <td>290.173165</td>\n",
              "      <td>290.173165</td>\n",
              "      <td>290.173165</td>\n",
              "      <td>290.173165</td>\n",
              "      <td>290.173165</td>\n",
              "      <td>...</td>\n",
              "      <td>290.173165</td>\n",
              "      <td>290.173165</td>\n",
              "      <td>290.173165</td>\n",
              "      <td>290.173165</td>\n",
              "      <td>290.173165</td>\n",
              "      <td>290.173165</td>\n",
              "      <td>290.173165</td>\n",
              "      <td>290.173165</td>\n",
              "      <td>290.173165</td>\n",
              "      <td>290.173165</td>\n",
              "      <td>290.173165</td>\n",
              "      <td>290.173165</td>\n",
              "      <td>45.025836</td>\n",
              "      <td>290.173165</td>\n",
              "      <td>290.173165</td>\n",
              "      <td>290.173165</td>\n",
              "      <td>290.173165</td>\n",
              "      <td>290.173165</td>\n",
              "      <td>290.173165</td>\n",
              "      <td>290.173165</td>\n",
              "      <td>290.173165</td>\n",
              "      <td>290.173165</td>\n",
              "      <td>290.173165</td>\n",
              "      <td>290.173165</td>\n",
              "      <td>290.173165</td>\n",
              "      <td>290.173165</td>\n",
              "      <td>290.173165</td>\n",
              "      <td>290.173165</td>\n",
              "      <td>290.173165</td>\n",
              "      <td>290.173165</td>\n",
              "      <td>290.173165</td>\n",
              "      <td>290.173165</td>\n",
              "      <td>290.173165</td>\n",
              "      <td>290.173165</td>\n",
              "      <td>290.173165</td>\n",
              "      <td>290.173165</td>\n",
              "      <td>290.173165</td>\n",
              "      <td>290.173165</td>\n",
              "      <td>290.173165</td>\n",
              "      <td>290.173165</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8 rows × 784 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4d947ad0-78d5-4cdd-b98a-35768bbd011d')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-4d947ad0-78d5-4cdd-b98a-35768bbd011d button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-4d947ad0-78d5-4cdd-b98a-35768bbd011d');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "           0        1             2    ...           781           782           783\n",
              "count  10000.0  10000.0  10000.000000  ...  10000.000000  10000.000000  10000.000000\n",
              "mean       1.0      1.0      1.003667  ...      1.512261      1.232128      1.032410\n",
              "std        0.0      0.0      0.239659  ...      5.421710      4.847282      2.899793\n",
              "min        1.0      1.0      1.000000  ...      1.000000      1.000000      1.000000\n",
              "25%        1.0      1.0      1.000000  ...      1.000000      1.000000      1.000000\n",
              "50%        1.0      1.0      1.000000  ...      1.000000      1.000000      1.000000\n",
              "75%        1.0      1.0      1.000000  ...      1.000000      1.000000      1.000000\n",
              "max        1.0      1.0     21.217439  ...    290.173165    290.173165    290.173165\n",
              "\n",
              "[8 rows x 784 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SNN model creation"
      ],
      "metadata": {
        "id": "EOcDsitclOYJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oxprud_N1l8p"
      },
      "outputs": [],
      "source": [
        "tau_mem = 10e-3\n",
        "tau_syn = 5e-3\n",
        "\n",
        "alpha   = float(np.exp(-time_step/tau_syn))\n",
        "beta    = float(np.exp(-time_step/tau_mem))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7aIxMLD71p81",
        "outputId": "aca3629e-d47a-4d97-9a52-94efa58fb387"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "init done\n"
          ]
        }
      ],
      "source": [
        "weight_scale = 7*(1.0-beta) # this should give us some spikes to begin with\n",
        "\n",
        "w1 = torch.empty((nb_inputs, nb_hidden),  device=device, dtype=dtype, requires_grad=True)\n",
        "torch.nn.init.normal_(w1, mean=0.0, std=weight_scale/np.sqrt(nb_inputs))\n",
        "\n",
        "w2 = torch.empty((nb_hidden, nb_outputs), device=device, dtype=dtype, requires_grad=True)\n",
        "torch.nn.init.normal_(w2, mean=0.0, std=weight_scale/np.sqrt(nb_hidden))\n",
        "\n",
        "print(\"init done\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k3gQSq2j16ff"
      },
      "outputs": [],
      "source": [
        "def plot_voltage_traces(mem, spk=None, dim=(3,5), spike_height=5):\n",
        "    gs=GridSpec(*dim)\n",
        "    if spk is not None:\n",
        "        dat = 1.0*mem\n",
        "        dat[spk>0.0] = spike_height\n",
        "        dat = dat.detach().cpu().numpy()\n",
        "    else:\n",
        "        dat = mem.detach().cpu().numpy()\n",
        "    for i in range(np.prod(dim)):\n",
        "        if i==0: a0=ax=plt.subplot(gs[i])\n",
        "        else: ax=plt.subplot(gs[i],sharey=a0)\n",
        "        ax.plot(dat[i])\n",
        "        ax.axis(\"off\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TdBfc3ib1-Ad"
      },
      "outputs": [],
      "source": [
        "class SurrGradSpike(torch.autograd.Function):\n",
        "    \"\"\"\n",
        "    Here we implement our spiking nonlinearity which also implements \n",
        "    the surrogate gradient.\n",
        "    \"\"\"\n",
        "    \n",
        "    scale = 100.0 # controls steepness of surrogate gradient\n",
        "\n",
        "    @staticmethod\n",
        "    def forward(ctx, input):\n",
        "        \"\"\"\n",
        "        In the forward pass we compute a step function of the input Tensor\n",
        "        and return it.\n",
        "        \"\"\"\n",
        "        ctx.save_for_backward(input)\n",
        "        out = torch.zeros_like(input)\n",
        "        out[input > 0] = 1.0\n",
        "        return out\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        \"\"\"\n",
        "        In the backward pass we receive a Tensor we need to compute the \n",
        "        surrogate gradient of the loss with respect to the input. \n",
        "        \"\"\"\n",
        "        input, = ctx.saved_tensors\n",
        "        grad_input = grad_output.clone()\n",
        "        grad = grad_input/(SurrGradSpike.scale*torch.abs(input)+1.0)**2\n",
        "        return grad\n",
        "    \n",
        "# here we overwrite our naive spike function by the \"SurrGradSpike\" nonlinearity which implements a surrogate gradient\n",
        "spike_fn  = SurrGradSpike.apply"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eMBcNYAr2BxH"
      },
      "outputs": [],
      "source": [
        "def run_snn(inputs):\n",
        "    h1 = torch.einsum(\"abc,cd->abd\", (inputs, w1))\n",
        "    syn = torch.zeros((batch_size,nb_hidden), device=device, dtype=dtype)\n",
        "    mem = torch.zeros((batch_size,nb_hidden), device=device, dtype=dtype)\n",
        "\n",
        "    mem_rec = []\n",
        "    spk_rec = []\n",
        "\n",
        "    # Compute hidden layer activity\n",
        "    for t in range(nb_steps):\n",
        "        mthr = mem-1.0\n",
        "        out = spike_fn(mthr)\n",
        "        rst = out.detach() # We do not want to backprop through the reset\n",
        "\n",
        "        new_syn = alpha*syn +h1[:,t]\n",
        "        new_mem = (beta*mem +syn)*(1.0-rst)\n",
        "\n",
        "        mem_rec.append(mem)\n",
        "        spk_rec.append(out)\n",
        "        \n",
        "        mem = new_mem\n",
        "        syn = new_syn\n",
        "\n",
        "    mem_rec = torch.stack(mem_rec,dim=1)\n",
        "    spk_rec = torch.stack(spk_rec,dim=1)\n",
        "\n",
        "    # Readout layer\n",
        "    h2= torch.einsum(\"abc,cd->abd\", (spk_rec, w2))\n",
        "    flt = torch.zeros((batch_size,nb_outputs), device=device, dtype=dtype)\n",
        "    out = torch.zeros((batch_size,nb_outputs), device=device, dtype=dtype)\n",
        "    out_rec = [out]\n",
        "    for t in range(nb_steps):\n",
        "        new_flt = alpha*flt +h2[:,t]\n",
        "        new_out = beta*out +flt\n",
        "\n",
        "        flt = new_flt\n",
        "        out = new_out\n",
        "\n",
        "        out_rec.append(out)\n",
        "\n",
        "    out_rec = torch.stack(out_rec,dim=1)\n",
        "    other_recs = [mem_rec, spk_rec]\n",
        "    return out_rec, other_recs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iz4_ZYmn2KBT"
      },
      "outputs": [],
      "source": [
        "def train(x_data, y_data, lr=2e-3, nb_epochs=10):\n",
        "    params = [w1,w2]\n",
        "    optimizer = torch.optim.Adam(params, lr=lr, betas=(0.9,0.999))\n",
        "\n",
        "    log_softmax_fn = nn.LogSoftmax(dim=1)\n",
        "    loss_fn = nn.NLLLoss()\n",
        "    \n",
        "    loss_hist = []\n",
        "    for e in range(nb_epochs):\n",
        "        local_loss = []\n",
        "        for x_local, y_local in sparse_data_generator(x_data, y_data, batch_size, nb_steps, nb_inputs):\n",
        "            output,_ = run_snn(x_local.to_dense())\n",
        "            m,_=torch.max(output,1)\n",
        "            log_p_y = log_softmax_fn(m)\n",
        "            loss_val = loss_fn(log_p_y, y_local)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss_val.backward()\n",
        "            optimizer.step()\n",
        "            local_loss.append(loss_val.item())\n",
        "        mean_loss = np.mean(local_loss)\n",
        "        print(\"Epoch %i: loss=%.5f\"%(e+1,mean_loss))\n",
        "        loss_hist.append(mean_loss)\n",
        "        \n",
        "    return loss_hist\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GIlGIMLE2WtX"
      },
      "outputs": [],
      "source": [
        "def compute_classification_accuracy(x_data, y_data):\n",
        "    \"\"\" Computes classification accuracy on supplied data in batches. \"\"\"\n",
        "    accs = []\n",
        "    for x_local, y_local in sparse_data_generator(x_data, y_data, batch_size, nb_steps, nb_inputs, shuffle=False):\n",
        "        output,_ = run_snn(x_local.to_dense())\n",
        "        m,_= torch.max(output,1) # max over time\n",
        "        _,am=torch.max(m,1)      # argmax over output units\n",
        "        tmp = np.mean((y_local==am).detach().cpu().numpy()) # compare to labels\n",
        "        accs.append(tmp)\n",
        "    return np.mean(accs)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SNN Training"
      ],
      "metadata": {
        "id": "qlCOp9mplcRC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d9gTxmoJ2Y79",
        "outputId": "94c9e8ef-6198-47b4-d3bb-aebcbaa485df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:35: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:41: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: loss=0.93590\n",
            "Epoch 2: loss=0.52856\n",
            "Epoch 3: loss=0.47442\n",
            "Epoch 4: loss=0.44522\n",
            "Epoch 5: loss=0.42003\n",
            "Epoch 6: loss=0.40063\n",
            "Epoch 7: loss=0.38725\n",
            "Epoch 8: loss=0.37511\n",
            "Epoch 9: loss=0.36511\n",
            "Epoch 10: loss=0.35576\n",
            "Epoch 11: loss=0.35075\n",
            "Epoch 12: loss=0.33949\n",
            "Epoch 13: loss=0.33083\n",
            "Epoch 14: loss=0.32491\n",
            "Epoch 15: loss=0.31933\n",
            "Epoch 16: loss=0.31451\n",
            "Epoch 17: loss=0.30733\n",
            "Epoch 18: loss=0.30243\n",
            "Epoch 19: loss=0.29639\n",
            "Epoch 20: loss=0.29093\n",
            "Epoch 21: loss=0.28765\n",
            "Epoch 22: loss=0.28414\n",
            "Epoch 23: loss=0.27830\n",
            "Epoch 24: loss=0.27513\n",
            "Epoch 25: loss=0.27077\n",
            "Epoch 26: loss=0.26585\n",
            "Epoch 27: loss=0.26280\n",
            "Epoch 28: loss=0.25866\n",
            "Epoch 29: loss=0.25791\n",
            "Epoch 30: loss=0.25222\n"
          ]
        }
      ],
      "source": [
        "loss_hist = train(x_train, y_train, lr=2e-4, nb_epochs=30)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "id": "w-QMMOr42bjP",
        "outputId": "857190f8-818f-4620-aaff-fe6596d11f90"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 495x300 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.figure(figsize=(3.3,2),dpi=150)\n",
        "plt.plot(loss_hist)\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "sns.despine()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SNN Testing"
      ],
      "metadata": {
        "id": "jhpG3pyOlgIR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZRVucZ4U2eTg",
        "outputId": "5899a3be-47cf-416a-c904-1c52c36e698d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training accuracy: 0.911\n",
            "Test accuracy: 0.864\n"
          ]
        }
      ],
      "source": [
        "print(\"Training accuracy: %.3f\"%(compute_classification_accuracy(x_train,y_train)))\n",
        "print(\"Test accuracy: %.3f\"%(compute_classification_accuracy(x_test,y_test)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQkPbD_ZKqVQ"
      },
      "source": [
        "## Testing using secure MPC\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "This part of the project may not execute correctly when using GPU and world_size=2. To use GPU check our additional code on our GitLab page\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "LdFyDW9UKbqo",
        "outputId": "824bea02-4d5c-4cfd-a288-56be7ec38ccb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting crypten\n",
            "  Downloading crypten-0.4.0-py3-none-any.whl (245 kB)\n",
            "\u001b[?25l\r\u001b[K     |█▍                              | 10 kB 20.3 MB/s eta 0:00:01\r\u001b[K     |██▊                             | 20 kB 11.1 MB/s eta 0:00:01\r\u001b[K     |████                            | 30 kB 9.4 MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 40 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 51 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████                        | 61 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 71 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 81 kB 6.3 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 92 kB 4.8 MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 102 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 112 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 122 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 133 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 143 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 153 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 163 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 174 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 184 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 194 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 204 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 215 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 225 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 235 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 245 kB 5.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: sklearn in /usr/local/lib/python3.7/dist-packages (from crypten) (0.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from crypten) (0.16.0)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.7/dist-packages (from crypten) (2.7.0)\n",
            "Requirement already satisfied: pandas>=1.2.2 in /usr/local/lib/python3.7/dist-packages (from crypten) (1.3.5)\n",
            "Requirement already satisfied: torchvision>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from crypten) (0.11.1+cu111)\n",
            "Requirement already satisfied: torch>=1.7.0 in /usr/local/lib/python3.7/dist-packages (from crypten) (1.10.0+cu111)\n",
            "Collecting scipy>=1.6.0\n",
            "  Downloading scipy-1.7.3-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (38.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 38.1 MB 1.7 MB/s \n",
            "\u001b[?25hCollecting omegaconf>=2.0.6\n",
            "  Downloading omegaconf-2.1.1-py3-none-any.whl (74 kB)\n",
            "\u001b[K     |████████████████████████████████| 74 kB 3.1 MB/s \n",
            "\u001b[?25hCollecting pyyaml>=5.3.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 48.4 MB/s \n",
            "\u001b[?25hCollecting onnx>=1.7.0\n",
            "  Downloading onnx-1.10.2-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (12.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.7 MB 29.1 MB/s \n",
            "\u001b[?25hCollecting antlr4-python3-runtime==4.8\n",
            "  Downloading antlr4-python3-runtime-4.8.tar.gz (112 kB)\n",
            "\u001b[K     |████████████████████████████████| 112 kB 47.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from onnx>=1.7.0->crypten) (3.17.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.2.1 in /usr/local/lib/python3.7/dist-packages (from onnx>=1.7.0->crypten) (3.10.0.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from onnx>=1.7.0->crypten) (1.15.0)\n",
            "Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.7/dist-packages (from onnx>=1.7.0->crypten) (1.19.5)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.2.2->crypten) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.2.2->crypten) (2.8.2)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision>=0.9.1->crypten) (7.1.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sklearn->crypten) (1.0.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn->crypten) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn->crypten) (3.0.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->crypten) (1.8.1)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard->crypten) (1.43.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard->crypten) (1.0.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard->crypten) (0.37.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->crypten) (0.6.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard->crypten) (0.4.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->crypten) (2.23.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard->crypten) (1.0.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard->crypten) (1.35.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->crypten) (57.4.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard->crypten) (3.3.6)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard->crypten) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard->crypten) (4.8)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard->crypten) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->crypten) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard->crypten) (4.10.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard->crypten) (3.7.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->crypten) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard->crypten) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard->crypten) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard->crypten) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard->crypten) (1.24.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->crypten) (3.1.1)\n",
            "Building wheels for collected packages: antlr4-python3-runtime\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-py3-none-any.whl size=141230 sha256=04e84864ff161bae511f415b7a29fa7daa8a7f764ae85aaa21a0a17f3c4f2e6e\n",
            "  Stored in directory: /root/.cache/pip/wheels/ca/33/b7/336836125fc9bb4ceaa4376d8abca10ca8bc84ddc824baea6c\n",
            "Successfully built antlr4-python3-runtime\n",
            "Installing collected packages: scipy, pyyaml, antlr4-python3-runtime, onnx, omegaconf, crypten\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.4.1\n",
            "    Uninstalling scipy-1.4.1:\n",
            "      Successfully uninstalled scipy-1.4.1\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed antlr4-python3-runtime-4.8 crypten-0.4.0 omegaconf-2.1.1 onnx-1.10.2 pyyaml-6.0 scipy-1.7.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pydevd_plugins",
                  "scipy"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install crypten"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EBIOyi0cKdLF"
      },
      "outputs": [],
      "source": [
        "import crypten\n",
        "import crypten.mpc as mpc\n",
        "import crypten.communicator as comm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l7qDiLe0Ke_A"
      },
      "outputs": [],
      "source": [
        "crypten.init()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QwaRIBPJKgfP"
      },
      "outputs": [],
      "source": [
        "w_size=2\n",
        "ALICE=0\n",
        "BOB=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K-T1fWlLKh9_"
      },
      "outputs": [],
      "source": [
        "\"\"\"returns spikes\"\"\"\n",
        "\n",
        "#@mpc.run_multiprocess(world_size=2)\n",
        "def spike_fn_test(x):\n",
        "    sub=torch.zeros(x.size(),dtype=float)\n",
        "    sub.to(device)\n",
        "    sub_enc=crypten.cryptensor(sub,device=device)\n",
        "    out=x>sub_enc\n",
        "    out.to(device)\n",
        " \n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4I4n5GvNKk3U"
      },
      "outputs": [],
      "source": [
        "from numpy.core.arrayprint import format_float_scientific\n",
        "\n",
        "#@mpc.run_multiprocess(world_size=2)\n",
        "def run_snn_test(inputs,w1_enc,w2_enc):\n",
        "    #h1 = torch.einsum(\"abc,cd->abd\", (inputs, w1))\n",
        "    syn = torch.zeros((batch_size,nb_hidden), device=device, dtype=dtype)\n",
        "    mem = torch.zeros((batch_size,nb_hidden), device=device, dtype=dtype)\n",
        "\n",
        "    syn_enc=crypten.cryptensor(syn)\n",
        "    mem_enc=crypten.cryptensor(mem)\n",
        "    #print(syn_enc.device)\n",
        "    #print(mem_enc.device)\n",
        "\n",
        "\n",
        "\n",
        "    mem_rec = []\n",
        "    spk_rec = []\n",
        "\n",
        "    # Compute hidden layer activity\n",
        "   # print(\"first hidden layer\")\n",
        "    mult_1=inputs.matmul(w1_enc)\n",
        "    for t in range(nb_steps):\n",
        "        #print(\"step number :\"+str(t))\n",
        "        mthr = mem_enc-crypten.cryptensor(1.0)\n",
        "        #print(mthr.device)\n",
        "        mthr.to(device)\n",
        "        out = spike_fn_test(mthr)\n",
        "        rst = out.detach() # We do not want to backprop through the reset\n",
        "\n",
        "        new_syn = alpha*syn_enc*alpha +mult_1[:,t]\n",
        "        new_mem = (beta*mem_enc +syn_enc)*(1.0-rst)\n",
        "\n",
        "        mem_rec.append(mem_enc)\n",
        "        spk_rec.append(out)\n",
        "        \n",
        "        mem_enc = new_mem\n",
        "        syn_enc = new_syn\n",
        "\n",
        "    mem_rec = crypten.stack(mem_rec,dim=1)\n",
        "    spk_rec = crypten.stack(spk_rec,dim=1)\n",
        "\n",
        "    #print(mem_rec.get_plain_text())\n",
        "    print(spk_rec.get_plain_text())\n",
        "\n",
        "    # Readout layer\n",
        "    #print(\"readout layer\")\n",
        "    #h2= torch.einsum(\"abc,cd->abd\", (spk_rec, w2))\n",
        "    \n",
        "    \n",
        "    flt = torch.zeros((batch_size,nb_outputs), device=device, dtype=dtype)\n",
        "    #print(\"flt\")\n",
        "    out = torch.zeros((batch_size,nb_outputs), device=device, dtype=dtype)\n",
        "    #print(\"out\")\n",
        "\n",
        "    flt_enc=crypten.cryptensor(flt)\n",
        "    #print(\"flt\")\n",
        "    out_enc=crypten.cryptensor(out)\n",
        "    #print(\"out\")\n",
        "\n",
        "\n",
        "    out_rec = []\n",
        "    out_rec.append(out_enc)\n",
        "\n",
        "    #print(\"entering for iteration\")\n",
        "\n",
        "    #print(w2_enc.get_plain_text())\n",
        "    mult_2=spk_rec.matmul(w2_enc)\n",
        "    for t in range(nb_steps):\n",
        "        #print(\"step number :\"+str(t))\n",
        "        #if t<100 and t>95:\n",
        "          #print(flt_enc.get_plain_text())\n",
        "        new_flt = alpha*flt_enc +mult_2[:,t]\n",
        "        new_out = beta*out_enc +flt_enc\n",
        "\n",
        "        flt_enc = new_flt\n",
        "        out_enc = new_out\n",
        "\n",
        "        out_rec.append(out_enc)\n",
        "\n",
        "    out_rec = crypten.stack(out_rec,dim=1)\n",
        "    #print(out_rec.get_plain_text())\n",
        "    other_recs = [mem_rec, spk_rec]\n",
        "    return out_rec, other_recs"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%xmode Verbose"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DtpcBEAZdRJA",
        "outputId": "e6af7a0f-adfc-4d80-ff87-49c3453a963c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exception reporting mode: Verbose\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bDDlGWxKKoZ9"
      },
      "outputs": [],
      "source": [
        "#@mpc.run_multiprocess(world_size=2)\n",
        "def compute_classification_accuracy_test(x_data, y_data):\n",
        "    \"\"\" Computes classification accuracy on supplied data in batches. \"\"\"\n",
        "\n",
        "    w1_enc=crypten.cryptensor(w1,src=0)\n",
        "    w2_enc=crypten.cryptensor(w2,src=0)\n",
        "    #print(w1_enc.device)\n",
        "\n",
        "    #print(w2_enc.device)\n",
        "\n",
        "    \n",
        "    \n",
        "    accs = []\n",
        "    i=0\n",
        "    for x_local, y_local in sparse_data_generator(x_data, y_data, batch_size, nb_steps, nb_inputs, shuffle=False):\n",
        "        x_data_enc=crypten.cryptensor(x_local.to_dense())\n",
        "        #print(x_data_enc.device)\n",
        "        output_enc,_ = run_snn_test(x_data_enc,w1_enc,w2_enc)\n",
        "        output=output_enc.get_plain_text()\n",
        "        m,_= torch.max(output,1) # max over time\n",
        "        _,am=torch.max(m,1)      # argmax over output units\n",
        "        tmp = np.mean((y_local==am).detach().cpu().numpy()) # compare to labels\n",
        "        accs.append(tmp)\n",
        "        print(tmp)\n",
        "        i+=1\n",
        "    return np.mean(accs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sFdH-OngPn_8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8b0f1e7-107e-4c54-df6c-47abfab956c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]]], device='cuda:0')\n",
            "0.8828125\n",
            "tensor([[[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]]], device='cuda:0')\n",
            "0.85546875\n",
            "tensor([[[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]]], device='cuda:0')\n",
            "0.85546875\n",
            "tensor([[[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]]], device='cuda:0')\n",
            "0.890625\n",
            "tensor([[[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]]], device='cuda:0')\n",
            "0.859375\n",
            "tensor([[[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]]], device='cuda:0')\n",
            "0.86328125\n",
            "tensor([[[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]]], device='cuda:0')\n",
            "0.859375\n",
            "tensor([[[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]]], device='cuda:0')\n",
            "0.8515625\n",
            "tensor([[[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]]], device='cuda:0')\n",
            "0.86328125\n",
            "tensor([[[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]]], device='cuda:0')\n",
            "0.80859375\n",
            "tensor([[[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]]], device='cuda:0')\n",
            "0.86328125\n",
            "tensor([[[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]]], device='cuda:0')\n",
            "0.828125\n",
            "tensor([[[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]]], device='cuda:0')\n",
            "0.8203125\n",
            "tensor([[[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]]], device='cuda:0')\n",
            "0.8359375\n",
            "tensor([[[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]]], device='cuda:0')\n",
            "0.82421875\n",
            "tensor([[[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]]], device='cuda:0')\n",
            "0.82421875\n",
            "tensor([[[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]]], device='cuda:0')\n",
            "0.87890625\n",
            "tensor([[[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]]], device='cuda:0')\n",
            "0.84375\n",
            "tensor([[[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]]], device='cuda:0')\n",
            "0.7890625\n",
            "tensor([[[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]]], device='cuda:0')\n",
            "0.8046875\n",
            "tensor([[[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]]], device='cuda:0')\n",
            "0.828125\n",
            "tensor([[[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]]], device='cuda:0')\n",
            "0.796875\n",
            "tensor([[[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]]], device='cuda:0')\n",
            "0.875\n",
            "tensor([[[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]]], device='cuda:0')\n",
            "0.86328125\n",
            "tensor([[[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]]], device='cuda:0')\n",
            "0.84375\n",
            "tensor([[[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]]], device='cuda:0')\n",
            "0.81640625\n",
            "tensor([[[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]]], device='cuda:0')\n",
            "0.85546875\n",
            "tensor([[[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]]], device='cuda:0')\n",
            "0.83984375\n",
            "tensor([[[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]]], device='cuda:0')\n",
            "0.8359375\n",
            "tensor([[[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]]], device='cuda:0')\n",
            "0.8515625\n",
            "tensor([[[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]]], device='cuda:0')\n",
            "0.875\n",
            "tensor([[[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]]], device='cuda:0')\n",
            "0.87109375\n",
            "tensor([[[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]]], device='cuda:0')\n",
            "0.8828125\n",
            "tensor([[[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]]], device='cuda:0')\n",
            "0.87109375\n",
            "tensor([[[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]]], device='cuda:0')\n",
            "0.8046875\n",
            "tensor([[[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]]], device='cuda:0')\n",
            "0.84765625\n",
            "tensor([[[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]]], device='cuda:0')\n",
            "0.84765625\n",
            "tensor([[[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]]], device='cuda:0')\n",
            "0.83984375\n",
            "tensor([[[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]]], device='cuda:0')\n",
            "0.88671875\n",
            "CPU times: user 2min 50s, sys: 1.02 s, total: 2min 51s\n",
            "Wall time: 2min 55s\n"
          ]
        }
      ],
      "source": [
        "%time a=compute_classification_accuracy_test(x_test,y_test)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(a)"
      ],
      "metadata": {
        "id": "Govt-lKcVvSr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5acf1129-c308-462c-eca3-323de4168c75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8470552884615384\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def run_snn_test_2(inputs,w1_enc,w2_enc):\n",
        "    #h1 = torch.einsum(\"abc,cd->abd\", (inputs, w1))\n",
        "    syn = torch.zeros((nb_hidden), device=device, dtype=dtype)\n",
        "    mem = torch.zeros((nb_hidden), device=device, dtype=dtype)\n",
        "\n",
        "    syn_enc=crypten.cryptensor(syn)\n",
        "    mem_enc=crypten.cryptensor(mem)\n",
        "    #print(syn_enc.device)\n",
        "    #print(mem_enc.device)\n",
        "\n",
        "\n",
        "\n",
        "    mem_rec = []\n",
        "    spk_rec = []\n",
        "\n",
        "    # Compute hidden layer activity\n",
        "   # print(\"first hidden layer\")\n",
        "    mult_1=inputs.matmul(w1_enc)\n",
        "    for t in range(nb_steps):\n",
        "        #print(\"step number :\"+str(t))\n",
        "        mthr = mem_enc-crypten.cryptensor(1.0)\n",
        "        #print(mthr.device)\n",
        "        mthr.to(device)\n",
        "        out = spike_fn_test(mthr)\n",
        "        rst = out.detach() # We do not want to backprop through the reset\n",
        "\n",
        "        new_syn = alpha*syn_enc*alpha +mult_1[:,t]\n",
        "        new_mem = (beta*mem_enc +syn_enc)*(1.0-rst)\n",
        "\n",
        "        mem_rec.append(mem_enc)\n",
        "        spk_rec.append(out)\n",
        "        \n",
        "        mem_enc = new_mem\n",
        "        syn_enc = new_syn\n",
        "\n",
        "    mem_rec = crypten.stack(mem_rec,dim=1)\n",
        "    spk_rec = crypten.stack(spk_rec,dim=1)\n",
        "    #print(spk_rec.size())\n",
        "\n",
        "\n",
        "    #print(mem_rec.get_plain_text())\n",
        "    #print(spk_rec.get_plain_text())\n",
        "\n",
        "    # Readout layer\n",
        "    #print(\"readout layer\")\n",
        "    #h2= torch.einsum(\"abc,cd->abd\", (spk_rec, w2))\n",
        "    \n",
        "    \n",
        "    flt = torch.zeros((nb_outputs), device=device, dtype=dtype)\n",
        "    #print(\"flt\")\n",
        "    out = torch.zeros((nb_outputs), device=device, dtype=dtype)\n",
        "    #print(\"out\")\n",
        "\n",
        "    flt_enc=crypten.cryptensor(flt)\n",
        "    #print(\"flt\")\n",
        "    out_enc=crypten.cryptensor(out)\n",
        "    #print(\"out\")\n",
        "\n",
        "\n",
        "    out_rec = []\n",
        "    out_rec.append(out_enc)\n",
        "\n",
        "    #print(\"entering for iteration\")\n",
        "\n",
        "    #print(w2_enc.get_plain_text())\n",
        "    #print(spk_rec.matmul(w2_enc)[:,1].size())\n",
        "    #print(flt_enc.size())\n",
        "\n",
        "    mult_2=spk_rec.matmul(w2_enc)\n",
        "    for t in range(nb_steps):\n",
        "        #print(\"step number :\"+str(t))\n",
        "        #if t<100 and t>95:\n",
        "          #print(flt_enc.get_plain_text())\n",
        "        new_flt = alpha*flt_enc +mult_2[t,:]\n",
        "        new_out = beta*out_enc +flt_enc\n",
        "\n",
        "        flt_enc = new_flt\n",
        "        out_enc = new_out\n",
        "\n",
        "        out_rec.append(out_enc)\n",
        "\n",
        "    out_rec = crypten.stack(out_rec,dim=1)\n",
        "    #print(out_rec.get_plain_text())\n",
        "    other_recs = [mem_rec, spk_rec]\n",
        "    return out_rec, other_recs"
      ],
      "metadata": {
        "id": "BCgmCEhtk9da"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@mpc.run_multiprocess(world_size=2)\n",
        "def compute_classification_accuracy_test_2(x_data, y_data):\n",
        "    \"\"\" Computes classification accuracy on supplied data in batches. \"\"\"\n",
        "\n",
        "    w1_enc=crypten.cryptensor(w1,src=0)\n",
        "    w2_enc=crypten.cryptensor(w2,src=0)\n",
        "    #print(w1_enc.device)\n",
        "\n",
        "    #print(w2_enc.device)\n",
        "\n",
        "    \n",
        "    \n",
        "    accs = []\n",
        "    \n",
        "    for x_local, y_local in sparse_data_generator(x_data, y_data, batch_size, nb_steps, nb_inputs, shuffle=False):\n",
        "        \n",
        "        x_data_enc=crypten.cryptensor(x_local.to_dense())\n",
        "        #print(x_data_enc.device)\n",
        "        for i in range(batch_size):\n",
        "          batch_am=[]\n",
        "          output_enc,_ = run_snn_test_2(x_data_enc[i],w1_enc,w2_enc)\n",
        "          #print(output_enc.size())\n",
        "          output=output_enc.get_plain_text()\n",
        "          m,_= torch.max(output,1) # max over time\n",
        "          #print(m.size())\n",
        "          _,am=torch.max(m,0)# argmax over output units\n",
        "          #print(am)\n",
        "          batch_am.append(am) \n",
        "        batch_am_stack=torch.stack(batch_am)       \n",
        "        tmp = np.mean((y_local==batch_am_stack).detach().cpu().numpy()) # compare to labels\n",
        "        accs.append(tmp)\n",
        "        print(tmp)\n",
        "          \n",
        "    return np.mean(accs)"
      ],
      "metadata": {
        "id": "H9vonPMs2rEt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%time a=compute_classification_accuracy_test_2(x_test,y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 919
        },
        "id": "Czk4ErjB3Onk",
        "outputId": "5c1bd6da-6bd2-4bc9-f9fa-7f63f8d4339f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.10546875\n",
            "0.08203125\n",
            "0.078125\n",
            "0.11328125\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/crypten/config/config.py\u001b[0m in \u001b[0;36m__getattribute__\u001b[0;34m(self=<crypten.config.config.CrypTenConfig object>, name='debug')\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m        \u001b[0;36mglobal\u001b[0m \u001b[0;36mobject.__getattribute__\u001b[0m \u001b[0;34m= \u001b[0;36mundefined\u001b[0m\u001b[0m\u001b[0;34m\n        \u001b[0m\u001b[0;36mself\u001b[0m \u001b[0;34m= <crypten.config.config.CrypTenConfig object at 0x7fa0bfccf250>\u001b[0m\u001b[0;34m\n        \u001b[0m\u001b[0;36mname\u001b[0m \u001b[0;34m= 'debug'\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'CrypTenConfig' object has no attribute 'debug'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-581577000b8a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time a=compute_classification_accuracy_test_2(x_test,y_test)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m        \u001b[0;36mglobal\u001b[0m \u001b[0;36mget_ipython.magic\u001b[0m \u001b[0;34m= \u001b[0;36mundefined\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mmagic\u001b[0;34m(self=<google.colab._shell.Shell object>, arg_s='time a=compute_classification_accuracy_test_2(x_test,y_test)')\u001b[0m\n\u001b[1;32m   2158\u001b[0m         \u001b[0mmagic_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg_s\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2159\u001b[0m         \u001b[0mmagic_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmagic_name\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefilter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mESC_MAGIC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2160\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m        \u001b[0;36mself.run_line_magic\u001b[0m \u001b[0;34m= <bound method InteractiveShell.run_line_magic of <google.colab._shell.Shell object at 0x7fa1ec099750>>\u001b[0m\u001b[0;34m\n        \u001b[0m\u001b[0;36mmagic_name\u001b[0m \u001b[0;34m= 'time'\u001b[0m\u001b[0;34m\n        \u001b[0m\u001b[0;36mmagic_arg_s\u001b[0m \u001b[0;34m= 'a=compute_classification_accuracy_test_2(x_test,y_test)'\u001b[0m\n\u001b[1;32m   2161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2162\u001b[0m     \u001b[0;31m#-------------------------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_line_magic\u001b[0;34m(self=<google.colab._shell.Shell object>, magic_name='time', line='a=compute_classification_accuracy_test_2(x_test,y_test)')\u001b[0m\n\u001b[1;32m   2079\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'local_ns'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_locals\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2080\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2081\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m        \u001b[0;36mresult\u001b[0m \u001b[0;34m= \u001b[0;36mundefined\u001b[0m\u001b[0m\u001b[0;34m\n        \u001b[0m\u001b[0;36mfn\u001b[0m \u001b[0;34m= <bound method ExecutionMagics.time of <IPython.core.magics.execution.ExecutionMagics object at 0x7fa1e7b8df50>>\u001b[0m\u001b[0;34m\n        \u001b[0m\u001b[0;36margs\u001b[0m \u001b[0;34m= ['a=compute_classification_accuracy_test_2(x_test,y_test)']\u001b[0m\u001b[0;34m\n        \u001b[0m\u001b[0;36mkwargs\u001b[0m \u001b[0;34m= {'local_ns': {'__name__': '__main__', '__doc__': 'returns spikes', '__package__': None, '__loader__': None, '__spec__': None, '__builtin__': <module 'builtins' (built-in)>, '__builtins__': <module 'builtins' (built-in)>, '_ih': ['', 'import os\\n\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\nfrom matplotlib.gridspec import GridSpec\\nimport seaborn as sns\\n\\nimport torch\\nimport torch.nn as nn\\nimport torchvision\\n\\nimport pandas as pd', '# The coarse network structure is dicated by the Fashion MNIST dataset. \\nnb_inputs  = 28*28\\nnb_hidden  = 100\\nnb_outputs = 10\\n\\ntime_step = 1e-3\\nnb_steps  = 100\\n\\nbatch_size = 256', 'dtype = torch.float\\n\\n# Check whether a GPU is available\\nif torch.cuda.is_available():\\n    device = torch.device(\"cuda\")\\n    print(\"cuda\")     \\nelse:\\n    device = torch.device(\"cpu\")\\n    print(\"cpu\")', '# Here we load the Dataset\\nroot = os.path.expanduser(\"~/data/datasets/torch/fashion-mnist\")\\ntrain_dataset = torchvision.datasets.FashionMNIST(root, train=True, transform=None, target_transform=None, download=True)\\ntest_dataset = torchvision.datasets.FashionMNIST(root, train=False, transform=None, target_transform=None, download=True)', '# Standardize data\\n#x_train = torch.tensor(train_dataset.train_data, device=device, dtype=dtype)\\nx_train = np.array(train_dataset.data, dtype=np.float)\\nx_train = x_train.reshape(x_train.shape[0],-1)/255\\n#x_test = torch.tensor(test_dataset.test_data, device=device, ...\ncount  10000.0  10000.0  10000.000000  ...  10000.000000  10000.000000  10000.000000\nmean       1.0      1.0      1.003667  ...      1.512261      1.232128      1.032410\nstd        0.0      0.0      0.239659  ...      5.421710      4.847282      2.899793\nmin        1.0      1.0      1.000000  ...      1.000000      1.000000      1.000000\n25%        1.0      1.0      1.000000  ...      1.000000      1.000000      1.000000\n50%        1.0      1.0      1.000000  ...      1.000000      1.000000      1.000000\n75%        1.0      1.0      1.000000  ...      1.000000      1.000000      1.000000\nmax        1.0      1.0     21.217439  ...    290.173165    290.173165    290.173165\n\n[8 rows x 784 columns]}, '_dh': ['/content'], '_sh': <module 'IPython.core.shadowns' from '/usr/local/lib/python3.7/dist-packages/IPython/core/shadowns.py'>, 'In': ['', 'import os\\n\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\nfrom matplotlib.gridspec import GridSpec\\nimport seaborn as sns\\n\\nimport torch\\nimport torch.nn as nn\\nimport torchvision\\n\\nimport pandas as pd', '# The coarse network structure is dicated by the Fashion MNIST dataset. \\nnb_inputs  = 28*28\\nnb_hidden  = 100\\nnb_outputs = 10\\n\\ntime_step = 1e-3\\nnb_steps  = 100\\n\\nbatch_size = 256', 'dtype = torch.float\\n\\n# Check whether a GPU is available\\nif torch.cuda.is_available():\\n    device = torch.device(\"cuda\")\\n    print(\"cuda\")     \\nelse:\\n    device = torch.device(\"cpu\")\\n    print(\"cpu\")', '# Here we load the Dataset\\nroot = os.path.expanduser(\"~/data/datasets/torch/fashion-mnist\")\\ntrain_dataset = torchvision.datasets.FashionMNIST(root, train=True, transform=None, target_transform=None, download=True)\\ntest_dataset = torchvision.datasets.FashionMNIST(root, train=False, transform=None, target_transform=None, download=True)', '# Standardize data\\n#x_train = torch.tensor(train_dataset.train_data, device=device, dtype=dtype)\\nx_train = np.array(train_dataset.data, dtype=np.float)\\nx_train = x_train.reshape(x_train.shape[0],-1)/255\\n#x_test = torch.tensor(test_dataset.test_data, device=device, dtype=dtype)\\nx_test = np.array(test_dataset.data, dtype=np.float)\\nx_test = x_test.reshape(x_test.shap...\ncount  10000.0  10000.0  10000.000000  ...  10000.000000  10000.000000  10000.000000\nmean       1.0      1.0      1.003667  ...      1.512261      1.232128      1.032410\nstd        0.0      0.0      0.239659  ...      5.421710      4.847282      2.899793\nmin        1.0      1.0      1.000000  ...      1.000000      1.000000      1.000000\n25%        1.0      1.0      1.000000  ...      1.000000      1.000000      1.000000\n50%        1.0      1.0      1.000000  ...      1.000000      1.000000      1.000000\n75%        1.0      1.0      1.000000  ...      1.000000      1.000000      1.000000\nmax        1.0      1.0     21.217439  ...    290.173165    290.173165    290.173165\n\n[8 rows x 784 columns]}, 'get_ipython': <bound method InteractiveShell.get_ipython of <google.colab._shell.Shell object at 0x7fa1ec099750>>, 'exit': <IPython.core.autocall.ZMQExitAutocall object at 0x7fa1e7b79a50>, 'quit': <IPython.core.autocall.ZMQExitAutocall object at 0x7fa1e7b79a50>, '_':            0        1             2    ...           781           782           783\ncount  10000.0  10000.0  10000.000000  ...  10000.000000  10000.000000  10000.000000\nmean       1.0      1.0      1.003667  ...      1.512261      1.232128      1.032410\nstd        0.0      0.0      0.239659  ...      5.421710      4.847282      2.899793\nmin        1.0      1.0      1.000000  ...      1.000000      1.000000      1.000000\n25%        1.0      1.0      1.000000  ...      1.000000      1.000000      1.000000\n50%        1.0      1.0      1.000000  ...      1.000000      1.000000      1.000000\n75%        1.0      1.0      1.000000  ...      1.000000      1.000000      1.000000\nmax        1.0      1.0     21.217439  ...    290.173165    290.173165    290.173165\n\n[8 rows x 784 columns], '__': (-0.5, 27.5, 27.5, -0.5), '___': (60000,), '_i': '#@mpc.run_multiprocess(world_size=2)\\ndef compute_classification_accuracy_test_2(x_data, y_data):\\n    \"\"\" Computes classification accuracy on supplied data in batches. \"\"\"\\n\\n    w1_enc=crypten.cryptensor(w1,src=0)\\n    w2_enc=crypten.cryptensor(w2,src=0)\\n    #print(w1_enc.device)\\n\\n    #print(w2_enc.device)\\n\\n    \\n    \\n    accs = []\\n    \\n    for x_local, y_local in sparse_data_generator(x_data, y_data, batch_size, nb_steps, nb_inputs, shuffle=False):\\n        \\n        x_data_enc=crypten.cryptensor(x_local.to_dense())\\n        #print(x_data_enc.device)\\n        for i in range(batch_size):\\n          batch_am=[]\\n          output_enc,_ = run_snn_test_2(x_data_enc[i],w1_enc,w2_enc)\\n          #print(output_enc.size())\\n          output=output_enc.get_plain_text()\\n          m,_= torch.max(output,1) # max over time\\n          #print(m.size())\\n          _,am=torch.max(m,0)# argmax over output units\\n          #print(am)\\n          batch_am.append(am) \\n        batch_am_stack=torch.stack(batch_am)       \\n        tmp = np.mean((y_local==batch_am_stack).detach().cpu().numpy()) # compare to labels\\n        accs.append(tmp)\\n        print(tmp)\\n          \\n    return np.mean(accs)', '_ii': 'def run_snn_test_2(inputs,w1_enc,w2_enc):\\n    #h1 = torch.einsum(\"abc,cd->abd\", (inputs, w1))\\n    syn = torch.zeros((nb_hidden), device=device, dtype=dtype)\\n    mem = torch.zeros((nb_hidden), device=dev...\n    Number of datapoints: 60000\n    Root location: /root/data/datasets/torch/fashion-mnist\n    Split: Train, 'test_dataset': Dataset FashionMNIST\n    Number of datapoints: 10000\n    Root location: /root/data/datasets/torch/fashion-mnist\n    Split: Test, '_i5': '# Standardize data\\n#x_train = torch.tensor(train_dataset.train_data, device=device, dtype=dtype)\\nx_train = np.array(train_dataset.data, dtype=np.float)\\nx_train = x_train.reshape(x_train.shape[0],-1)/255\\n#x_test = torch.tensor(test_dataset.test_data, device=device, dtype=dtype)\\nx_test = np.array(test_dataset.data, dtype=np.float)\\nx_test = x_test.reshape(x_test.shape[0],-1)/255\\n\\n#y_train = torch.tensor(train_dataset.train_labels, device=device, dtype=dtype)\\n#y_test  = torch.tensor(test_dataset.test_labels, device=device, dtype=dtype)\\ny_train = np.array(train_dataset.targets, dtype=np.int)\\ny_test  = np.array(test_dataset.targets, dtype=np.int)\\n\\ny_train.shape', 'x_train': array([[0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       ...,\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.]]), 'x_test': array([[0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       ...,\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.]]), 'y_train': array([9, 0, 0, ..., 3, 0, 5]), 'y_test': array([9, 2, 1, ..., 8, 1, 5]), '_5': (60000,), '_i6': '# Here we plot one of the raw data points as an example\\ndata_id = 1\\nplt.imshow(x_train[data_id].reshape(28,28), cmap=plt.cm.gray_r)\\nplt.axis(\"off\")', 'data_id': 1, '_6': (-0.5, 27.5, 27.5, -0.5), '_i7': 'def current2firing_time(x, tau=20, thr=0.2, tmax=1.0, epsilon=1e-7):\\n    \"\"\" Computes first firing time latency for a current input x assuming the charge time of a current based LIF neuron.\\n\\n    Args:\\n    x -- The \"current\" values\\n\\n    Keyword args:\\n    tau -- The membrane time constant of the LIF neuron to be charged\\n    thr -- The firing threshold value \\n    tmax -- The maximum time returned \\n    epsilon -- A generic (small) epsilon > 0\\n\\n    Returns:\\n    Time to first spike for each \"current\" x\\n    \"\"\"\\n    idx = x<thr\\n    x = np.clip(x,thr+epsilon,1e9)\\n    T = tau*np.log(x/(x-thr))\\n    # T in [1.5,109]\\n    T[idx] = tmax\\n\\n    return T\\n \\n\\ndef sparse_data_generator(X, y, batch_size, nb_steps, nb_units, shuffle=True ):\\n    \"\"\" This generator takes datasets in analog format and generates spiking network input as sparse tensors. \\n\\n    Args:\\n        X: The data ( sample x event x 2 ) the last dim holds (time,neuron) tuples\\n        y: The labels\\n        nb_steps: number of time steps\\n        nb_units: 784 (28x28)\\n    \"\"\"\\n\\n    labels_ = np.array(y,dtype=np.int)\\n    number_of_batches = len(X)//bat...\n       [1., 1., 1., ..., 1., 1., 1.],\n       [1., 1., 1., ..., 1., 1., 1.],\n       ...,\n       [1., 1., 1., ..., 1., 1., 1.],\n       [1., 1., 1., ..., 1., 1., 1.],\n       [1., 1., 1., ..., 1., 1., 1.]]), 'df':       0    1    2    3    4    ...       779        780  781  782  783\n0     1.0  1.0  1.0  1.0  1.0  ...  1.000000   1.000000  1.0  1.0  1.0\n1     1.0  1.0  1.0  1.0  1.0  ...  6.289867  28.642078  1.0  1.0  1.0\n2     1.0  1.0  1.0  1.0  1.0  ...  1.000000   1.000000  1.0  1.0  1.0\n3     1.0  1.0  1.0  1.0  1.0  ...  1.000000   1.000000  1.0  1.0  1.0\n4     1.0  1.0  1.0  1.0  1.0  ...  1.000000   1.000000  1.0  1.0  1.0\n...   ...  ...  ...  ...  ...  ...       ...        ...  ...  ...  ...\n9995  1.0  1.0  1.0  1.0  1.0  ...  1.000000   1.000000  1.0  1.0  1.0\n9996  1.0  1.0  1.0  1.0  1.0  ...  1.000000   1.000000  1.0  1.0  1.0\n9997  1.0  1.0  1.0  1.0  1.0  ...  1.000000   1.000000  1.0  1.0  1.0\n9998  1.0  1.0  1.0  1.0  1.0  ...  1.000000   1.000000  1.0  1.0  1.0\n9999  1.0  1.0  1.0  1.0  1.0  ...  1.000000   1.000000  1.0  1.0  1.0\n\n[10000 rows x 784 columns], '_8':            0        1             2    ...           781           782           783\ncount  10000.0  10000.0  10000.000000  ...  10000.000000  10000.000000  10000.000000\nmean       1.0      1.0      1.003667  ...      1.512261      1.232128      1.032410\nstd        0.0      0.0      0.239659  ...      5.421710      4.847282      2.899793\nmin        1.0      1.0      1.000000  ...      1.000000      1.000000      1.000000\n25%        1.0      1.0      1.000000  ...      1.000000      1.000000      1.000000\n50%        1.0      1.0      1.000000  ...      1.000000      1.000000      1.000000\n75%        1.0      1.0      1.000000  ...      1.000000      1.000000      1.000000\nmax        1.0      1.0     21.217439  ...    290.173165    290.173165    290.173165\n\n[8 rows x 784 columns], '_i9': 'tau_mem = 10e-3\\ntau_syn = 5e-3\\n\\nalpha   = float(np.exp(-time_step/tau_syn))\\nbeta    = float(np.exp(-time_step/tau_mem))', 'tau_mem': 0.01, 'tau_syn': 0.005, 'alpha': 0.8187307530779818, 'beta': 0.9048374180359595, '_i10': 'weight_scale = 7*(1.0-beta) # this should give us some spikes to begin with\\n\\nw1 = torch.empty((nb_inputs, nb_hidden),  device=device, dtype=dtype, requires_grad=True)\\ntorch.nn.init.normal_(w1, mean=0.0, std=weight_scale/np.sqrt(nb_inputs))\\n\\nw2 = torch.empty((nb_hidden, nb_outputs), device=device, dtype=dtype, requires_grad=True)\\ntorch.nn.init.normal_(w2, mean=0.0, std=weight_scale/np.sqrt(nb_hidden))\\n\\nprint(\"init done\")', 'weight_scale': 0.6661380737482834, 'w1': tensor([[ 0.0317, -0.0074, -0.0046,  ..., -0.0084,  0.0029,  0.0114],\n        [-0.0092, -0.0094,  0.0259,  ..., -0.0285, -0.0183,  0.0310],\n        [-0.0434, -0.0603,  0.0610,  ..., -0.0179, -0.0341,  0.0463],\n        ...,\n        [-0.0010,  0.0624,  0.0427,  ..., -0.0107,  0.0160,  0.0125],\n        [ 0.0103,  0.0134,  0.0044,  ...,  0.0494,  0.0418,  0.0405],\n        [-0.0149, -0.0155,  0.0498,  ..., -0.0137,  0.0195,  0.0248]],\n       device='cuda:0', requires_grad=True), 'w2': tensor([[-0.1986,  0.1029,  0.2804, -0.1246,  0.0824,  0.0441, -0.1294, -0.0443,\n         -0.0726, -0.1272],\n        [-0.0771,  0.0405, -0.2326, -0.0589,  0.0478,  0.0490,  0.0410, -0.0368,\n         -0.0633,  0.2318],\n        [-0.0271, -0.1260, -0.0960,  0.2241, -0.0701,  0.0135,  0.2543, -0.0294,\n         -0.1198,  0.1058],\n        [ 0.1433, -0.0080, -0.0431, -0.0273, -0.0404, -0.0974,  0.0245,  0.1444,\n         -0.1474, -0.0234],\n        [-0.1444, -0.1040, -0.0937, -0.1058, -0.1024,  0.1221,  0.1276, -0.2422,\n          0.0444, -0.0340],\n        [ 0.1768,  0.0072, -0.0699, -0.0142, -0.2202,  0.3443,  0.0768, -0.0694,\n         -0.1682, -0.1279],\n        [-0.0887, -0.0579, -0.0455, -0.0285, -0.1044,  0.1477,  0.1423, -0.1033,\n         -0.3140,  0.1165],\n        [ 0.0872, -0.0269, -0.0572, -0.3258,  0.4544, -0.1016, -0.1280, -0.1004,\n         -0.1291, -0.1475],\n        [ 0.0967, -0.1322, -0.2227,  0.0672, -0.2587, -0.1005,  0.1743, -0.2541,\n         -0.0507, -0.0171],\n        [-0.0974, -0.1763,  0.0565, -0.1535,  0.0510,  0.1089, -0.1275,  0.1967,\n          0.0273,  0.0076],\n        [-0.0054,  0.2067, -0.0130, -0.1927, -0.0339,  0.1099, -0.0554, -0.0558,\n          0.2462, -0.2266],\n        [ 0.1419,  0.1212, -0.1839, -0.0392, -0.0719,  0.2055, -0.0313,  0.0519,\n         -0.1927, -0.0945],\n        [ 0.0581,  0.1463,  0.0111,  0.1325,  0.0344, -0.1576,  0.0500, -0.0229,\n         -0.1456, -0.1273],\n        [-0.1039, -0.0564, -0.1265,  0.1856,  0.0780,  0.1636, -0.1334,  0.0462,\n          0.0505, -0.1597],\n        [-0.0236, -0.0816, -0.0868,  0.2333, -0.1087,  0.2394, -0.1062, -0.1765,\n          0.0720, -0.1706],\n        [ 0.0278, -0.0397, -0.0354,  0.1376, -0.1149, -0.0461,  0.1464,  0.1325,\n         -0.0953,  0.0613],\n        [-0.1972,  0.1387, -0.1392,  0.1178,  0.0868,  0.0254, -0.1908, -0.0320,\n          0.1167, -0.1479],\n        [ 0.2684, -0.2125,  0.0666, -0.2469, -0.0369, -0.0407, -0.0222, -0.0328,\n          0.2151, -0.0287],\n        [ 0.0713, -0.1425,  0.0958,  0.1002, -0.0682,  0.0389, -0.1760, -0.0601,\n         -0.0703, -0.0676],\n        [ 0.0543,  0.0272, -0.0667, -0.0356,  0.0596, -0.2005, -0.0784, -0.0095,\n          0.1375,  0.0388],\n        [-0.0628,  0.1673, -0.0719, -0.0671,  0.0424, -0.1292, -0.0813,  0.0453,\n          0.0261,  0.1298],\n        [-0.1374, -0.1551, -0.1111, -0.0494,  0.0023, -0.1994,  0.3621, -0.0354,\n         -0.0389, -0.0478],\n        [ 0.1390, -0.0687, -0.0837,  0.0216,  0.0427,  0.1538,  0.1128, -0.0244,\n          0.1929, -0.0919],\n        [-0.1191, -0.2929, -0.1157,  0.1735,  0.1025, -0.0301,  0.0198, -0.1147,\n         -0.0497,  0.1266],\n        [ 0.0279, -0.1403, -0.1298,  0.0843, -0.0258, -0.0661, -0.0360, -0.0266,\n          0.3058, -0.1369],\n        [-0.1231,  0.1459, -0.0190,  0.0684, -0.0443, -0.0512, -0.1146,  0.1129,\n          0.0310,  0.0770],\n        [ 0.1087, -0.1419,  0.0297,  0.1609,  0.0516, -0.1239,  0.0241, -0.1560,\n          0.0375, -0.0590],\n        [-0.1057, -0.1465, -0.0808, -0.0159,  0.1380,  0.1348,  0.0731, -0.2030,\n         -0.0269, -0.0436],\n        [-0.1209,  0.0519,  0.2338, -0.0355, -0.2017, -0.0655,  0.0617,  0.2159,\n         -0.0340, -0.1140],\n        [ 0.0881, -0.0312, -0.1829, -0.1073, -0.0492, -0.1133,  0.0604,  0.3560,\n         -0.0958, -0.1451],\n        [ 0.1689, -0.0585,  0.0377, -0.1067,  0.1568, -0.2171, -0.0846,  0.1517,\n         -0.1095, -0.0404],\n        [ 0.1264, -0.1758, -0.0290, -0.0295, -0.3027,  0.1804,  0.1851,  0.0719,\n          0.1475, -0.1482],\n        [-0.0723, -0.1512, -0.1220, -0.0515, -0.0859, -0.0388,  0.0073, -0.0033,\n         -0.1413, -0.0705],\n        [-0.0528, -0.1508,  0.1466,  0.0229,  0.1729,  0.0838,  0.0931,  0.0381,\n         -0.2061, -0.1147],\n        [ 0.0147,  0.1958, -0.1150, -0.0016, -0.0463,  0.0309,  0.1752,  0.1266,\n         -0.2362, -0.1775],\n        [ 0.0137,  0.0147,  0.0932,  0.0671, -0.0242, -0.0685,  0.1872, -0.1139,\n          0.0881, -0.0808],\n        [ 0.0221,  0.0565,  0.0913, -0.0263,  0.0423, -0.1889,  0.1201, -0.0057,\n          0.1113,  0.0606],\n        [-0.0235,  0.0289, -0.1385,  0.1174, -0.1337, -0.1040, -0.0968,  0.1846,\n          0.2059,  0.0364],\n        [ 0.0109,  0.0159, -0.1398, -0.0025,  0.0988, -0.0477, -0.1744, -0.0472,\n          0.1052, -0.0528],\n        [ 0.0478, -0.1337,  0.1342, -0.0857,  0.0118, -0.1359, -0.2749,  0.2911,\n         -0.1627, -0.1562],\n        [ 0.0027,  0.0250,  0.0668, -0.0811,  0.0021,  0.0919,  0.0759,  0.0816,\n          0.0749,  0.0301],\n        [ 0.1854, -0.3053, -0.1311,  0.0279, -0.0955, -0.0176,  0.0727,  0.1239,\n          0.1995,  0.0237],\n        [ 0.1766,  0.0107,  0.1013, -0.1957, -0.0211, -0.0329,  0.1159,  0.0534,\n         -0.0885, -0.1707],\n        [ 0.1058, -0.0067,  0.0040,  0.0398, -0.1178,  0.1276,  0.0227,  0.0418,\n          0.0446,  0.0754],\n        [ 0.0682,  0.1378,  0.1058, -0.0457, -0.0081,  0.1124, -0.1298, -0.2169,\n         -0.1177, -0.1897],\n        [-0.0680,  0.0818,  0.0551,  0.0043, -0.1539, -0.2233, -0.0729,  0.1038,\n          0.0728, -0.0083],\n        [-0.1157, -0.1106, -0.0348,  0.3660,  0.0032, -0.0375, -0.0992,  0.0456,\n         -0.0838, -0.0676],\n        [ 0.0697, -0.1788,  0.3058,  0.1759, -0.1803, -0.0506, -0.1526, -0.0622,\n         -0.1908, -0.0884],\n        [ 0.1225,  0.0300,  0.0244,  0.0336,  0.0542, -0.1572,  0.0780,  0.0010,\n         -0.0970,  0.0749],\n        [-0.1630, -0.0252,  0.3333,  0.1130, -0.1956, -0.2423, -0.0111, -0.0027,\n         -0.0362,  0.1799],\n        [-0.0332,  0.0205, -0.1079, -0.1339,  0.1632, -0.0017, -0.0008, -0.0537,\n          0.0314,  0.1506],\n        [-0.0584, -0.0711,  0.0525,  0.0371,  0.0290,  0.0778, -0.0537,  0.0853,\n          0.0786, -0.0170],\n        [ 0.1132, -0.0997,  0.1461, -0.1337, -0.0739, -0.1184,  0.0796, -0.1403,\n          0.0315, -0.0427],\n        [ 0.0877,  0.0623, -0.0632, -0.1174,  0.0966, -0.1611,  0.0186,  0.0303,\n          0.1657, -0.0290],\n        [-0.0759,  0.1489,  0.0753, -0.0110, -0.0232,  0.0845,  0.0591, -0.1456,\n         -0.0223, -0.0675],\n        [-0.0475,  0.1977, -0.0568,  0.1006, -0.2473, -0.0866,  0.0106, -0.0184,\n          0.1187, -0.0346],\n        [-0.0820,  0.0868,  0.0773,  0.0904,  0.1253, -0.1189,  0.0369, -0.0630,\n         -0.0879, -0.0819],\n        [-0.1307,  0.2978, -0.1887, -0.1105,  0.1231,  0.0588, -0.0661, -0.1602,\n         -0.1989, -0.1269],\n        [ 0.0984, -0.0740,  0.0468, -0.2023, -0.1810,  0.1684,  0.0628, -0.0212,\n         -0.1441,  0.0046],\n        [-0.0171, -0.0609,  0.0694, -0.1318,  0.1043, -0.0652,  0.0750,  0.0592,\n          0.0539, -0.1655],\n        [-0.0831,  0.2221,  0.1359,  0.0724,  0.0781,  0.0715, -0.2118, -0.1426,\n         -0.0074, -0.0306],\n        [-0.0550,  0.1327, -0.1862, -0.1174, -0.1535, -0.2597, -0.0519,  0.1466,\n          0.0276,  0.2557],\n        [ 0.0750, -0.0354, -0.0103, -0.0550,  0.0094, -0.0045,  0.0935,  0.0133,\n         -0.0058, -0.0526],\n        [ 0.0909,  0.2230, -0.0917,  0.1488, -0.0075,  0.0181, -0.0332, -0.1026,\n         -0.1667, -0.0719],\n        [ 0.0230, -0.0966,  0.0681, -0.2221, -0.0749,  0.1228, -0.0590, -0.0878,\n         -0.2455,  0.1010],\n        [-0.1732,  0.0134,  0.0947,  0.1239, -0.1481,  0.2219, -0.0239, -0.2041,\n          0.1625, -0.0578],\n        [-0.0044, -0.1028, -0.1034, -0.0580,  0.3344, -0.0338, -0.0872,  0.1171,\n         -0.0975, -0.2122],\n        [ 0.0647, -0.0044,  0.0661,  0.1275,  0.0718, -0.1133, -0.0393, -0.1307,\n          0.0262,  0.0067],\n        [ 0.0084, -0.0591,  0.1222, -0.0194,  0.1739,  0.0586, -0.1299, -0.0604,\n          0.1219, -0.2655],\n        [-0.0189, -0.0400,  0.0212,  0.0866,  0.0961,  0.2202,  0.0264,  0.0176,\n          0.0143,  0.0424],\n        [ 0.1166, -0.1352,  0.2252, -0.0949, -0.0639,  0.1543, -0.1096, -0.1480,\n         -0.0601, -0.0023],\n        [-0.0282,  0.1385,  0.1827,  0.0920, -0.0596,  0.0841, -0.0673,  0.0702,\n         -0.1045,  0.1696],\n        [-0.0789, -0.0854, -0.0253, -0.0179,  0.1091, -0.1932,  0.2328, -0.1380,\n         -0.3146, -0.0337],\n        [ 0.0275, -0.0505,  0.0420, -0.0873,  0.0222, -0.2003,  0.0676, -0.0591,\n         -0.0684, -0.1267],\n        [ 0.1602,  0.0662, -0.0210, -0.1054,  0.1088, -0.0827,  0.0719,  0.1331,\n         -0.0789,  0.0448],\n        [-0.0628, -0.1487, -0.1589,  0.0571,  0.0401, -0.1933, -0.1505,  0.2391,\n          0.0094,  0.0839],\n        [ 0.0052, -0.1301, -0.0514,  0.0077,  0.0713, -0.0794,  0.0172, -0.0529,\n         -0.0365,  0.0935],\n        [-0.1054, -0.1724, -0.0813,  0.0056, -0.1422, -0.2807, -0.0501,  0.0947,\n          0.3366, -0.1593],\n        [ 0.1211,  0.1327,  0.1332, -0.0148, -0.0580,  0.0146, -0.0060, -0.0330,\n         -0.3109, -0.0572],\n        [ 0.0048, -0.1166, -0.0625, -0.0788,  0.0642,  0.3513,  0.1839, -0.0978,\n         -0.0079, -0.1909],\n        [ 0.0941, -0.1040,  0.2028,  0.0221, -0.0503,  0.0050, -0.1797, -0.1263,\n         -0.1084,  0.0334],\n        [-0.1489, -0.1613, -0.1127, -0.0964, -0.0460, -0.1206,  0.1914,  0.0107,\n          0.1277, -0.0326],\n        [-0.1679, -0.0056,  0.0693, -0.0354,  0.1590, -0.0951,  0.0225, -0.0171,\n          0.2011, -0.0967],\n        [-0.0426, -0.1441, -0.0676, -0.0986, -0.0863,  0.3538, -0.0219, -0.1838,\n          0.0792, -0.1597],\n        [-0.0056, -0.0513, -0.0636,  0.0196,  0.0356, -0.0695,  0.2944, -0.0729,\n         -0.0720, -0.0290],\n        [-0.1267,  0.3450, -0.1943,  0.1044,  0.0271,  0.0660, -0.1011,  0.0610,\n          0.0803,  0.1605],\n        [-0.1311,  0.0280, -0.0164, -0.0447,  0.0288, -0.0702,  0.0929, -0.1607,\n          0.1414,  0.1704],\n        [ 0.0570,  0.0851,  0.0618, -0.1352,  0.1811, -0.0506, -0.0435, -0.1860,\n          0.0746, -0.0197],\n        [ 0.2024, -0.1481,  0.0115, -0.0476, -0.1410, -0.0696, -0.0949,  0.0870,\n         -0.0231,  0.0101],\n        [ 0.0962,  0.1031,  0.1220,  0.0736,  0.0366, -0.0543, -0.0928, -0.1079,\n          0.1275, -0.0523],\n        [-0.0869, -0.0895,  0.0215,  0.0966, -0.2076, -0.1393, -0.1579,  0.1928,\n         -0.2106, -0.0879],\n        [ 0.0558, -0.0662,  0.0213, -0.1372, -0.1480,  0.3953,  0.1189, -0.2241,\n         -0.0923, -0.1733],\n        [ 0.3835, -0.0052, -0.0122,  0.0485, -0.1948, -0.1183, -0.1600, -0.1797,\n         -0.1680, -0.0013],\n        [-0.0765, -0.1223,  0.1224,  0.2189, -0.1172, -0.0610,  0.0329,  0.2054,\n         -0.1402, -0.0905],\n        [ 0.0771, -0.0799, -0.1385, -0.0524,  0.0225,  0.3306,  0.1590, -0.2098,\n         -0.1429, -0.1242],\n        [-0.0827, -0.1647, -0.0895,  0.0197, -0.0412, -0.0297,  0.2319,  0.0133,\n         -0.2460,  0.0670],\n        [-0.0944, -0.1014, -0.0261, -0.0015, -0.0615, -0.1928, -0.1582,  0.0055,\n          0.2352,  0.0826],\n        [ 0.0535, -0.0082, -0.1263, -0.1443, -0.1309, -0.0855, -0.0126,  0.0866,\n          0.0490,  0.1650],\n        [ 0.0274, -0.0824,  0.1809,  0.1854, -0.1224, -0.1543, -0.1143, -0.0405,\n         -0.0611, -0.0475],\n        [ 0.1210, -0.0325, -0.1075, -0.1316, -0.1749,  0.1951, -0.1079,  0.0284,\n         -0.0043,  0.0729]], device='cuda:0', requires_grad=True), '_i11': 'def plot_voltage_traces(mem, spk=None, dim=(3,5), spike_height=5):\\n    gs=GridSpec(*dim)\\n    if spk is not None:\\n        dat = 1.0*mem\\n        dat[spk>0.0] = spike_height\\n        dat = dat.detach().cpu().numpy()\\n    else:\\n        dat = mem.detach().cpu().numpy()\\n    for i in range(np.prod(dim)):\\n        if i==0: a0=ax=plt.subplot(gs[i])\\n        else: ax=plt.subplot(gs[i],sharey=a0)\\n        ax.plot(dat[i])\\n        ax.axis(\"off\")', 'plot_voltage_traces': <function plot_voltage_traces at 0x7fa0d003a050>, '_i12': 'class SurrGradSpike(torch.autograd.Function):\\n    \"\"\"\\n    Here we implement our spiking nonlinearity which also implements \\n    the surrogate gradient. By subclassing torch.autograd.Function, \\n    we will be able to use all of PyTorch\\'s autograd functionality.\\n    Here we use the normalized negative part of a fast sigmoid \\n    as this was done in Zenke & Ganguli (2018).\\n    \"\"\"\\n    \\n    scale = 100.0 # controls steepness of surrogate gradient\\n\\n    @staticmethod\\n    def forward(ctx, input):\\n        \"\"\"\\n        In the forward pass we compute a step function of the input Tensor\\n        and return it. ctx is a context object that we use to stash information which \\n        we need to later backpropagate our error signals. To achieve this we use the \\n        ctx.save_for_backward method.\\n        \"\"\"\\n        ctx.save_for_backward(input)\\n        out = torch.zeros_like...\n\u001b[1;32m   2082\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2083\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<decorator-gen-53>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self=<IPython.core.magics.execution.ExecutionMagics object>, line='a=compute_classification_accuracy_test_2(x_test,y_test)', cell=None, local_ns={'ALICE': 0, 'BOB': 1, 'GridSpec': <class 'matplotlib.gridspec.GridSpec'>, 'In': ['', 'import os\\n\\nimport numpy as np\\nimport matplotlib.....nn as nn\\nimport torchvision\\n\\nimport pandas as pd', '# The coarse network structure is dicated by the...ime_step = 1e-3\\nnb_steps  = 100\\n\\nbatch_size = 256', 'dtype = torch.float\\n\\n# Check whether a GPU is av...    device = torch.device(\"cpu\")\\n    print(\"cpu\")', '# Here we load the Dataset\\nroot = os.path.expand...sform=None, target_transform=None, download=True)', '# Standardize data\\n#x_train = torch.tensor(train...est_dataset.targets, dtype=np.int)\\n\\ny_train.shape', '# Here we plot one of the raw data points as an ...shape(28,28), cmap=plt.cm.gray_r)\\nplt.axis(\"off\")', 'def current2firing_time(x, tau=20, thr=0.2, tmax..., y_batch.to(device=device)\\n\\n        counter += 1', 'T=current2firing_time(x_test)\\ndf = pd.DataFrame(T)\\ndf.describe()', 'tau_mem = 10e-3\\ntau_syn = 5e-3\\n\\nalpha   = float(...syn))\\nbeta    = float(np.exp(-time_step/tau_mem))', 'weight_scale = 7*(1.0-beta) # this should give u...ght_scale/np.sqrt(nb_hidden))\\n\\nprint(\"init done\")', 'def plot_voltage_traces(mem, spk=None, dim=(3,5)...0)\\n        ax.plot(dat[i])\\n        ax.axis(\"off\")', 'class SurrGradSpike(torch.autograd.Function):\\n  ...urrogate gradient\\nspike...\n\n[8 rows x 784 columns]}, 'SurrGradSpike': <class '__main__.SurrGradSpike'>, 'T': array([[1., 1., 1., ..., 1., 1., 1.],\n       [1...., 1., 1.],\n       [1., 1., 1., ..., 1., 1., 1.]]), '_':            0        1             2    ...      ... 290.173165    290.173165\n\n[8 rows x 784 columns], '_5': (60000,), '_6': (-0.5, 27.5, 27.5, -0.5), ...})\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f=<function ExecutionMagics.time>, *a=(<IPython.core.magics.execution.ExecutionMagics object>, 'a=compute_classification_accuracy_test_2(x_test,y_test)', None, {'ALICE': 0, 'BOB': 1, 'GridSpec': <class 'matplotlib.gridspec.GridSpec'>, 'In': ['', 'import os\\n\\nimport numpy as np\\nimport matplotlib.....nn as nn\\nimport torchvision\\n\\nimport pandas as pd', '# The coarse network structure is dicated by the...ime_step = 1e-3\\nnb_steps  = 100\\n\\nbatch_size = 256', 'dtype = torch.float\\n\\n# Check whether a GPU is av...    device = torch.device(\"cpu\")\\n    print(\"cpu\")', '# Here we load the Dataset\\nroot = os.path.expand...sform=None, target_transform=None, download=True)', '# Standardize data\\n#x_train = torch.tensor(train...est_dataset.targets, dtype=np.int)\\n\\ny_train.shape', '# Here we plot one of the raw data points as an ...shape(28,28), cmap=plt.cm.gray_r)\\nplt.axis(\"off\")', 'def current2firing_time(x, tau=20, thr=0.2, tmax..., y_batch.to(device=device)\\n\\n        counter += 1', 'T=current2firing_time(x_test)\\ndf = pd.DataFrame(T)\\ndf.describe()', 'tau_mem = 10e-3\\ntau_syn = 5e-3\\n\\nalpha   = float(...syn))\\nbeta    = float(np.exp(-time_step/tau_mem))', 'weight_scale = 7*(1.0-beta) # this should give u...ght_scale/np.sqrt(nb_hidden))\\n\\nprint(\"init done\")', 'def plot_voltage_traces(mem, spk=None, dim=(3,5)...0)\\n        ax.plot(dat[i])\\n        ax.axis(\"off\")', 'class SurrGradS...\n\n[8 rows x 784 columns]}, 'SurrGradSpike': <class '__main__.SurrGradSpike'>, 'T': array([[1., 1., 1., ..., 1., 1., 1.],\n       [1...., 1., 1.],\n       [1., 1., 1., ..., 1., 1., 1.]]), '_':            0        1             2    ...      ... 290.173165    290.173165\n\n[8 rows x 784 columns], '_5': (60000,), '_6': (-0.5, 27.5, 27.5, -0.5), ...}), **k={})\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m        \u001b[0;36mglobal\u001b[0m \u001b[0;36mcall\u001b[0m \u001b[0;34m= \u001b[0;36mundefined\u001b[0m\u001b[0m\u001b[0;34m\n        \u001b[0m\u001b[0;36mf\u001b[0m \u001b[0;34m= <function ExecutionMagics.time at 0x7fa202bdee60>\u001b[0m\u001b[0;34m\n        \u001b[0m\u001b[0;36ma\u001b[0m \u001b[0;34m= (<IPython.core.magics.execution.ExecutionMagics object at 0x7fa1e7b8df50>, 'a=compute_classification_accuracy_test_2(x_test,y_test)', None, {'__name__': '__main__', '__doc__': 'returns spikes', '__package__': None, '__loader__': None, '__spec__': None, '__builtin__': <module 'builtins' (built-in)>, '__builtins__': <module 'builtins' (built-in)>, '_ih': ['', 'import os\\n\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\nfrom matplotlib.gridspec import GridSpec\\nimport seaborn as sns\\n\\nimport torch\\nimport torch.nn as nn\\nimport torchvision\\n\\nimport pandas as pd', '# The coarse network structure is dicated by the Fashion MNIST dataset. \\nnb_inputs  = 28*28\\nnb_hidden  = 100\\nnb_outputs = 10\\n\\ntime_step = 1e-3\\nnb_steps  = 100\\n\\nbatch_size = 256', 'dtype = torch.float\\n\\n# Check whether a GPU is available\\nif torch.cuda.is_available():\\n    device = torch.device(\"cuda\")\\n    print(\"cuda\")     \\nelse:\\n    device = torch.device(\"cpu\")\\n    print(\"cpu\")', '# Here we load the Dataset\\nroot = os.path.expanduser(\"~/data/datasets/torch/fashion-mnist\")\\ntrain_dataset = torchvision.datasets.FashionMNIST(root, train=True, transform=None, target_transform=None, download=True)\\ntest_dataset = torchvision.datasets.FashionMNIST(root, train=False, transform=None, target_transform=None, download=True)', '# Standardize data\\n#x_train = torch.tensor(train_dataset.train_data, device=device, dtype=dtype)\\nx_train = np.array(train_dataset.data, dtype=np....\ncount  10000.0  10000.0  10000.000000  ...  10000.000000  10000.000000  10000.000000\nmean       1.0      1.0      1.003667  ...      1.512261      1.232128      1.032410\nstd        0.0      0.0      0.239659  ...      5.421710      4.847282      2.899793\nmin        1.0      1.0      1.000000  ...      1.000000      1.000000      1.000000\n25%        1.0      1.0      1.000000  ...      1.000000      1.000000      1.000000\n50%        1.0      1.0      1.000000  ...      1.000000      1.000000      1.000000\n75%        1.0      1.0      1.000000  ...      1.000000      1.000000      1.000000\nmax        1.0      1.0     21.217439  ...    290.173165    290.173165    290.173165\n\n[8 rows x 784 columns]}, '_dh': ['/content'], '_sh': <module 'IPython.core.shadowns' from '/usr/local/lib/python3.7/dist-packages/IPython/core/shadowns.py'>, 'In': ['', 'import os\\n\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\nfrom matplotlib.gridspec import GridSpec\\nimport seaborn as sns\\n\\nimport torch\\nimport torch.nn as nn\\nimport torchvision\\n\\nimport pandas as pd', '# The coarse network structure is dicated by the Fashion MNIST dataset. \\nnb_inputs  = 28*28\\nnb_hidden  = 100\\nnb_outputs = 10\\n\\ntime_step = 1e-3\\nnb_steps  = 100\\n\\nbatch_size = 256', 'dtype = torch.float\\n\\n# Check whether a GPU is available\\nif torch.cuda.is_available():\\n    device = torch.device(\"cuda\")\\n    print(\"cuda\")     \\nelse:\\n    device = torch.device(\"cpu\")\\n    print(\"cpu\")', '# Here we load the Dataset\\nroot = os.path.expanduser(\"~/data/datasets/torch/fashion-mnist\")\\ntrain_dataset = torchvision.datasets.FashionMNIST(root, train=True, transform=None, target_transform=None, download=True)\\ntest_dataset = torchvision.datasets.FashionMNIST(root, train=False, transform=None, target_transform=None, download=True)', '# Standardize data\\n#x_train = torch.tensor(train_dataset.train_data, device=device, dtype=dtype)\\nx_train = np.array(train_dataset.data, dtype=np.float)\\nx_train = x_train.reshape(x_train.shape[0],-1)/255\\n#x_test = torch.tensor(test_dataset.test_data, device=device, dtype=dtype)\\nx_test = np.array(test_dataset.data, dtype=np.float)\\nx_test = x_test.reshape(x_test.shap...\ncount  10000.0  10000.0  10000.000000  ...  10000.000000  10000.000000  10000.000000\nmean       1.0      1.0      1.003667  ...      1.512261      1.232128      1.032410\nstd        0.0      0.0      0.239659  ...      5.421710      4.847282      2.899793\nmin        1.0      1.0      1.000000  ...      1.000000      1.000000      1.000000\n25%        1.0      1.0      1.000000  ...      1.000000      1.000000      1.000000\n50%        1.0      1.0      1.000000  ...      1.000000      1.000000      1.000000\n75%        1.0      1.0      1.000000  ...      1.000000      1.000000      1.000000\nmax        1.0      1.0     21.217439  ...    290.173165    290.173165    290.173165\n\n[8 rows x 784 columns]}, 'get_ipython': <bound method InteractiveShell.get_ipython of <google.colab._shell.Shell object at 0x7fa1ec099750>>, 'exit': <IPython.core.autocall.ZMQExitAutocall object at 0x7fa1e7b79a50>, 'quit': <IPython.core.autocall.ZMQExitAutocall object at 0x7fa1e7b79a50>, '_':            0        1             2    ...           781           782           783\ncount  10000.0  10000.0  10000.000000  ...  10000.000000  10000.000000  10000.000000\nmean       1.0      1.0      1.003667  ...      1.512261      1.232128      1.032410\nstd        0.0      0.0      0.239659  ...      5.421710      4.847282      2.899793\nmin        1.0      1.0      1.000000  ...      1.000000      1.000000      1.000000\n25%        1.0      1.0      1.000000  ...      1.000000      1.000000      1.000000\n50%        1.0      1.0      1.000000  ...      1.000000      1.000000      1.000000\n75%        1.0      1.0      1.000000  ...      1.000000      1.000000      1.000000\nmax        1.0      1.0     21.217439  ...    290.173165    290.173165    290.173165\n\n[8 rows x 784 columns], '__': (-0.5, 27.5, 27.5, -0.5), '___': (60000,), '_i': '#@mpc.run_multiprocess(world_size=2)\\ndef compute_classification_accuracy_test_2(x_data, y_data):\\n    \"\"\" Computes classification accuracy on supplied data in batches. \"\"\"\\n\\n    w1_enc=crypten.cryptensor(w1,src=0)\\n    w2_enc=crypten.cryptensor(w2,src=0)\\n    #print(w1_enc.device)\\n\\n    #print(w2_enc.device)\\n\\n    \\n    \\n    accs = []\\n    \\n    for x_local, y_local in sparse_data_generator(x_data, y_data, batch_size, nb_steps, nb_inputs, shuffle=False):\\n        \\n        x_data_enc=crypten.cryptensor(x_local.to_dense())\\n        #print(x_data_enc.device)\\n        for i in range(batch_size):\\n          batch_am=[]\\n          output_enc,_ = run_snn_test_2(x_data_enc[i],w1_enc,w2_enc)\\n          #print(output_enc.size())\\n          output=output_enc.get_plain_text()\\n          m,_= torch.max(output,1) # max over time\\n          #print(m.size())\\n          _,am=torch.max(m,0)# argmax over output units\\n          #print(am)\\n          batch_am.append(am) \\n        batch_am_stack=torch.stack(batch_am)       \\n        tmp = np.mean((y_local==batch_am_stack).detach().cpu().numpy()) # compare to labels\\n        accs.append(tmp)\\n        print(tmp)\\n          \\n    return np.mean(accs)', '_ii': 'def run_snn_test_2(inputs,w1_enc,w2_enc):\\n    #h1 = torch.einsum(\"abc,cd->abd\", (inputs, w1))\\n    syn = torch.zeros((nb_hidden), device=device, dtype=dtype)\\n    mem = torch.zeros((nb_hidden), device=dev...\n    Number of datapoints: 60000\n    Root location: /root/data/datasets/torch/fashion-mnist\n    Split: Train, 'test_dataset': Dataset FashionMNIST\n    Number of datapoints: 10000\n    Root location: /root/data/datasets/torch/fashion-mnist\n    Split: Test, '_i5': '# Standardize data\\n#x_train = torch.tensor(train_dataset.train_data, device=device, dtype=dtype)\\nx_train = np.array(train_dataset.data, dtype=np.float)\\nx_train = x_train.reshape(x_train.shape[0],-1)/255\\n#x_test = torch.tensor(test_dataset.test_data, device=device, dtype=dtype)\\nx_test = np.array(test_dataset.data, dtype=np.float)\\nx_test = x_test.reshape(x_test.shape[0],-1)/255\\n\\n#y_train = torch.tensor(train_dataset.train_labels, device=device, dtype=dtype)\\n#y_test  = torch.tensor(test_dataset.test_labels, device=device, dtype=dtype)\\ny_train = np.array(train_dataset.targets, dtype=np.int)\\ny_test  = np.array(test_dataset.targets, dtype=np.int)\\n\\ny_train.shape', 'x_train': array([[0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       ...,\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.]]), 'x_test': array([[0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       ...,\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.]]), 'y_train': array([9, 0, 0, ..., 3, 0, 5]), 'y_test': array([9, 2, 1, ..., 8, 1, 5]), '_5': (60000,), '_i6': '# Here we plot one of the raw data points as an example\\ndata_id = 1\\nplt.imshow(x_train[data_id].reshape(28,28), cmap=plt.cm.gray_r)\\nplt.axis(\"off\")', 'data_id': 1, '_6': (-0.5, 27.5, 27.5, -0.5), '_i7': 'def current2firing_time(x, tau=20, thr=0.2, tmax=1.0, epsilon=1e-7):\\n    \"\"\" Computes first firing time latency for a current input x assuming the charge time of a current based LIF neuron.\\n\\n    Args:\\n    x -- The \"current\" values\\n\\n    Keyword args:\\n    tau -- The membrane time constant of the LIF neuron to be charged\\n    thr -- The firing threshold value \\n    tmax -- The maximum time returned \\n    epsilon -- A generic (small) epsilon > 0\\n\\n    Returns:\\n    Time to first spike for each \"current\" x\\n    \"\"\"\\n    idx = x<thr\\n    x = np.clip(x,thr+epsilon,1e9)\\n    T = tau*np.log(x/(x-thr))\\n    # T in [1.5,109]\\n    T[idx] = tmax\\n\\n    return T\\n \\n\\ndef sparse_data_generator(X, y, batch_size, nb_steps, nb_units, shuffle=True ):\\n    \"\"\" This generator takes datasets in analog format and generates spiking network input as sparse tensors. \\n\\n    Args:\\n        X: The data ( sample x event x 2 ) the last dim holds (time,neuron) tuples\\n        y: The labels\\n        nb_steps: number of time steps\\n        nb_units: 784 (28x28)\\n    \"\"\"\\n\\n    labels_ = np.array(y,dtype=np.int)\\n    number_of_batches = len(X)//bat...\n       [1., 1., 1., ..., 1., 1., 1.],\n       [1., 1., 1., ..., 1., 1., 1.],\n       ...,\n       [1., 1., 1., ..., 1., 1., 1.],\n       [1., 1., 1., ..., 1., 1., 1.],\n       [1., 1., 1., ..., 1., 1., 1.]]), 'df':       0    1    2    3    4    ...       779        780  781  782  783\n0     1.0  1.0  1.0  1.0  1.0  ...  1.000000   1.000000  1.0  1.0  1.0\n1     1.0  1.0  1.0  1.0  1.0  ...  6.289867  28.642078  1.0  1.0  1.0\n2     1.0  1.0  1.0  1.0  1.0  ...  1.000000   1.000000  1.0  1.0  1.0\n3     1.0  1.0  1.0  1.0  1.0  ...  1.000000   1.000000  1.0  1.0  1.0\n4     1.0  1.0  1.0  1.0  1.0  ...  1.000000   1.000000  1.0  1.0  1.0\n...   ...  ...  ...  ...  ...  ...       ...        ...  ...  ...  ...\n9995  1.0  1.0  1.0  1.0  1.0  ...  1.000000   1.000000  1.0  1.0  1.0\n9996  1.0  1.0  1.0  1.0  1.0  ...  1.000000   1.000000  1.0  1.0  1.0\n9997  1.0  1.0  1.0  1.0  1.0  ...  1.000000   1.000000  1.0  1.0  1.0\n9998  1.0  1.0  1.0  1.0  1.0  ...  1.000000   1.000000  1.0  1.0  1.0\n9999  1.0  1.0  1.0  1.0  1.0  ...  1.000000   1.000000  1.0  1.0  1.0\n\n[10000 rows x 784 columns], '_8':            0        1             2    ...           781           782           783\ncount  10000.0  10000.0  10000.000000  ...  10000.000000  10000.000000  10000.000000\nmean       1.0      1.0      1.003667  ...      1.512261      1.232128      1.032410\nstd        0.0      0.0      0.239659  ...      5.421710      4.847282      2.899793\nmin        1.0      1.0      1.000000  ...      1.000000      1.000000      1.000000\n25%        1.0      1.0      1.000000  ...      1.000000      1.000000      1.000000\n50%        1.0      1.0      1.000000  ...      1.000000      1.000000      1.000000\n75%        1.0      1.0      1.000000  ...      1.000000      1.000000      1.000000\nmax        1.0      1.0     21.217439  ...    290.173165    290.173165    290.173165\n\n[8 rows x 784 columns], '_i9': 'tau_mem = 10e-3\\ntau_syn = 5e-3\\n\\nalpha   = float(np.exp(-time_step/tau_syn))\\nbeta    = float(np.exp(-time_step/tau_mem))', 'tau_mem': 0.01, 'tau_syn': 0.005, 'alpha': 0.8187307530779818, 'beta': 0.9048374180359595, '_i10': 'weight_scale = 7*(1.0-beta) # this should give us some spikes to begin with\\n\\nw1 = torch.empty((nb_inputs, nb_hidden),  device=device, dtype=dtype, requires_grad=True)\\ntorch.nn.init.normal_(w1, mean=0.0, std=weight_scale/np.sqrt(nb_inputs))\\n\\nw2 = torch.empty((nb_hidden, nb_outputs), device=device, dtype=dtype, requires_grad=True)\\ntorch.nn.init.normal_(w2, mean=0.0, std=weight_scale/np.sqrt(nb_hidden))\\n\\nprint(\"init done\")', 'weight_scale': 0.6661380737482834, 'w1': tensor([[ 0.0317, -0.0074, -0.0046,  ..., -0.0084,  0.0029,  0.0114],\n        [-0.0092, -0.0094,  0.0259,  ..., -0.0285, -0.0183,  0.0310],\n        [-0.0434, -0.0603,  0.0610,  ..., -0.0179, -0.0341,  0.0463],\n        ...,\n        [-0.0010,  0.0624,  0.0427,  ..., -0.0107,  0.0160,  0.0125],\n        [ 0.0103,  0.0134,  0.0044,  ...,  0.0494,  0.0418,  0.0405],\n        [-0.0149, -0.0155,  0.0498,  ..., -0.0137,  0.0195,  0.0248]],\n       device='cuda:0', requires_grad=True), 'w2': tensor([[-0.1986,  0.1029,  0.2804, -0.1246,  0.0824,  0.0441, -0.1294, -0.0443,\n         -0.0726, -0.1272],\n        [-0.0771,  0.0405, -0.2326, -0.0589,  0.0478,  0.0490,  0.0410, -0.0368,\n         -0.0633,  0.2318],\n        [-0.0271, -0.1260, -0.0960,  0.2241, -0.0701,  0.0135,  0.2543, -0.0294,\n         -0.1198,  0.1058],\n        [ 0.1433, -0.0080, -0.0431, -0.0273, -0.0404, -0.0974,  0.0245,  0.1444,\n         -0.1474, -0.0234],\n        [-0.1444, -0.1040, -0.0937, -0.1058, -0.1024,  0.1221,  0.1276, -0.2422,\n          0.0444, -0.0340],\n        [ 0.1768,  0.0072, -0.0699, -0.0142, -0.2202,  0.3443,  0.0768, -0.0694,\n         -0.1682, -0.1279],\n        [-0.0887, -0.0579, -0.0455, -0.0285, -0.1044,  0.1477,  0.1423, -0.1033,\n         -0.3140,  0.1165],\n        [ 0.0872, -0.0269, -0.0572, -0.3258,  0.4544, -0.1016, -0.1280, -0.1004,\n         -0.1291, -0.1475],\n        [ 0.0967, -0.1322, -0.2227,  0.0672, -0.2587, -0.1005,  0.1743, -0.2541,\n         -0.0507, -0.0171],\n        [-0.0974, -0.1763,  0.0565, -0.1535,  0.0510,  0.1089, -0.1275,  0.1967,\n          0.0273,  0.0076],\n        [-0.0054,  0.2067, -0.0130, -0.1927, -0.0339,  0.1099, -0.0554, -0.0558,\n          0.2462, -0.2266],\n        [ 0.1419,  0.1212, -0.1839, -0.0392, -0.0719,  0.2055, -0.0313,  0.0519,\n         -0.1927, -0.0945],\n        [ 0.0581,  0.1463,  0.0111,  0.1325,  0.0344, -0.1576,  0.0500, -0.0229,\n         -0.1456, -0.1273],\n        [-0.1039, -0.0564, -0.1265,  0.1856,  0.0780,  0.1636, -0.1334,  0.0462,\n          0.0505, -0.1597],\n        [-0.0236, -0.0816, -0.0868,  0.2333, -0.1087,  0.2394, -0.1062, -0.1765,\n          0.0720, -0.1706],\n        [ 0.0278, -0.0397, -0.0354,  0.1376, -0.1149, -0.0461,  0.1464,  0.1325,\n         -0.0953,  0.0613],\n        [-0.1972,  0.1387, -0.1392,  0.1178,  0.0868,  0.0254, -0.1908, -0.0320,\n          0.1167, -0.1479],\n        [ 0.2684, -0.2125,  0.0666, -0.2469, -0.0369, -0.0407, -0.0222, -0.0328,\n          0.2151, -0.0287],\n        [ 0.0713, -0.1425,  0.0958,  0.1002, -0.0682,  0.0389, -0.1760, -0.0601,\n         -0.0703, -0.0676],\n        [ 0.0543,  0.0272, -0.0667, -0.0356,  0.0596, -0.2005, -0.0784, -0.0095,\n          0.1375,  0.0388],\n        [-0.0628,  0.1673, -0.0719, -0.0671,  0.0424, -0.1292, -0.0813,  0.0453,\n          0.0261,  0.1298],\n        [-0.1374, -0.1551, -0.1111, -0.0494,  0.0023, -0.1994,  0.3621, -0.0354,\n         -0.0389, -0.0478],\n        [ 0.1390, -0.0687, -0.0837,  0.0216,  0.0427,  0.1538,  0.1128, -0.0244,\n          0.1929, -0.0919],\n        [-0.1191, -0.2929, -0.1157,  0.1735,  0.1025, -0.0301,  0.0198, -0.1147,\n         -0.0497,  0.1266],\n        [ 0.0279, -0.1403, -0.1298,  0.0843, -0.0258, -0.0661, -0.0360, -0.0266,\n          0.3058, -0.1369],\n        [-0.1231,  0.1459, -0.0190,  0.0684, -0.0443, -0.0512, -0.1146,  0.1129,\n          0.0310,  0.0770],\n        [ 0.1087, -0.1419,  0.0297,  0.1609,  0.0516, -0.1239,  0.0241, -0.1560,\n          0.0375, -0.0590],\n        [-0.1057, -0.1465, -0.0808, -0.0159,  0.1380,  0.1348,  0.0731, -0.2030,\n         -0.0269, -0.0436],\n        [-0.1209,  0.0519,  0.2338, -0.0355, -0.2017, -0.0655,  0.0617,  0.2159,\n         -0.0340, -0.1140],\n        [ 0.0881, -0.0312, -0.1829, -0.1073, -0.0492, -0.1133,  0.0604,  0.3560,\n         -0.0958, -0.1451],\n        [ 0.1689, -0.0585,  0.0377, -0.1067,  0.1568, -0.2171, -0.0846,  0.1517,\n         -0.1095, -0.0404],\n        [ 0.1264, -0.1758, -0.0290, -0.0295, -0.3027,  0.1804,  0.1851,  0.0719,\n          0.1475, -0.1482],\n        [-0.0723, -0.1512, -0.1220, -0.0515, -0.0859, -0.0388,  0.0073, -0.0033,\n         -0.1413, -0.0705],\n        [-0.0528, -0.1508,  0.1466,  0.0229,  0.1729,  0.0838,  0.0931,  0.0381,\n         -0.2061, -0.1147],\n        [ 0.0147,  0.1958, -0.1150, -0.0016, -0.0463,  0.0309,  0.1752,  0.1266,\n         -0.2362, -0.1775],\n        [ 0.0137,  0.0147,  0.0932,  0.0671, -0.0242, -0.0685,  0.1872, -0.1139,\n          0.0881, -0.0808],\n        [ 0.0221,  0.0565,  0.0913, -0.0263,  0.0423, -0.1889,  0.1201, -0.0057,\n          0.1113,  0.0606],\n        [-0.0235,  0.0289, -0.1385,  0.1174, -0.1337, -0.1040, -0.0968,  0.1846,\n          0.2059,  0.0364],\n        [ 0.0109,  0.0159, -0.1398, -0.0025,  0.0988, -0.0477, -0.1744, -0.0472,\n          0.1052, -0.0528],\n        [ 0.0478, -0.1337,  0.1342, -0.0857,  0.0118, -0.1359, -0.2749,  0.2911,\n         -0.1627, -0.1562],\n        [ 0.0027,  0.0250,  0.0668, -0.0811,  0.0021,  0.0919,  0.0759,  0.0816,\n          0.0749,  0.0301],\n        [ 0.1854, -0.3053, -0.1311,  0.0279, -0.0955, -0.0176,  0.0727,  0.1239,\n          0.1995,  0.0237],\n        [ 0.1766,  0.0107,  0.1013, -0.1957, -0.0211, -0.0329,  0.1159,  0.0534,\n         -0.0885, -0.1707],\n        [ 0.1058, -0.0067,  0.0040,  0.0398, -0.1178,  0.1276,  0.0227,  0.0418,\n          0.0446,  0.0754],\n        [ 0.0682,  0.1378,  0.1058, -0.0457, -0.0081,  0.1124, -0.1298, -0.2169,\n         -0.1177, -0.1897],\n        [-0.0680,  0.0818,  0.0551,  0.0043, -0.1539, -0.2233, -0.0729,  0.1038,\n          0.0728, -0.0083],\n        [-0.1157, -0.1106, -0.0348,  0.3660,  0.0032, -0.0375, -0.0992,  0.0456,\n         -0.0838, -0.0676],\n        [ 0.0697, -0.1788,  0.3058,  0.1759, -0.1803, -0.0506, -0.1526, -0.0622,\n         -0.1908, -0.0884],\n        [ 0.1225,  0.0300,  0.0244,  0.0336,  0.0542, -0.1572,  0.0780,  0.0010,\n         -0.0970,  0.0749],\n        [-0.1630, -0.0252,  0.3333,  0.1130, -0.1956, -0.2423, -0.0111, -0.0027,\n         -0.0362,  0.1799],\n        [-0.0332,  0.0205, -0.1079, -0.1339,  0.1632, -0.0017, -0.0008, -0.0537,\n          0.0314,  0.1506],\n        [-0.0584, -0.0711,  0.0525,  0.0371,  0.0290,  0.0778, -0.0537,  0.0853,\n          0.0786, -0.0170],\n        [ 0.1132, -0.0997,  0.1461, -0.1337, -0.0739, -0.1184,  0.0796, -0.1403,\n          0.0315, -0.0427],\n        [ 0.0877,  0.0623, -0.0632, -0.1174,  0.0966, -0.1611,  0.0186,  0.0303,\n          0.1657, -0.0290],\n        [-0.0759,  0.1489,  0.0753, -0.0110, -0.0232,  0.0845,  0.0591, -0.1456,\n         -0.0223, -0.0675],\n        [-0.0475,  0.1977, -0.0568,  0.1006, -0.2473, -0.0866,  0.0106, -0.0184,\n          0.1187, -0.0346],\n        [-0.0820,  0.0868,  0.0773,  0.0904,  0.1253, -0.1189,  0.0369, -0.0630,\n         -0.0879, -0.0819],\n        [-0.1307,  0.2978, -0.1887, -0.1105,  0.1231,  0.0588, -0.0661, -0.1602,\n         -0.1989, -0.1269],\n        [ 0.0984, -0.0740,  0.0468, -0.2023, -0.1810,  0.1684,  0.0628, -0.0212,\n         -0.1441,  0.0046],\n        [-0.0171, -0.0609,  0.0694, -0.1318,  0.1043, -0.0652,  0.0750,  0.0592,\n          0.0539, -0.1655],\n        [-0.0831,  0.2221,  0.1359,  0.0724,  0.0781,  0.0715, -0.2118, -0.1426,\n         -0.0074, -0.0306],\n        [-0.0550,  0.1327, -0.1862, -0.1174, -0.1535, -0.2597, -0.0519,  0.1466,\n          0.0276,  0.2557],\n        [ 0.0750, -0.0354, -0.0103, -0.0550,  0.0094, -0.0045,  0.0935,  0.0133,\n         -0.0058, -0.0526],\n        [ 0.0909,  0.2230, -0.0917,  0.1488, -0.0075,  0.0181, -0.0332, -0.1026,\n         -0.1667, -0.0719],\n        [ 0.0230, -0.0966,  0.0681, -0.2221, -0.0749,  0.1228, -0.0590, -0.0878,\n         -0.2455,  0.1010],\n        [-0.1732,  0.0134,  0.0947,  0.1239, -0.1481,  0.2219, -0.0239, -0.2041,\n          0.1625, -0.0578],\n        [-0.0044, -0.1028, -0.1034, -0.0580,  0.3344, -0.0338, -0.0872,  0.1171,\n         -0.0975, -0.2122],\n        [ 0.0647, -0.0044,  0.0661,  0.1275,  0.0718, -0.1133, -0.0393, -0.1307,\n          0.0262,  0.0067],\n        [ 0.0084, -0.0591,  0.1222, -0.0194,  0.1739,  0.0586, -0.1299, -0.0604,\n          0.1219, -0.2655],\n        [-0.0189, -0.0400,  0.0212,  0.0866,  0.0961,  0.2202,  0.0264,  0.0176,\n          0.0143,  0.0424],\n        [ 0.1166, -0.1352,  0.2252, -0.0949, -0.0639,  0.1543, -0.1096, -0.1480,\n         -0.0601, -0.0023],\n        [-0.0282,  0.1385,  0.1827,  0.0920, -0.0596,  0.0841, -0.0673,  0.0702,\n         -0.1045,  0.1696],\n        [-0.0789, -0.0854, -0.0253, -0.0179,  0.1091, -0.1932,  0.2328, -0.1380,\n         -0.3146, -0.0337],\n        [ 0.0275, -0.0505,  0.0420, -0.0873,  0.0222, -0.2003,  0.0676, -0.0591,\n         -0.0684, -0.1267],\n        [ 0.1602,  0.0662, -0.0210, -0.1054,  0.1088, -0.0827,  0.0719,  0.1331,\n         -0.0789,  0.0448],\n        [-0.0628, -0.1487, -0.1589,  0.0571,  0.0401, -0.1933, -0.1505,  0.2391,\n          0.0094,  0.0839],\n        [ 0.0052, -0.1301, -0.0514,  0.0077,  0.0713, -0.0794,  0.0172, -0.0529,\n         -0.0365,  0.0935],\n        [-0.1054, -0.1724, -0.0813,  0.0056, -0.1422, -0.2807, -0.0501,  0.0947,\n          0.3366, -0.1593],\n        [ 0.1211,  0.1327,  0.1332, -0.0148, -0.0580,  0.0146, -0.0060, -0.0330,\n         -0.3109, -0.0572],\n        [ 0.0048, -0.1166, -0.0625, -0.0788,  0.0642,  0.3513,  0.1839, -0.0978,\n         -0.0079, -0.1909],\n        [ 0.0941, -0.1040,  0.2028,  0.0221, -0.0503,  0.0050, -0.1797, -0.1263,\n         -0.1084,  0.0334],\n        [-0.1489, -0.1613, -0.1127, -0.0964, -0.0460, -0.1206,  0.1914,  0.0107,\n          0.1277, -0.0326],\n        [-0.1679, -0.0056,  0.0693, -0.0354,  0.1590, -0.0951,  0.0225, -0.0171,\n          0.2011, -0.0967],\n        [-0.0426, -0.1441, -0.0676, -0.0986, -0.0863,  0.3538, -0.0219, -0.1838,\n          0.0792, -0.1597],\n        [-0.0056, -0.0513, -0.0636,  0.0196,  0.0356, -0.0695,  0.2944, -0.0729,\n         -0.0720, -0.0290],\n        [-0.1267,  0.3450, -0.1943,  0.1044,  0.0271,  0.0660, -0.1011,  0.0610,\n          0.0803,  0.1605],\n        [-0.1311,  0.0280, -0.0164, -0.0447,  0.0288, -0.0702,  0.0929, -0.1607,\n          0.1414,  0.1704],\n        [ 0.0570,  0.0851,  0.0618, -0.1352,  0.1811, -0.0506, -0.0435, -0.1860,\n          0.0746, -0.0197],\n        [ 0.2024, -0.1481,  0.0115, -0.0476, -0.1410, -0.0696, -0.0949,  0.0870,\n         -0.0231,  0.0101],\n        [ 0.0962,  0.1031,  0.1220,  0.0736,  0.0366, -0.0543, -0.0928, -0.1079,\n          0.1275, -0.0523],\n        [-0.0869, -0.0895,  0.0215,  0.0966, -0.2076, -0.1393, -0.1579,  0.1928,\n         -0.2106, -0.0879],\n        [ 0.0558, -0.0662,  0.0213, -0.1372, -0.1480,  0.3953,  0.1189, -0.2241,\n         -0.0923, -0.1733],\n        [ 0.3835, -0.0052, -0.0122,  0.0485, -0.1948, -0.1183, -0.1600, -0.1797,\n         -0.1680, -0.0013],\n        [-0.0765, -0.1223,  0.1224,  0.2189, -0.1172, -0.0610,  0.0329,  0.2054,\n         -0.1402, -0.0905],\n        [ 0.0771, -0.0799, -0.1385, -0.0524,  0.0225,  0.3306,  0.1590, -0.2098,\n         -0.1429, -0.1242],\n        [-0.0827, -0.1647, -0.0895,  0.0197, -0.0412, -0.0297,  0.2319,  0.0133,\n         -0.2460,  0.0670],\n        [-0.0944, -0.1014, -0.0261, -0.0015, -0.0615, -0.1928, -0.1582,  0.0055,\n          0.2352,  0.0826],\n        [ 0.0535, -0.0082, -0.1263, -0.1443, -0.1309, -0.0855, -0.0126,  0.0866,\n          0.0490,  0.1650],\n        [ 0.0274, -0.0824,  0.1809,  0.1854, -0.1224, -0.1543, -0.1143, -0.0405,\n         -0.0611, -0.0475],\n        [ 0.1210, -0.0325, -0.1075, -0.1316, -0.1749,  0.1951, -0.1079,  0.0284,\n         -0.0043,  0.0729]], device='cuda:0', requires_grad=True), '_i11': 'def plot_voltage_traces(mem, spk=None, dim=(3,5), spike_height=5):\\n    gs=GridSpec(*dim)\\n    if spk is not None:\\n        dat = 1.0*mem\\n        dat[spk>0.0] = spike_height\\n        dat = dat.detach().cpu().numpy()\\n    else:\\n        dat = mem.detach().cpu().numpy()\\n    for i in range(np.prod(dim)):\\n        if i==0: a0=ax=plt.subplot(gs[i])\\n        else: ax=plt.subplot(gs[i],sharey=a0)\\n        ax.plot(dat[i])\\n        ax.axis(\"off\")', 'plot_voltage_traces': <function plot_voltage_traces at 0x7fa0d003a050>, '_i12': 'class SurrGradSpike(torch.autograd.Function):\\n    \"\"\"\\n    Here we implement our spiking nonlinearity which also implements \\n    the surrogate gradient. By subclassing torch.autograd.Function, \\n    we will be able to use all of PyTorch\\'s autograd functionality.\\n    Here we use the normalized negative part of a fast sigmoid \\n    as this was done in Zenke & Ganguli (2018).\\n    \"\"\"\\n    \\n    scale = 100.0 # controls steepness of surrogate gradient\\n\\n    @staticmethod\\n    def forward(ctx, input):\\n        \"\"\"\\n        In the forward pass we compute a step function of the input Tensor\\n        and return it. ctx is a context object that we use to stash information which \\n        we need to later backpropagate our error signals. To achieve this we use the \\n        ctx.save_for_backward method.\\n        \"\"\"\\n        ctx.save_for_backward(input)\\n        out = torch.zeros_like...\n        \u001b[0m\u001b[0;36mk\u001b[0m \u001b[0;34m= {}\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self=<IPython.core.magics.execution.ExecutionMagics object>, line='a=compute_classification_accuracy_test_2(x_test,y_test)', cell=None, local_ns={'ALICE': 0, 'BOB': 1, 'GridSpec': <class 'matplotlib.gridspec.GridSpec'>, 'In': ['', 'import os\\n\\nimport numpy as np\\nimport matplotlib.....nn as nn\\nimport torchvision\\n\\nimport pandas as pd', '# The coarse network structure is dicated by the...ime_step = 1e-3\\nnb_steps  = 100\\n\\nbatch_size = 256', 'dtype = torch.float\\n\\n# Check whether a GPU is av...    device = torch.device(\"cpu\")\\n    print(\"cpu\")', '# Here we load the Dataset\\nroot = os.path.expand...sform=None, target_transform=None, download=True)', '# Standardize data\\n#x_train = torch.tensor(train...est_dataset.targets, dtype=np.int)\\n\\ny_train.shape', '# Here we plot one of the raw data points as an ...shape(28,28), cmap=plt.cm.gray_r)\\nplt.axis(\"off\")', 'def current2firing_time(x, tau=20, thr=0.2, tmax..., y_batch.to(device=device)\\n\\n        counter += 1', 'T=current2firing_time(x_test)\\ndf = pd.DataFrame(T)\\ndf.describe()', 'tau_mem = 10e-3\\ntau_syn = 5e-3\\n\\nalpha   = float(...syn))\\nbeta    = float(np.exp(-time_step/tau_mem))', 'weight_scale = 7*(1.0-beta) # this should give u...ght_scale/np.sqrt(nb_hidden))\\n\\nprint(\"init done\")', 'def plot_voltage_traces(mem, spk=None, dim=(3,5)...0)\\n        ax.plot(dat[i])\\n        ax.axis(\"off\")', 'class SurrGradSpike(tor...\n\n[8 rows x 784 columns]}, 'SurrGradSpike': <class '__main__.SurrGradSpike'>, 'T': array([[1., 1., 1., ..., 1., 1., 1.],\n       [1...., 1., 1.],\n       [1., 1., 1., ..., 1., 1., 1.]]), '_':            0        1             2    ...      ... 290.173165    290.173165\n\n[8 rows x 784 columns], '_5': (60000,), '_6': (-0.5, 27.5, 27.5, -0.5), ...})\u001b[0m\n\u001b[1;32m   1191\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1193\u001b[0;31m             \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m        \u001b[0;36mglobal\u001b[0m \u001b[0;36mexec\u001b[0m \u001b[0;34m= \u001b[0;36mundefined\u001b[0m\u001b[0m\u001b[0;34m\n        \u001b[0m\u001b[0;36mcode\u001b[0m \u001b[0;34m= <code object <module> at 0x7fa05129bed0, file \"<timed exec>\", line 1>\u001b[0m\u001b[0;34m\n        \u001b[0m\u001b[0;36mglob\u001b[0m \u001b[0;34m= {'__name__': '__main__', '__doc__': 'returns spikes', '__package__': None, '__loader__': None, '__spec__': None, '__builtin__': <module 'builtins' (built-in)>, '__builtins__': <module 'builtins' (built-in)>, '_ih': ['', 'import os\\n\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\nfrom matplotlib.gridspec import GridSpec\\nimport seaborn as sns\\n\\nimport torch\\nimport torch.nn as nn\\nimport torchvision\\n\\nimport pandas as pd', '# The coarse network structure is dicated by the Fashion MNIST dataset. \\nnb_inputs  = 28*28\\nnb_hidden  = 100\\nnb_outputs = 10\\n\\ntime_step = 1e-3\\nnb_steps  = 100\\n\\nbatch_size = 256', 'dtype = torch.float\\n\\n# Check whether a GPU is available\\nif torch.cuda.is_available():\\n    device = torch.device(\"cuda\")\\n    print(\"cuda\")     \\nelse:\\n    device = torch.device(\"cpu\")\\n    print(\"cpu\")', '# Here we load the Dataset\\nroot = os.path.expanduser(\"~/data/datasets/torch/fashion-mnist\")\\ntrain_dataset = torchvision.datasets.FashionMNIST(root, train=True, transform=None, target_transform=None, download=True)\\ntest_dataset = torchvision.datasets.FashionMNIST(root, train=False, transform=None, target_transform=None, download=True)', '# Standardize data\\n#x_train = torch.tensor(train_dataset.train_data, device=device, dtype=dtype)\\nx_train = np.array(train_dataset.data, dtype=np.float)\\nx_train = x_train.reshape(x_train.shape[0],-1)/255\\n#x_test = torch.tensor(test_dataset.test_data, device=device, dtype=dtype)\\nx...\ncount  10000.0  10000.0  10000.000000  ...  10000.000000  10000.000000  10000.000000\nmean       1.0      1.0      1.003667  ...      1.512261      1.232128      1.032410\nstd        0.0      0.0      0.239659  ...      5.421710      4.847282      2.899793\nmin        1.0      1.0      1.000000  ...      1.000000      1.000000      1.000000\n25%        1.0      1.0      1.000000  ...      1.000000      1.000000      1.000000\n50%        1.0      1.0      1.000000  ...      1.000000      1.000000      1.000000\n75%        1.0      1.0      1.000000  ...      1.000000      1.000000      1.000000\nmax        1.0      1.0     21.217439  ...    290.173165    290.173165    290.173165\n\n[8 rows x 784 columns]}, '_dh': ['/content'], '_sh': <module 'IPython.core.shadowns' from '/usr/local/lib/python3.7/dist-packages/IPython/core/shadowns.py'>, 'In': ['', 'import os\\n\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\nfrom matplotlib.gridspec import GridSpec\\nimport seaborn as sns\\n\\nimport torch\\nimport torch.nn as nn\\nimport torchvision\\n\\nimport pandas as pd', '# The coarse network structure is dicated by the Fashion MNIST dataset. \\nnb_inputs  = 28*28\\nnb_hidden  = 100\\nnb_outputs = 10\\n\\ntime_step = 1e-3\\nnb_steps  = 100\\n\\nbatch_size = 256', 'dtype = torch.float\\n\\n# Check whether a GPU is available\\nif torch.cuda.is_available():\\n    device = torch.device(\"cuda\")\\n    print(\"cuda\")     \\nelse:\\n    device = torch.device(\"cpu\")\\n    print(\"cpu\")', '# Here we load the Dataset\\nroot = os.path.expanduser(\"~/data/datasets/torch/fashion-mnist\")\\ntrain_dataset = torchvision.datasets.FashionMNIST(root, train=True, transform=None, target_transform=None, download=True)\\ntest_dataset = torchvision.datasets.FashionMNIST(root, train=False, transform=None, target_transform=None, download=True)', '# Standardize data\\n#x_train = torch.tensor(train_dataset.train_data, device=device, dtype=dtype)\\nx_train = np.array(train_dataset.data, dtype=np.float)\\nx_train = x_train.reshape(x_train.shape[0],-1)/255\\n#x_test = torch.tensor(test_dataset.test_data, device=device, dtype=dtype)\\nx_test = np.array(test_dataset.data, dtype=np.float)\\nx_test = x_test.reshape(x_test.shap...\ncount  10000.0  10000.0  10000.000000  ...  10000.000000  10000.000000  10000.000000\nmean       1.0      1.0      1.003667  ...      1.512261      1.232128      1.032410\nstd        0.0      0.0      0.239659  ...      5.421710      4.847282      2.899793\nmin        1.0      1.0      1.000000  ...      1.000000      1.000000      1.000000\n25%        1.0      1.0      1.000000  ...      1.000000      1.000000      1.000000\n50%        1.0      1.0      1.000000  ...      1.000000      1.000000      1.000000\n75%        1.0      1.0      1.000000  ...      1.000000      1.000000      1.000000\nmax        1.0      1.0     21.217439  ...    290.173165    290.173165    290.173165\n\n[8 rows x 784 columns]}, 'get_ipython': <bound method InteractiveShell.get_ipython of <google.colab._shell.Shell object at 0x7fa1ec099750>>, 'exit': <IPython.core.autocall.ZMQExitAutocall object at 0x7fa1e7b79a50>, 'quit': <IPython.core.autocall.ZMQExitAutocall object at 0x7fa1e7b79a50>, '_':            0        1             2    ...           781           782           783\ncount  10000.0  10000.0  10000.000000  ...  10000.000000  10000.000000  10000.000000\nmean       1.0      1.0      1.003667  ...      1.512261      1.232128      1.032410\nstd        0.0      0.0      0.239659  ...      5.421710      4.847282      2.899793\nmin        1.0      1.0      1.000000  ...      1.000000      1.000000      1.000000\n25%        1.0      1.0      1.000000  ...      1.000000      1.000000      1.000000\n50%        1.0      1.0      1.000000  ...      1.000000      1.000000      1.000000\n75%        1.0      1.0      1.000000  ...      1.000000      1.000000      1.000000\nmax        1.0      1.0     21.217439  ...    290.173165    290.173165    290.173165\n\n[8 rows x 784 columns], '__': (-0.5, 27.5, 27.5, -0.5), '___': (60000,), '_i': '#@mpc.run_multiprocess(world_size=2)\\ndef compute_classification_accuracy_test_2(x_data, y_data):\\n    \"\"\" Computes classification accuracy on supplied data in batches. \"\"\"\\n\\n    w1_enc=crypten.cryptensor(w1,src=0)\\n    w2_enc=crypten.cryptensor(w2,src=0)\\n    #print(w1_enc.device)\\n\\n    #print(w2_enc.device)\\n\\n    \\n    \\n    accs = []\\n    \\n    for x_local, y_local in sparse_data_generator(x_data, y_data, batch_size, nb_steps, nb_inputs, shuffle=False):\\n        \\n        x_data_enc=crypten.cryptensor(x_local.to_dense())\\n        #print(x_data_enc.device)\\n        for i in range(batch_size):\\n          batch_am=[]\\n          output_enc,_ = run_snn_test_2(x_data_enc[i],w1_enc,w2_enc)\\n          #print(output_enc.size())\\n          output=output_enc.get_plain_text()\\n          m,_= torch.max(output,1) # max over time\\n          #print(m.size())\\n          _,am=torch.max(m,0)# argmax over output units\\n          #print(am)\\n          batch_am.append(am) \\n        batch_am_stack=torch.stack(batch_am)       \\n        tmp = np.mean((y_local==batch_am_stack).detach().cpu().numpy()) # compare to labels\\n        accs.append(tmp)\\n        print(tmp)\\n          \\n    return np.mean(accs)', '_ii': 'def run_snn_test_2(inputs,w1_enc,w2_enc):\\n    #h1 = torch.einsum(\"abc,cd->abd\", (inputs, w1))\\n    syn = torch.zeros((nb_hidden), device=device, dtype=dtype)\\n    mem = torch.zeros((nb_hidden), device=dev...\n    Number of datapoints: 60000\n    Root location: /root/data/datasets/torch/fashion-mnist\n    Split: Train, 'test_dataset': Dataset FashionMNIST\n    Number of datapoints: 10000\n    Root location: /root/data/datasets/torch/fashion-mnist\n    Split: Test, '_i5': '# Standardize data\\n#x_train = torch.tensor(train_dataset.train_data, device=device, dtype=dtype)\\nx_train = np.array(train_dataset.data, dtype=np.float)\\nx_train = x_train.reshape(x_train.shape[0],-1)/255\\n#x_test = torch.tensor(test_dataset.test_data, device=device, dtype=dtype)\\nx_test = np.array(test_dataset.data, dtype=np.float)\\nx_test = x_test.reshape(x_test.shape[0],-1)/255\\n\\n#y_train = torch.tensor(train_dataset.train_labels, device=device, dtype=dtype)\\n#y_test  = torch.tensor(test_dataset.test_labels, device=device, dtype=dtype)\\ny_train = np.array(train_dataset.targets, dtype=np.int)\\ny_test  = np.array(test_dataset.targets, dtype=np.int)\\n\\ny_train.shape', 'x_train': array([[0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       ...,\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.]]), 'x_test': array([[0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       ...,\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.]]), 'y_train': array([9, 0, 0, ..., 3, 0, 5]), 'y_test': array([9, 2, 1, ..., 8, 1, 5]), '_5': (60000,), '_i6': '# Here we plot one of the raw data points as an example\\ndata_id = 1\\nplt.imshow(x_train[data_id].reshape(28,28), cmap=plt.cm.gray_r)\\nplt.axis(\"off\")', 'data_id': 1, '_6': (-0.5, 27.5, 27.5, -0.5), '_i7': 'def current2firing_time(x, tau=20, thr=0.2, tmax=1.0, epsilon=1e-7):\\n    \"\"\" Computes first firing time latency for a current input x assuming the charge time of a current based LIF neuron.\\n\\n    Args:\\n    x -- The \"current\" values\\n\\n    Keyword args:\\n    tau -- The membrane time constant of the LIF neuron to be charged\\n    thr -- The firing threshold value \\n    tmax -- The maximum time returned \\n    epsilon -- A generic (small) epsilon > 0\\n\\n    Returns:\\n    Time to first spike for each \"current\" x\\n    \"\"\"\\n    idx = x<thr\\n    x = np.clip(x,thr+epsilon,1e9)\\n    T = tau*np.log(x/(x-thr))\\n    # T in [1.5,109]\\n    T[idx] = tmax\\n\\n    return T\\n \\n\\ndef sparse_data_generator(X, y, batch_size, nb_steps, nb_units, shuffle=True ):\\n    \"\"\" This generator takes datasets in analog format and generates spiking network input as sparse tensors. \\n\\n    Args:\\n        X: The data ( sample x event x 2 ) the last dim holds (time,neuron) tuples\\n        y: The labels\\n        nb_steps: number of time steps\\n        nb_units: 784 (28x28)\\n    \"\"\"\\n\\n    labels_ = np.array(y,dtype=np.int)\\n    number_of_batches = len(X)//bat...\n       [1., 1., 1., ..., 1., 1., 1.],\n       [1., 1., 1., ..., 1., 1., 1.],\n       ...,\n       [1., 1., 1., ..., 1., 1., 1.],\n       [1., 1., 1., ..., 1., 1., 1.],\n       [1., 1., 1., ..., 1., 1., 1.]]), 'df':       0    1    2    3    4    ...       779        780  781  782  783\n0     1.0  1.0  1.0  1.0  1.0  ...  1.000000   1.000000  1.0  1.0  1.0\n1     1.0  1.0  1.0  1.0  1.0  ...  6.289867  28.642078  1.0  1.0  1.0\n2     1.0  1.0  1.0  1.0  1.0  ...  1.000000   1.000000  1.0  1.0  1.0\n3     1.0  1.0  1.0  1.0  1.0  ...  1.000000   1.000000  1.0  1.0  1.0\n4     1.0  1.0  1.0  1.0  1.0  ...  1.000000   1.000000  1.0  1.0  1.0\n...   ...  ...  ...  ...  ...  ...       ...        ...  ...  ...  ...\n9995  1.0  1.0  1.0  1.0  1.0  ...  1.000000   1.000000  1.0  1.0  1.0\n9996  1.0  1.0  1.0  1.0  1.0  ...  1.000000   1.000000  1.0  1.0  1.0\n9997  1.0  1.0  1.0  1.0  1.0  ...  1.000000   1.000000  1.0  1.0  1.0\n9998  1.0  1.0  1.0  1.0  1.0  ...  1.000000   1.000000  1.0  1.0  1.0\n9999  1.0  1.0  1.0  1.0  1.0  ...  1.000000   1.000000  1.0  1.0  1.0\n\n[10000 rows x 784 columns], '_8':            0        1             2    ...           781           782           783\ncount  10000.0  10000.0  10000.000000  ...  10000.000000  10000.000000  10000.000000\nmean       1.0      1.0      1.003667  ...      1.512261      1.232128      1.032410\nstd        0.0      0.0      0.239659  ...      5.421710      4.847282      2.899793\nmin        1.0      1.0      1.000000  ...      1.000000      1.000000      1.000000\n25%        1.0      1.0      1.000000  ...      1.000000      1.000000      1.000000\n50%        1.0      1.0      1.000000  ...      1.000000      1.000000      1.000000\n75%        1.0      1.0      1.000000  ...      1.000000      1.000000      1.000000\nmax        1.0      1.0     21.217439  ...    290.173165    290.173165    290.173165\n\n[8 rows x 784 columns], '_i9': 'tau_mem = 10e-3\\ntau_syn = 5e-3\\n\\nalpha   = float(np.exp(-time_step/tau_syn))\\nbeta    = float(np.exp(-time_step/tau_mem))', 'tau_mem': 0.01, 'tau_syn': 0.005, 'alpha': 0.8187307530779818, 'beta': 0.9048374180359595, '_i10': 'weight_scale = 7*(1.0-beta) # this should give us some spikes to begin with\\n\\nw1 = torch.empty((nb_inputs, nb_hidden),  device=device, dtype=dtype, requires_grad=True)\\ntorch.nn.init.normal_(w1, mean=0.0, std=weight_scale/np.sqrt(nb_inputs))\\n\\nw2 = torch.empty((nb_hidden, nb_outputs), device=device, dtype=dtype, requires_grad=True)\\ntorch.nn.init.normal_(w2, mean=0.0, std=weight_scale/np.sqrt(nb_hidden))\\n\\nprint(\"init done\")', 'weight_scale': 0.6661380737482834, 'w1': tensor([[ 0.0317, -0.0074, -0.0046,  ..., -0.0084,  0.0029,  0.0114],\n        [-0.0092, -0.0094,  0.0259,  ..., -0.0285, -0.0183,  0.0310],\n        [-0.0434, -0.0603,  0.0610,  ..., -0.0179, -0.0341,  0.0463],\n        ...,\n        [-0.0010,  0.0624,  0.0427,  ..., -0.0107,  0.0160,  0.0125],\n        [ 0.0103,  0.0134,  0.0044,  ...,  0.0494,  0.0418,  0.0405],\n        [-0.0149, -0.0155,  0.0498,  ..., -0.0137,  0.0195,  0.0248]],\n       device='cuda:0', requires_grad=True), 'w2': tensor([[-0.1986,  0.1029,  0.2804, -0.1246,  0.0824,  0.0441, -0.1294, -0.0443,\n         -0.0726, -0.1272],\n        [-0.0771,  0.0405, -0.2326, -0.0589,  0.0478,  0.0490,  0.0410, -0.0368,\n         -0.0633,  0.2318],\n        [-0.0271, -0.1260, -0.0960,  0.2241, -0.0701,  0.0135,  0.2543, -0.0294,\n         -0.1198,  0.1058],\n        [ 0.1433, -0.0080, -0.0431, -0.0273, -0.0404, -0.0974,  0.0245,  0.1444,\n         -0.1474, -0.0234],\n        [-0.1444, -0.1040, -0.0937, -0.1058, -0.1024,  0.1221,  0.1276, -0.2422,\n          0.0444, -0.0340],\n        [ 0.1768,  0.0072, -0.0699, -0.0142, -0.2202,  0.3443,  0.0768, -0.0694,\n         -0.1682, -0.1279],\n        [-0.0887, -0.0579, -0.0455, -0.0285, -0.1044,  0.1477,  0.1423, -0.1033,\n         -0.3140,  0.1165],\n        [ 0.0872, -0.0269, -0.0572, -0.3258,  0.4544, -0.1016, -0.1280, -0.1004,\n         -0.1291, -0.1475],\n        [ 0.0967, -0.1322, -0.2227,  0.0672, -0.2587, -0.1005,  0.1743, -0.2541,\n         -0.0507, -0.0171],\n        [-0.0974, -0.1763,  0.0565, -0.1535,  0.0510,  0.1089, -0.1275,  0.1967,\n          0.0273,  0.0076],\n        [-0.0054,  0.2067, -0.0130, -0.1927, -0.0339,  0.1099, -0.0554, -0.0558,\n          0.2462, -0.2266],\n        [ 0.1419,  0.1212, -0.1839, -0.0392, -0.0719,  0.2055, -0.0313,  0.0519,\n         -0.1927, -0.0945],\n        [ 0.0581,  0.1463,  0.0111,  0.1325,  0.0344, -0.1576,  0.0500, -0.0229,\n         -0.1456, -0.1273],\n        [-0.1039, -0.0564, -0.1265,  0.1856,  0.0780,  0.1636, -0.1334,  0.0462,\n          0.0505, -0.1597],\n        [-0.0236, -0.0816, -0.0868,  0.2333, -0.1087,  0.2394, -0.1062, -0.1765,\n          0.0720, -0.1706],\n        [ 0.0278, -0.0397, -0.0354,  0.1376, -0.1149, -0.0461,  0.1464,  0.1325,\n         -0.0953,  0.0613],\n        [-0.1972,  0.1387, -0.1392,  0.1178,  0.0868,  0.0254, -0.1908, -0.0320,\n          0.1167, -0.1479],\n        [ 0.2684, -0.2125,  0.0666, -0.2469, -0.0369, -0.0407, -0.0222, -0.0328,\n          0.2151, -0.0287],\n        [ 0.0713, -0.1425,  0.0958,  0.1002, -0.0682,  0.0389, -0.1760, -0.0601,\n         -0.0703, -0.0676],\n        [ 0.0543,  0.0272, -0.0667, -0.0356,  0.0596, -0.2005, -0.0784, -0.0095,\n          0.1375,  0.0388],\n        [-0.0628,  0.1673, -0.0719, -0.0671,  0.0424, -0.1292, -0.0813,  0.0453,\n          0.0261,  0.1298],\n        [-0.1374, -0.1551, -0.1111, -0.0494,  0.0023, -0.1994,  0.3621, -0.0354,\n         -0.0389, -0.0478],\n        [ 0.1390, -0.0687, -0.0837,  0.0216,  0.0427,  0.1538,  0.1128, -0.0244,\n          0.1929, -0.0919],\n        [-0.1191, -0.2929, -0.1157,  0.1735,  0.1025, -0.0301,  0.0198, -0.1147,\n         -0.0497,  0.1266],\n        [ 0.0279, -0.1403, -0.1298,  0.0843, -0.0258, -0.0661, -0.0360, -0.0266,\n          0.3058, -0.1369],\n        [-0.1231,  0.1459, -0.0190,  0.0684, -0.0443, -0.0512, -0.1146,  0.1129,\n          0.0310,  0.0770],\n        [ 0.1087, -0.1419,  0.0297,  0.1609,  0.0516, -0.1239,  0.0241, -0.1560,\n          0.0375, -0.0590],\n        [-0.1057, -0.1465, -0.0808, -0.0159,  0.1380,  0.1348,  0.0731, -0.2030,\n         -0.0269, -0.0436],\n        [-0.1209,  0.0519,  0.2338, -0.0355, -0.2017, -0.0655,  0.0617,  0.2159,\n         -0.0340, -0.1140],\n        [ 0.0881, -0.0312, -0.1829, -0.1073, -0.0492, -0.1133,  0.0604,  0.3560,\n         -0.0958, -0.1451],\n        [ 0.1689, -0.0585,  0.0377, -0.1067,  0.1568, -0.2171, -0.0846,  0.1517,\n         -0.1095, -0.0404],\n        [ 0.1264, -0.1758, -0.0290, -0.0295, -0.3027,  0.1804,  0.1851,  0.0719,\n          0.1475, -0.1482],\n        [-0.0723, -0.1512, -0.1220, -0.0515, -0.0859, -0.0388,  0.0073, -0.0033,\n         -0.1413, -0.0705],\n        [-0.0528, -0.1508,  0.1466,  0.0229,  0.1729,  0.0838,  0.0931,  0.0381,\n         -0.2061, -0.1147],\n        [ 0.0147,  0.1958, -0.1150, -0.0016, -0.0463,  0.0309,  0.1752,  0.1266,\n         -0.2362, -0.1775],\n        [ 0.0137,  0.0147,  0.0932,  0.0671, -0.0242, -0.0685,  0.1872, -0.1139,\n          0.0881, -0.0808],\n        [ 0.0221,  0.0565,  0.0913, -0.0263,  0.0423, -0.1889,  0.1201, -0.0057,\n          0.1113,  0.0606],\n        [-0.0235,  0.0289, -0.1385,  0.1174, -0.1337, -0.1040, -0.0968,  0.1846,\n          0.2059,  0.0364],\n        [ 0.0109,  0.0159, -0.1398, -0.0025,  0.0988, -0.0477, -0.1744, -0.0472,\n          0.1052, -0.0528],\n        [ 0.0478, -0.1337,  0.1342, -0.0857,  0.0118, -0.1359, -0.2749,  0.2911,\n         -0.1627, -0.1562],\n        [ 0.0027,  0.0250,  0.0668, -0.0811,  0.0021,  0.0919,  0.0759,  0.0816,\n          0.0749,  0.0301],\n        [ 0.1854, -0.3053, -0.1311,  0.0279, -0.0955, -0.0176,  0.0727,  0.1239,\n          0.1995,  0.0237],\n        [ 0.1766,  0.0107,  0.1013, -0.1957, -0.0211, -0.0329,  0.1159,  0.0534,\n         -0.0885, -0.1707],\n        [ 0.1058, -0.0067,  0.0040,  0.0398, -0.1178,  0.1276,  0.0227,  0.0418,\n          0.0446,  0.0754],\n        [ 0.0682,  0.1378,  0.1058, -0.0457, -0.0081,  0.1124, -0.1298, -0.2169,\n         -0.1177, -0.1897],\n        [-0.0680,  0.0818,  0.0551,  0.0043, -0.1539, -0.2233, -0.0729,  0.1038,\n          0.0728, -0.0083],\n        [-0.1157, -0.1106, -0.0348,  0.3660,  0.0032, -0.0375, -0.0992,  0.0456,\n         -0.0838, -0.0676],\n        [ 0.0697, -0.1788,  0.3058,  0.1759, -0.1803, -0.0506, -0.1526, -0.0622,\n         -0.1908, -0.0884],\n        [ 0.1225,  0.0300,  0.0244,  0.0336,  0.0542, -0.1572,  0.0780,  0.0010,\n         -0.0970,  0.0749],\n        [-0.1630, -0.0252,  0.3333,  0.1130, -0.1956, -0.2423, -0.0111, -0.0027,\n         -0.0362,  0.1799],\n        [-0.0332,  0.0205, -0.1079, -0.1339,  0.1632, -0.0017, -0.0008, -0.0537,\n          0.0314,  0.1506],\n        [-0.0584, -0.0711,  0.0525,  0.0371,  0.0290,  0.0778, -0.0537,  0.0853,\n          0.0786, -0.0170],\n        [ 0.1132, -0.0997,  0.1461, -0.1337, -0.0739, -0.1184,  0.0796, -0.1403,\n          0.0315, -0.0427],\n        [ 0.0877,  0.0623, -0.0632, -0.1174,  0.0966, -0.1611,  0.0186,  0.0303,\n          0.1657, -0.0290],\n        [-0.0759,  0.1489,  0.0753, -0.0110, -0.0232,  0.0845,  0.0591, -0.1456,\n         -0.0223, -0.0675],\n        [-0.0475,  0.1977, -0.0568,  0.1006, -0.2473, -0.0866,  0.0106, -0.0184,\n          0.1187, -0.0346],\n        [-0.0820,  0.0868,  0.0773,  0.0904,  0.1253, -0.1189,  0.0369, -0.0630,\n         -0.0879, -0.0819],\n        [-0.1307,  0.2978, -0.1887, -0.1105,  0.1231,  0.0588, -0.0661, -0.1602,\n         -0.1989, -0.1269],\n        [ 0.0984, -0.0740,  0.0468, -0.2023, -0.1810,  0.1684,  0.0628, -0.0212,\n         -0.1441,  0.0046],\n        [-0.0171, -0.0609,  0.0694, -0.1318,  0.1043, -0.0652,  0.0750,  0.0592,\n          0.0539, -0.1655],\n        [-0.0831,  0.2221,  0.1359,  0.0724,  0.0781,  0.0715, -0.2118, -0.1426,\n         -0.0074, -0.0306],\n        [-0.0550,  0.1327, -0.1862, -0.1174, -0.1535, -0.2597, -0.0519,  0.1466,\n          0.0276,  0.2557],\n        [ 0.0750, -0.0354, -0.0103, -0.0550,  0.0094, -0.0045,  0.0935,  0.0133,\n         -0.0058, -0.0526],\n        [ 0.0909,  0.2230, -0.0917,  0.1488, -0.0075,  0.0181, -0.0332, -0.1026,\n         -0.1667, -0.0719],\n        [ 0.0230, -0.0966,  0.0681, -0.2221, -0.0749,  0.1228, -0.0590, -0.0878,\n         -0.2455,  0.1010],\n        [-0.1732,  0.0134,  0.0947,  0.1239, -0.1481,  0.2219, -0.0239, -0.2041,\n          0.1625, -0.0578],\n        [-0.0044, -0.1028, -0.1034, -0.0580,  0.3344, -0.0338, -0.0872,  0.1171,\n         -0.0975, -0.2122],\n        [ 0.0647, -0.0044,  0.0661,  0.1275,  0.0718, -0.1133, -0.0393, -0.1307,\n          0.0262,  0.0067],\n        [ 0.0084, -0.0591,  0.1222, -0.0194,  0.1739,  0.0586, -0.1299, -0.0604,\n          0.1219, -0.2655],\n        [-0.0189, -0.0400,  0.0212,  0.0866,  0.0961,  0.2202,  0.0264,  0.0176,\n          0.0143,  0.0424],\n        [ 0.1166, -0.1352,  0.2252, -0.0949, -0.0639,  0.1543, -0.1096, -0.1480,\n         -0.0601, -0.0023],\n        [-0.0282,  0.1385,  0.1827,  0.0920, -0.0596,  0.0841, -0.0673,  0.0702,\n         -0.1045,  0.1696],\n        [-0.0789, -0.0854, -0.0253, -0.0179,  0.1091, -0.1932,  0.2328, -0.1380,\n         -0.3146, -0.0337],\n        [ 0.0275, -0.0505,  0.0420, -0.0873,  0.0222, -0.2003,  0.0676, -0.0591,\n         -0.0684, -0.1267],\n        [ 0.1602,  0.0662, -0.0210, -0.1054,  0.1088, -0.0827,  0.0719,  0.1331,\n         -0.0789,  0.0448],\n        [-0.0628, -0.1487, -0.1589,  0.0571,  0.0401, -0.1933, -0.1505,  0.2391,\n          0.0094,  0.0839],\n        [ 0.0052, -0.1301, -0.0514,  0.0077,  0.0713, -0.0794,  0.0172, -0.0529,\n         -0.0365,  0.0935],\n        [-0.1054, -0.1724, -0.0813,  0.0056, -0.1422, -0.2807, -0.0501,  0.0947,\n          0.3366, -0.1593],\n        [ 0.1211,  0.1327,  0.1332, -0.0148, -0.0580,  0.0146, -0.0060, -0.0330,\n         -0.3109, -0.0572],\n        [ 0.0048, -0.1166, -0.0625, -0.0788,  0.0642,  0.3513,  0.1839, -0.0978,\n         -0.0079, -0.1909],\n        [ 0.0941, -0.1040,  0.2028,  0.0221, -0.0503,  0.0050, -0.1797, -0.1263,\n         -0.1084,  0.0334],\n        [-0.1489, -0.1613, -0.1127, -0.0964, -0.0460, -0.1206,  0.1914,  0.0107,\n          0.1277, -0.0326],\n        [-0.1679, -0.0056,  0.0693, -0.0354,  0.1590, -0.0951,  0.0225, -0.0171,\n          0.2011, -0.0967],\n        [-0.0426, -0.1441, -0.0676, -0.0986, -0.0863,  0.3538, -0.0219, -0.1838,\n          0.0792, -0.1597],\n        [-0.0056, -0.0513, -0.0636,  0.0196,  0.0356, -0.0695,  0.2944, -0.0729,\n         -0.0720, -0.0290],\n        [-0.1267,  0.3450, -0.1943,  0.1044,  0.0271,  0.0660, -0.1011,  0.0610,\n          0.0803,  0.1605],\n        [-0.1311,  0.0280, -0.0164, -0.0447,  0.0288, -0.0702,  0.0929, -0.1607,\n          0.1414,  0.1704],\n        [ 0.0570,  0.0851,  0.0618, -0.1352,  0.1811, -0.0506, -0.0435, -0.1860,\n          0.0746, -0.0197],\n        [ 0.2024, -0.1481,  0.0115, -0.0476, -0.1410, -0.0696, -0.0949,  0.0870,\n         -0.0231,  0.0101],\n        [ 0.0962,  0.1031,  0.1220,  0.0736,  0.0366, -0.0543, -0.0928, -0.1079,\n          0.1275, -0.0523],\n        [-0.0869, -0.0895,  0.0215,  0.0966, -0.2076, -0.1393, -0.1579,  0.1928,\n         -0.2106, -0.0879],\n        [ 0.0558, -0.0662,  0.0213, -0.1372, -0.1480,  0.3953,  0.1189, -0.2241,\n         -0.0923, -0.1733],\n        [ 0.3835, -0.0052, -0.0122,  0.0485, -0.1948, -0.1183, -0.1600, -0.1797,\n         -0.1680, -0.0013],\n        [-0.0765, -0.1223,  0.1224,  0.2189, -0.1172, -0.0610,  0.0329,  0.2054,\n         -0.1402, -0.0905],\n        [ 0.0771, -0.0799, -0.1385, -0.0524,  0.0225,  0.3306,  0.1590, -0.2098,\n         -0.1429, -0.1242],\n        [-0.0827, -0.1647, -0.0895,  0.0197, -0.0412, -0.0297,  0.2319,  0.0133,\n         -0.2460,  0.0670],\n        [-0.0944, -0.1014, -0.0261, -0.0015, -0.0615, -0.1928, -0.1582,  0.0055,\n          0.2352,  0.0826],\n        [ 0.0535, -0.0082, -0.1263, -0.1443, -0.1309, -0.0855, -0.0126,  0.0866,\n          0.0490,  0.1650],\n        [ 0.0274, -0.0824,  0.1809,  0.1854, -0.1224, -0.1543, -0.1143, -0.0405,\n         -0.0611, -0.0475],\n        [ 0.1210, -0.0325, -0.1075, -0.1316, -0.1749,  0.1951, -0.1079,  0.0284,\n         -0.0043,  0.0729]], device='cuda:0', requires_grad=True), '_i11': 'def plot_voltage_traces(mem, spk=None, dim=(3,5), spike_height=5):\\n    gs=GridSpec(*dim)\\n    if spk is not None:\\n        dat = 1.0*mem\\n        dat[spk>0.0] = spike_height\\n        dat = dat.detach().cpu().numpy()\\n    else:\\n        dat = mem.detach().cpu().numpy()\\n    for i in range(np.prod(dim)):\\n        if i==0: a0=ax=plt.subplot(gs[i])\\n        else: ax=plt.subplot(gs[i],sharey=a0)\\n        ax.plot(dat[i])\\n        ax.axis(\"off\")', 'plot_voltage_traces': <function plot_voltage_traces at 0x7fa0d003a050>, '_i12': 'class SurrGradSpike(torch.autograd.Function):\\n    \"\"\"\\n    Here we implement our spiking nonlinearity which also implements \\n    the surrogate gradient. By subclassing torch.autograd.Function, \\n    we will be able to use all of PyTorch\\'s autograd functionality.\\n    Here we use the normalized negative part of a fast sigmoid \\n    as this was done in Zenke & Ganguli (2018).\\n    \"\"\"\\n    \\n    scale = 100.0 # controls steepness of surrogate gradient\\n\\n    @staticmethod\\n    def forward(ctx, input):\\n        \"\"\"\\n        In the forward pass we compute a step function of the input Tensor\\n        and return it. ctx is a context object that we use to stash information which \\n        we need to later backpropagate our error signals. To achieve this we use the \\n        ctx.save_for_backward method.\\n        \"\"\"\\n        ctx.save_for_backward(input)\\n        out = torch.zeros_like...\n        \u001b[0m\u001b[0;36mlocal_ns\u001b[0m \u001b[0;34m= {'__name__': '__main__', '__doc__': 'returns spikes', '__package__': None, '__loader__': None, '__spec__': None, '__builtin__': <module 'builtins' (built-in)>, '__builtins__': <module 'builtins' (built-in)>, '_ih': ['', 'import os\\n\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\nfrom matplotlib.gridspec import GridSpec\\nimport seaborn as sns\\n\\nimport torch\\nimport torch.nn as nn\\nimport torchvision\\n\\nimport pandas as pd', '# The coarse network structure is dicated by the Fashion MNIST dataset. \\nnb_inputs  = 28*28\\nnb_hidden  = 100\\nnb_outputs = 10\\n\\ntime_step = 1e-3\\nnb_steps  = 100\\n\\nbatch_size = 256', 'dtype = torch.float\\n\\n# Check whether a GPU is available\\nif torch.cuda.is_available():\\n    device = torch.device(\"cuda\")\\n    print(\"cuda\")     \\nelse:\\n    device = torch.device(\"cpu\")\\n    print(\"cpu\")', '# Here we load the Dataset\\nroot = os.path.expanduser(\"~/data/datasets/torch/fashion-mnist\")\\ntrain_dataset = torchvision.datasets.FashionMNIST(root, train=True, transform=None, target_transform=None, download=True)\\ntest_dataset = torchvision.datasets.FashionMNIST(root, train=False, transform=None, target_transform=None, download=True)', '# Standardize data\\n#x_train = torch.tensor(train_dataset.train_data, device=device, dtype=dtype)\\nx_train = np.array(train_dataset.data, dtype=np.float)\\nx_train = x_train.reshape(x_train.shape[0],-1)/255\\n#x_test = torch.tensor(test_dataset.test_data, device=device, dtype=dtype...\ncount  10000.0  10000.0  10000.000000  ...  10000.000000  10000.000000  10000.000000\nmean       1.0      1.0      1.003667  ...      1.512261      1.232128      1.032410\nstd        0.0      0.0      0.239659  ...      5.421710      4.847282      2.899793\nmin        1.0      1.0      1.000000  ...      1.000000      1.000000      1.000000\n25%        1.0      1.0      1.000000  ...      1.000000      1.000000      1.000000\n50%        1.0      1.0      1.000000  ...      1.000000      1.000000      1.000000\n75%        1.0      1.0      1.000000  ...      1.000000      1.000000      1.000000\nmax        1.0      1.0     21.217439  ...    290.173165    290.173165    290.173165\n\n[8 rows x 784 columns]}, '_dh': ['/content'], '_sh': <module 'IPython.core.shadowns' from '/usr/local/lib/python3.7/dist-packages/IPython/core/shadowns.py'>, 'In': ['', 'import os\\n\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\nfrom matplotlib.gridspec import GridSpec\\nimport seaborn as sns\\n\\nimport torch\\nimport torch.nn as nn\\nimport torchvision\\n\\nimport pandas as pd', '# The coarse network structure is dicated by the Fashion MNIST dataset. \\nnb_inputs  = 28*28\\nnb_hidden  = 100\\nnb_outputs = 10\\n\\ntime_step = 1e-3\\nnb_steps  = 100\\n\\nbatch_size = 256', 'dtype = torch.float\\n\\n# Check whether a GPU is available\\nif torch.cuda.is_available():\\n    device = torch.device(\"cuda\")\\n    print(\"cuda\")     \\nelse:\\n    device = torch.device(\"cpu\")\\n    print(\"cpu\")', '# Here we load the Dataset\\nroot = os.path.expanduser(\"~/data/datasets/torch/fashion-mnist\")\\ntrain_dataset = torchvision.datasets.FashionMNIST(root, train=True, transform=None, target_transform=None, download=True)\\ntest_dataset = torchvision.datasets.FashionMNIST(root, train=False, transform=None, target_transform=None, download=True)', '# Standardize data\\n#x_train = torch.tensor(train_dataset.train_data, device=device, dtype=dtype)\\nx_train = np.array(train_dataset.data, dtype=np.float)\\nx_train = x_train.reshape(x_train.shape[0],-1)/255\\n#x_test = torch.tensor(test_dataset.test_data, device=device, dtype=dtype)\\nx_test = np.array(test_dataset.data, dtype=np.float)\\nx_test = x_test.reshape(x_test.shap...\ncount  10000.0  10000.0  10000.000000  ...  10000.000000  10000.000000  10000.000000\nmean       1.0      1.0      1.003667  ...      1.512261      1.232128      1.032410\nstd        0.0      0.0      0.239659  ...      5.421710      4.847282      2.899793\nmin        1.0      1.0      1.000000  ...      1.000000      1.000000      1.000000\n25%        1.0      1.0      1.000000  ...      1.000000      1.000000      1.000000\n50%        1.0      1.0      1.000000  ...      1.000000      1.000000      1.000000\n75%        1.0      1.0      1.000000  ...      1.000000      1.000000      1.000000\nmax        1.0      1.0     21.217439  ...    290.173165    290.173165    290.173165\n\n[8 rows x 784 columns]}, 'get_ipython': <bound method InteractiveShell.get_ipython of <google.colab._shell.Shell object at 0x7fa1ec099750>>, 'exit': <IPython.core.autocall.ZMQExitAutocall object at 0x7fa1e7b79a50>, 'quit': <IPython.core.autocall.ZMQExitAutocall object at 0x7fa1e7b79a50>, '_':            0        1             2    ...           781           782           783\ncount  10000.0  10000.0  10000.000000  ...  10000.000000  10000.000000  10000.000000\nmean       1.0      1.0      1.003667  ...      1.512261      1.232128      1.032410\nstd        0.0      0.0      0.239659  ...      5.421710      4.847282      2.899793\nmin        1.0      1.0      1.000000  ...      1.000000      1.000000      1.000000\n25%        1.0      1.0      1.000000  ...      1.000000      1.000000      1.000000\n50%        1.0      1.0      1.000000  ...      1.000000      1.000000      1.000000\n75%        1.0      1.0      1.000000  ...      1.000000      1.000000      1.000000\nmax        1.0      1.0     21.217439  ...    290.173165    290.173165    290.173165\n\n[8 rows x 784 columns], '__': (-0.5, 27.5, 27.5, -0.5), '___': (60000,), '_i': '#@mpc.run_multiprocess(world_size=2)\\ndef compute_classification_accuracy_test_2(x_data, y_data):\\n    \"\"\" Computes classification accuracy on supplied data in batches. \"\"\"\\n\\n    w1_enc=crypten.cryptensor(w1,src=0)\\n    w2_enc=crypten.cryptensor(w2,src=0)\\n    #print(w1_enc.device)\\n\\n    #print(w2_enc.device)\\n\\n    \\n    \\n    accs = []\\n    \\n    for x_local, y_local in sparse_data_generator(x_data, y_data, batch_size, nb_steps, nb_inputs, shuffle=False):\\n        \\n        x_data_enc=crypten.cryptensor(x_local.to_dense())\\n        #print(x_data_enc.device)\\n        for i in range(batch_size):\\n          batch_am=[]\\n          output_enc,_ = run_snn_test_2(x_data_enc[i],w1_enc,w2_enc)\\n          #print(output_enc.size())\\n          output=output_enc.get_plain_text()\\n          m,_= torch.max(output,1) # max over time\\n          #print(m.size())\\n          _,am=torch.max(m,0)# argmax over output units\\n          #print(am)\\n          batch_am.append(am) \\n        batch_am_stack=torch.stack(batch_am)       \\n        tmp = np.mean((y_local==batch_am_stack).detach().cpu().numpy()) # compare to labels\\n        accs.append(tmp)\\n        print(tmp)\\n          \\n    return np.mean(accs)', '_ii': 'def run_snn_test_2(inputs,w1_enc,w2_enc):\\n    #h1 = torch.einsum(\"abc,cd->abd\", (inputs, w1))\\n    syn = torch.zeros((nb_hidden), device=device, dtype=dtype)\\n    mem = torch.zeros((nb_hidden), device=dev...\n    Number of datapoints: 60000\n    Root location: /root/data/datasets/torch/fashion-mnist\n    Split: Train, 'test_dataset': Dataset FashionMNIST\n    Number of datapoints: 10000\n    Root location: /root/data/datasets/torch/fashion-mnist\n    Split: Test, '_i5': '# Standardize data\\n#x_train = torch.tensor(train_dataset.train_data, device=device, dtype=dtype)\\nx_train = np.array(train_dataset.data, dtype=np.float)\\nx_train = x_train.reshape(x_train.shape[0],-1)/255\\n#x_test = torch.tensor(test_dataset.test_data, device=device, dtype=dtype)\\nx_test = np.array(test_dataset.data, dtype=np.float)\\nx_test = x_test.reshape(x_test.shape[0],-1)/255\\n\\n#y_train = torch.tensor(train_dataset.train_labels, device=device, dtype=dtype)\\n#y_test  = torch.tensor(test_dataset.test_labels, device=device, dtype=dtype)\\ny_train = np.array(train_dataset.targets, dtype=np.int)\\ny_test  = np.array(test_dataset.targets, dtype=np.int)\\n\\ny_train.shape', 'x_train': array([[0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       ...,\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.]]), 'x_test': array([[0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       ...,\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.]]), 'y_train': array([9, 0, 0, ..., 3, 0, 5]), 'y_test': array([9, 2, 1, ..., 8, 1, 5]), '_5': (60000,), '_i6': '# Here we plot one of the raw data points as an example\\ndata_id = 1\\nplt.imshow(x_train[data_id].reshape(28,28), cmap=plt.cm.gray_r)\\nplt.axis(\"off\")', 'data_id': 1, '_6': (-0.5, 27.5, 27.5, -0.5), '_i7': 'def current2firing_time(x, tau=20, thr=0.2, tmax=1.0, epsilon=1e-7):\\n    \"\"\" Computes first firing time latency for a current input x assuming the charge time of a current based LIF neuron.\\n\\n    Args:\\n    x -- The \"current\" values\\n\\n    Keyword args:\\n    tau -- The membrane time constant of the LIF neuron to be charged\\n    thr -- The firing threshold value \\n    tmax -- The maximum time returned \\n    epsilon -- A generic (small) epsilon > 0\\n\\n    Returns:\\n    Time to first spike for each \"current\" x\\n    \"\"\"\\n    idx = x<thr\\n    x = np.clip(x,thr+epsilon,1e9)\\n    T = tau*np.log(x/(x-thr))\\n    # T in [1.5,109]\\n    T[idx] = tmax\\n\\n    return T\\n \\n\\ndef sparse_data_generator(X, y, batch_size, nb_steps, nb_units, shuffle=True ):\\n    \"\"\" This generator takes datasets in analog format and generates spiking network input as sparse tensors. \\n\\n    Args:\\n        X: The data ( sample x event x 2 ) the last dim holds (time,neuron) tuples\\n        y: The labels\\n        nb_steps: number of time steps\\n        nb_units: 784 (28x28)\\n    \"\"\"\\n\\n    labels_ = np.array(y,dtype=np.int)\\n    number_of_batches = len(X)//bat...\n       [1., 1., 1., ..., 1., 1., 1.],\n       [1., 1., 1., ..., 1., 1., 1.],\n       ...,\n       [1., 1., 1., ..., 1., 1., 1.],\n       [1., 1., 1., ..., 1., 1., 1.],\n       [1., 1., 1., ..., 1., 1., 1.]]), 'df':       0    1    2    3    4    ...       779        780  781  782  783\n0     1.0  1.0  1.0  1.0  1.0  ...  1.000000   1.000000  1.0  1.0  1.0\n1     1.0  1.0  1.0  1.0  1.0  ...  6.289867  28.642078  1.0  1.0  1.0\n2     1.0  1.0  1.0  1.0  1.0  ...  1.000000   1.000000  1.0  1.0  1.0\n3     1.0  1.0  1.0  1.0  1.0  ...  1.000000   1.000000  1.0  1.0  1.0\n4     1.0  1.0  1.0  1.0  1.0  ...  1.000000   1.000000  1.0  1.0  1.0\n...   ...  ...  ...  ...  ...  ...       ...        ...  ...  ...  ...\n9995  1.0  1.0  1.0  1.0  1.0  ...  1.000000   1.000000  1.0  1.0  1.0\n9996  1.0  1.0  1.0  1.0  1.0  ...  1.000000   1.000000  1.0  1.0  1.0\n9997  1.0  1.0  1.0  1.0  1.0  ...  1.000000   1.000000  1.0  1.0  1.0\n9998  1.0  1.0  1.0  1.0  1.0  ...  1.000000   1.000000  1.0  1.0  1.0\n9999  1.0  1.0  1.0  1.0  1.0  ...  1.000000   1.000000  1.0  1.0  1.0\n\n[10000 rows x 784 columns], '_8':            0        1             2    ...           781           782           783\ncount  10000.0  10000.0  10000.000000  ...  10000.000000  10000.000000  10000.000000\nmean       1.0      1.0      1.003667  ...      1.512261      1.232128      1.032410\nstd        0.0      0.0      0.239659  ...      5.421710      4.847282      2.899793\nmin        1.0      1.0      1.000000  ...      1.000000      1.000000      1.000000\n25%        1.0      1.0      1.000000  ...      1.000000      1.000000      1.000000\n50%        1.0      1.0      1.000000  ...      1.000000      1.000000      1.000000\n75%        1.0      1.0      1.000000  ...      1.000000      1.000000      1.000000\nmax        1.0      1.0     21.217439  ...    290.173165    290.173165    290.173165\n\n[8 rows x 784 columns], '_i9': 'tau_mem = 10e-3\\ntau_syn = 5e-3\\n\\nalpha   = float(np.exp(-time_step/tau_syn))\\nbeta    = float(np.exp(-time_step/tau_mem))', 'tau_mem': 0.01, 'tau_syn': 0.005, 'alpha': 0.8187307530779818, 'beta': 0.9048374180359595, '_i10': 'weight_scale = 7*(1.0-beta) # this should give us some spikes to begin with\\n\\nw1 = torch.empty((nb_inputs, nb_hidden),  device=device, dtype=dtype, requires_grad=True)\\ntorch.nn.init.normal_(w1, mean=0.0, std=weight_scale/np.sqrt(nb_inputs))\\n\\nw2 = torch.empty((nb_hidden, nb_outputs), device=device, dtype=dtype, requires_grad=True)\\ntorch.nn.init.normal_(w2, mean=0.0, std=weight_scale/np.sqrt(nb_hidden))\\n\\nprint(\"init done\")', 'weight_scale': 0.6661380737482834, 'w1': tensor([[ 0.0317, -0.0074, -0.0046,  ..., -0.0084,  0.0029,  0.0114],\n        [-0.0092, -0.0094,  0.0259,  ..., -0.0285, -0.0183,  0.0310],\n        [-0.0434, -0.0603,  0.0610,  ..., -0.0179, -0.0341,  0.0463],\n        ...,\n        [-0.0010,  0.0624,  0.0427,  ..., -0.0107,  0.0160,  0.0125],\n        [ 0.0103,  0.0134,  0.0044,  ...,  0.0494,  0.0418,  0.0405],\n        [-0.0149, -0.0155,  0.0498,  ..., -0.0137,  0.0195,  0.0248]],\n       device='cuda:0', requires_grad=True), 'w2': tensor([[-0.1986,  0.1029,  0.2804, -0.1246,  0.0824,  0.0441, -0.1294, -0.0443,\n         -0.0726, -0.1272],\n        [-0.0771,  0.0405, -0.2326, -0.0589,  0.0478,  0.0490,  0.0410, -0.0368,\n         -0.0633,  0.2318],\n        [-0.0271, -0.1260, -0.0960,  0.2241, -0.0701,  0.0135,  0.2543, -0.0294,\n         -0.1198,  0.1058],\n        [ 0.1433, -0.0080, -0.0431, -0.0273, -0.0404, -0.0974,  0.0245,  0.1444,\n         -0.1474, -0.0234],\n        [-0.1444, -0.1040, -0.0937, -0.1058, -0.1024,  0.1221,  0.1276, -0.2422,\n          0.0444, -0.0340],\n        [ 0.1768,  0.0072, -0.0699, -0.0142, -0.2202,  0.3443,  0.0768, -0.0694,\n         -0.1682, -0.1279],\n        [-0.0887, -0.0579, -0.0455, -0.0285, -0.1044,  0.1477,  0.1423, -0.1033,\n         -0.3140,  0.1165],\n        [ 0.0872, -0.0269, -0.0572, -0.3258,  0.4544, -0.1016, -0.1280, -0.1004,\n         -0.1291, -0.1475],\n        [ 0.0967, -0.1322, -0.2227,  0.0672, -0.2587, -0.1005,  0.1743, -0.2541,\n         -0.0507, -0.0171],\n        [-0.0974, -0.1763,  0.0565, -0.1535,  0.0510,  0.1089, -0.1275,  0.1967,\n          0.0273,  0.0076],\n        [-0.0054,  0.2067, -0.0130, -0.1927, -0.0339,  0.1099, -0.0554, -0.0558,\n          0.2462, -0.2266],\n        [ 0.1419,  0.1212, -0.1839, -0.0392, -0.0719,  0.2055, -0.0313,  0.0519,\n         -0.1927, -0.0945],\n        [ 0.0581,  0.1463,  0.0111,  0.1325,  0.0344, -0.1576,  0.0500, -0.0229,\n         -0.1456, -0.1273],\n        [-0.1039, -0.0564, -0.1265,  0.1856,  0.0780,  0.1636, -0.1334,  0.0462,\n          0.0505, -0.1597],\n        [-0.0236, -0.0816, -0.0868,  0.2333, -0.1087,  0.2394, -0.1062, -0.1765,\n          0.0720, -0.1706],\n        [ 0.0278, -0.0397, -0.0354,  0.1376, -0.1149, -0.0461,  0.1464,  0.1325,\n         -0.0953,  0.0613],\n        [-0.1972,  0.1387, -0.1392,  0.1178,  0.0868,  0.0254, -0.1908, -0.0320,\n          0.1167, -0.1479],\n        [ 0.2684, -0.2125,  0.0666, -0.2469, -0.0369, -0.0407, -0.0222, -0.0328,\n          0.2151, -0.0287],\n        [ 0.0713, -0.1425,  0.0958,  0.1002, -0.0682,  0.0389, -0.1760, -0.0601,\n         -0.0703, -0.0676],\n        [ 0.0543,  0.0272, -0.0667, -0.0356,  0.0596, -0.2005, -0.0784, -0.0095,\n          0.1375,  0.0388],\n        [-0.0628,  0.1673, -0.0719, -0.0671,  0.0424, -0.1292, -0.0813,  0.0453,\n          0.0261,  0.1298],\n        [-0.1374, -0.1551, -0.1111, -0.0494,  0.0023, -0.1994,  0.3621, -0.0354,\n         -0.0389, -0.0478],\n        [ 0.1390, -0.0687, -0.0837,  0.0216,  0.0427,  0.1538,  0.1128, -0.0244,\n          0.1929, -0.0919],\n        [-0.1191, -0.2929, -0.1157,  0.1735,  0.1025, -0.0301,  0.0198, -0.1147,\n         -0.0497,  0.1266],\n        [ 0.0279, -0.1403, -0.1298,  0.0843, -0.0258, -0.0661, -0.0360, -0.0266,\n          0.3058, -0.1369],\n        [-0.1231,  0.1459, -0.0190,  0.0684, -0.0443, -0.0512, -0.1146,  0.1129,\n          0.0310,  0.0770],\n        [ 0.1087, -0.1419,  0.0297,  0.1609,  0.0516, -0.1239,  0.0241, -0.1560,\n          0.0375, -0.0590],\n        [-0.1057, -0.1465, -0.0808, -0.0159,  0.1380,  0.1348,  0.0731, -0.2030,\n         -0.0269, -0.0436],\n        [-0.1209,  0.0519,  0.2338, -0.0355, -0.2017, -0.0655,  0.0617,  0.2159,\n         -0.0340, -0.1140],\n        [ 0.0881, -0.0312, -0.1829, -0.1073, -0.0492, -0.1133,  0.0604,  0.3560,\n         -0.0958, -0.1451],\n        [ 0.1689, -0.0585,  0.0377, -0.1067,  0.1568, -0.2171, -0.0846,  0.1517,\n         -0.1095, -0.0404],\n        [ 0.1264, -0.1758, -0.0290, -0.0295, -0.3027,  0.1804,  0.1851,  0.0719,\n          0.1475, -0.1482],\n        [-0.0723, -0.1512, -0.1220, -0.0515, -0.0859, -0.0388,  0.0073, -0.0033,\n         -0.1413, -0.0705],\n        [-0.0528, -0.1508,  0.1466,  0.0229,  0.1729,  0.0838,  0.0931,  0.0381,\n         -0.2061, -0.1147],\n        [ 0.0147,  0.1958, -0.1150, -0.0016, -0.0463,  0.0309,  0.1752,  0.1266,\n         -0.2362, -0.1775],\n        [ 0.0137,  0.0147,  0.0932,  0.0671, -0.0242, -0.0685,  0.1872, -0.1139,\n          0.0881, -0.0808],\n        [ 0.0221,  0.0565,  0.0913, -0.0263,  0.0423, -0.1889,  0.1201, -0.0057,\n          0.1113,  0.0606],\n        [-0.0235,  0.0289, -0.1385,  0.1174, -0.1337, -0.1040, -0.0968,  0.1846,\n          0.2059,  0.0364],\n        [ 0.0109,  0.0159, -0.1398, -0.0025,  0.0988, -0.0477, -0.1744, -0.0472,\n          0.1052, -0.0528],\n        [ 0.0478, -0.1337,  0.1342, -0.0857,  0.0118, -0.1359, -0.2749,  0.2911,\n         -0.1627, -0.1562],\n        [ 0.0027,  0.0250,  0.0668, -0.0811,  0.0021,  0.0919,  0.0759,  0.0816,\n          0.0749,  0.0301],\n        [ 0.1854, -0.3053, -0.1311,  0.0279, -0.0955, -0.0176,  0.0727,  0.1239,\n          0.1995,  0.0237],\n        [ 0.1766,  0.0107,  0.1013, -0.1957, -0.0211, -0.0329,  0.1159,  0.0534,\n         -0.0885, -0.1707],\n        [ 0.1058, -0.0067,  0.0040,  0.0398, -0.1178,  0.1276,  0.0227,  0.0418,\n          0.0446,  0.0754],\n        [ 0.0682,  0.1378,  0.1058, -0.0457, -0.0081,  0.1124, -0.1298, -0.2169,\n         -0.1177, -0.1897],\n        [-0.0680,  0.0818,  0.0551,  0.0043, -0.1539, -0.2233, -0.0729,  0.1038,\n          0.0728, -0.0083],\n        [-0.1157, -0.1106, -0.0348,  0.3660,  0.0032, -0.0375, -0.0992,  0.0456,\n         -0.0838, -0.0676],\n        [ 0.0697, -0.1788,  0.3058,  0.1759, -0.1803, -0.0506, -0.1526, -0.0622,\n         -0.1908, -0.0884],\n        [ 0.1225,  0.0300,  0.0244,  0.0336,  0.0542, -0.1572,  0.0780,  0.0010,\n         -0.0970,  0.0749],\n        [-0.1630, -0.0252,  0.3333,  0.1130, -0.1956, -0.2423, -0.0111, -0.0027,\n         -0.0362,  0.1799],\n        [-0.0332,  0.0205, -0.1079, -0.1339,  0.1632, -0.0017, -0.0008, -0.0537,\n          0.0314,  0.1506],\n        [-0.0584, -0.0711,  0.0525,  0.0371,  0.0290,  0.0778, -0.0537,  0.0853,\n          0.0786, -0.0170],\n        [ 0.1132, -0.0997,  0.1461, -0.1337, -0.0739, -0.1184,  0.0796, -0.1403,\n          0.0315, -0.0427],\n        [ 0.0877,  0.0623, -0.0632, -0.1174,  0.0966, -0.1611,  0.0186,  0.0303,\n          0.1657, -0.0290],\n        [-0.0759,  0.1489,  0.0753, -0.0110, -0.0232,  0.0845,  0.0591, -0.1456,\n         -0.0223, -0.0675],\n        [-0.0475,  0.1977, -0.0568,  0.1006, -0.2473, -0.0866,  0.0106, -0.0184,\n          0.1187, -0.0346],\n        [-0.0820,  0.0868,  0.0773,  0.0904,  0.1253, -0.1189,  0.0369, -0.0630,\n         -0.0879, -0.0819],\n        [-0.1307,  0.2978, -0.1887, -0.1105,  0.1231,  0.0588, -0.0661, -0.1602,\n         -0.1989, -0.1269],\n        [ 0.0984, -0.0740,  0.0468, -0.2023, -0.1810,  0.1684,  0.0628, -0.0212,\n         -0.1441,  0.0046],\n        [-0.0171, -0.0609,  0.0694, -0.1318,  0.1043, -0.0652,  0.0750,  0.0592,\n          0.0539, -0.1655],\n        [-0.0831,  0.2221,  0.1359,  0.0724,  0.0781,  0.0715, -0.2118, -0.1426,\n         -0.0074, -0.0306],\n        [-0.0550,  0.1327, -0.1862, -0.1174, -0.1535, -0.2597, -0.0519,  0.1466,\n          0.0276,  0.2557],\n        [ 0.0750, -0.0354, -0.0103, -0.0550,  0.0094, -0.0045,  0.0935,  0.0133,\n         -0.0058, -0.0526],\n        [ 0.0909,  0.2230, -0.0917,  0.1488, -0.0075,  0.0181, -0.0332, -0.1026,\n         -0.1667, -0.0719],\n        [ 0.0230, -0.0966,  0.0681, -0.2221, -0.0749,  0.1228, -0.0590, -0.0878,\n         -0.2455,  0.1010],\n        [-0.1732,  0.0134,  0.0947,  0.1239, -0.1481,  0.2219, -0.0239, -0.2041,\n          0.1625, -0.0578],\n        [-0.0044, -0.1028, -0.1034, -0.0580,  0.3344, -0.0338, -0.0872,  0.1171,\n         -0.0975, -0.2122],\n        [ 0.0647, -0.0044,  0.0661,  0.1275,  0.0718, -0.1133, -0.0393, -0.1307,\n          0.0262,  0.0067],\n        [ 0.0084, -0.0591,  0.1222, -0.0194,  0.1739,  0.0586, -0.1299, -0.0604,\n          0.1219, -0.2655],\n        [-0.0189, -0.0400,  0.0212,  0.0866,  0.0961,  0.2202,  0.0264,  0.0176,\n          0.0143,  0.0424],\n        [ 0.1166, -0.1352,  0.2252, -0.0949, -0.0639,  0.1543, -0.1096, -0.1480,\n         -0.0601, -0.0023],\n        [-0.0282,  0.1385,  0.1827,  0.0920, -0.0596,  0.0841, -0.0673,  0.0702,\n         -0.1045,  0.1696],\n        [-0.0789, -0.0854, -0.0253, -0.0179,  0.1091, -0.1932,  0.2328, -0.1380,\n         -0.3146, -0.0337],\n        [ 0.0275, -0.0505,  0.0420, -0.0873,  0.0222, -0.2003,  0.0676, -0.0591,\n         -0.0684, -0.1267],\n        [ 0.1602,  0.0662, -0.0210, -0.1054,  0.1088, -0.0827,  0.0719,  0.1331,\n         -0.0789,  0.0448],\n        [-0.0628, -0.1487, -0.1589,  0.0571,  0.0401, -0.1933, -0.1505,  0.2391,\n          0.0094,  0.0839],\n        [ 0.0052, -0.1301, -0.0514,  0.0077,  0.0713, -0.0794,  0.0172, -0.0529,\n         -0.0365,  0.0935],\n        [-0.1054, -0.1724, -0.0813,  0.0056, -0.1422, -0.2807, -0.0501,  0.0947,\n          0.3366, -0.1593],\n        [ 0.1211,  0.1327,  0.1332, -0.0148, -0.0580,  0.0146, -0.0060, -0.0330,\n         -0.3109, -0.0572],\n        [ 0.0048, -0.1166, -0.0625, -0.0788,  0.0642,  0.3513,  0.1839, -0.0978,\n         -0.0079, -0.1909],\n        [ 0.0941, -0.1040,  0.2028,  0.0221, -0.0503,  0.0050, -0.1797, -0.1263,\n         -0.1084,  0.0334],\n        [-0.1489, -0.1613, -0.1127, -0.0964, -0.0460, -0.1206,  0.1914,  0.0107,\n          0.1277, -0.0326],\n        [-0.1679, -0.0056,  0.0693, -0.0354,  0.1590, -0.0951,  0.0225, -0.0171,\n          0.2011, -0.0967],\n        [-0.0426, -0.1441, -0.0676, -0.0986, -0.0863,  0.3538, -0.0219, -0.1838,\n          0.0792, -0.1597],\n        [-0.0056, -0.0513, -0.0636,  0.0196,  0.0356, -0.0695,  0.2944, -0.0729,\n         -0.0720, -0.0290],\n        [-0.1267,  0.3450, -0.1943,  0.1044,  0.0271,  0.0660, -0.1011,  0.0610,\n          0.0803,  0.1605],\n        [-0.1311,  0.0280, -0.0164, -0.0447,  0.0288, -0.0702,  0.0929, -0.1607,\n          0.1414,  0.1704],\n        [ 0.0570,  0.0851,  0.0618, -0.1352,  0.1811, -0.0506, -0.0435, -0.1860,\n          0.0746, -0.0197],\n        [ 0.2024, -0.1481,  0.0115, -0.0476, -0.1410, -0.0696, -0.0949,  0.0870,\n         -0.0231,  0.0101],\n        [ 0.0962,  0.1031,  0.1220,  0.0736,  0.0366, -0.0543, -0.0928, -0.1079,\n          0.1275, -0.0523],\n        [-0.0869, -0.0895,  0.0215,  0.0966, -0.2076, -0.1393, -0.1579,  0.1928,\n         -0.2106, -0.0879],\n        [ 0.0558, -0.0662,  0.0213, -0.1372, -0.1480,  0.3953,  0.1189, -0.2241,\n         -0.0923, -0.1733],\n        [ 0.3835, -0.0052, -0.0122,  0.0485, -0.1948, -0.1183, -0.1600, -0.1797,\n         -0.1680, -0.0013],\n        [-0.0765, -0.1223,  0.1224,  0.2189, -0.1172, -0.0610,  0.0329,  0.2054,\n         -0.1402, -0.0905],\n        [ 0.0771, -0.0799, -0.1385, -0.0524,  0.0225,  0.3306,  0.1590, -0.2098,\n         -0.1429, -0.1242],\n        [-0.0827, -0.1647, -0.0895,  0.0197, -0.0412, -0.0297,  0.2319,  0.0133,\n         -0.2460,  0.0670],\n        [-0.0944, -0.1014, -0.0261, -0.0015, -0.0615, -0.1928, -0.1582,  0.0055,\n          0.2352,  0.0826],\n        [ 0.0535, -0.0082, -0.1263, -0.1443, -0.1309, -0.0855, -0.0126,  0.0866,\n          0.0490,  0.1650],\n        [ 0.0274, -0.0824,  0.1809,  0.1854, -0.1224, -0.1543, -0.1143, -0.0405,\n         -0.0611, -0.0475],\n        [ 0.1210, -0.0325, -0.1075, -0.1316, -0.1749,  0.1951, -0.1079,  0.0284,\n         -0.0043,  0.0729]], device='cuda:0', requires_grad=True), '_i11': 'def plot_voltage_traces(mem, spk=None, dim=(3,5), spike_height=5):\\n    gs=GridSpec(*dim)\\n    if spk is not None:\\n        dat = 1.0*mem\\n        dat[spk>0.0] = spike_height\\n        dat = dat.detach().cpu().numpy()\\n    else:\\n        dat = mem.detach().cpu().numpy()\\n    for i in range(np.prod(dim)):\\n        if i==0: a0=ax=plt.subplot(gs[i])\\n        else: ax=plt.subplot(gs[i],sharey=a0)\\n        ax.plot(dat[i])\\n        ax.axis(\"off\")', 'plot_voltage_traces': <function plot_voltage_traces at 0x7fa0d003a050>, '_i12': 'class SurrGradSpike(torch.autograd.Function):\\n    \"\"\"\\n    Here we implement our spiking nonlinearity which also implements \\n    the surrogate gradient. By subclassing torch.autograd.Function, \\n    we will be able to use all of PyTorch\\'s autograd functionality.\\n    Here we use the normalized negative part of a fast sigmoid \\n    as this was done in Zenke & Ganguli (2018).\\n    \"\"\"\\n    \\n    scale = 100.0 # controls steepness of surrogate gradient\\n\\n    @staticmethod\\n    def forward(ctx, input):\\n        \"\"\"\\n        In the forward pass we compute a step function of the input Tensor\\n        and return it. ctx is a context object that we use to stash information which \\n        we need to later backpropagate our error signals. To achieve this we use the \\n        ctx.save_for_backward method.\\n        \"\"\"\\n        ctx.save_for_backward(input)\\n        out = torch.zeros_like...\n\u001b[1;32m   1194\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1195\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m<ipython-input-35-b0fa13cfff52>\u001b[0m in \u001b[0;36mcompute_classification_accuracy_test_2\u001b[0;34m(x_data=array([[0., 0., 0., ..., 0., 0., 0.],\n       [0...., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.]]), y_data=array([9, 2, 1, ..., 8, 1, 5]))\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m           \u001b[0mbatch_am\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m           \u001b[0moutput_enc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_snn_test_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_data_enc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw1_enc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw2_enc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m        \u001b[0;36moutput_enc\u001b[0m \u001b[0;34m= MPCTensor(\n\t_tensor=CUDALongTensor(tensor([[  0,   0,   0,  ...,   3,   2,   1],\n        [  0,   0,   0,  ...,  -6,  -5,  -4],\n        [  0,   0,   0,  ..., -22, -19, -17],\n        ...,\n        [  0,   0,   0,  ...,  -8,  -7,  -6],\n        [  0,   0,   0,  ..., -40, -36, -32],\n        [  0,   0,   0,  ...,  -5,  -4,  -3]], device='cuda:0'))\n\tplain_text=HIDDEN\n\tptype=ptype.arithmetic\n)\u001b[0m\u001b[0;34m\n        \u001b[0m\u001b[0;36m_\u001b[0m \u001b[0;34m= tensor(0.8135, device='cuda:0')\u001b[0m\u001b[0;34m\n        \u001b[0m\u001b[0;36mglobal\u001b[0m \u001b[0;36mrun_snn_test_2\u001b[0m \u001b[0;34m= <function run_snn_test_2 at 0x7fa05129b290>\u001b[0m\u001b[0;34m\n        \u001b[0m\u001b[0;36mx_data_enc\u001b[0m \u001b[0;34m= MPCTensor(\n\t_tensor=CUDALongTensor(tensor([[[0, 0, 0,  ..., 0, 0, 0],\n         [0, 0, 0,  ..., 0, 0, 0],\n         [0, 0, 0,  ..., 0, 0, 0],\n         ...,\n         [0, 0, 0,  ..., 0, 0, 0],\n         [0, 0, 0,  ..., 0, 0, 0],\n         [0, 0, 0,  ..., 0, 0, 0]],\n\n        [[0, 0, 0,  ..., 0, 0, 0],\n         [0, 0, 0,  ..., 0, 0, 0],\n         [0, 0, 0,  ..., 0, 0, 0],\n         ...,\n         [0, 0, 0,  ..., 0, 0, 0],\n         [0, 0, 0,  ..., 0, 0, 0],\n         [0, 0, 0,  ..., 0, 0, 0]],\n\n        [[0, 0, 0,  ..., 0, 0, 0],\n         [0, 0, 0,  ..., 0, 0, 0],\n         [0, 0, 0,  ..., 0, 0, 0],\n         ...,\n         [0, 0, 0,  ..., 0, 0, 0],\n         [0, 0, 0,  ..., 0, 0, 0],\n         [0, 0, 0,  ..., 0, 0, 0]],\n\n        ...,\n\n        [[0, 0, 0,  ..., 0, 0, 0],\n         [0, 0, 0,  ..., 0, 0, 0],\n         [0, 0, 0,  ..., 0, 0, 0],\n         ...,\n         [0, 0, 0,  ..., 0, 0, 0],\n         [0, 0, 0,  ..., 0, 0, 0],\n         [0, 0, 0,  ..., 0, 0, 0]],\n\n        [[0, 0, 0,  ..., 0, 0, 0],\n         [0, 0, 0,  ..., 0, 0, 0],\n         [0, 0, 0,  ..., 0, 0, 0],\n         ...,\n         [0, 0, 0,  ..., 0, 0, 0],\n         [0, 0, 0,  ..., 0, 0, 0],\n         [0, 0, 0,  ..., 0, 0, 0]],\n\n        [[0, 0, 0,  ..., 0, 0, 0],\n         [0, 0, 0,  ..., 0, 0, 0],\n         [0, 0, 0,  ..., 0, 0, 0],\n         ...,\n         [0, 0, 0,  ..., 0, 0, 0],\n         [0, 0, 0,  ..., 0, 0, 0],\n         [0, 0, 0,  ..., 0, 0, 0]]], device='cuda:0'))\n\tplain_text=HIDDEN\n\tptype=ptype.arithmetic\n)\u001b[0m\u001b[0;34m\n        \u001b[0m\u001b[0;36mi\u001b[0m \u001b[0;34m= 165\u001b[0m\u001b[0;34m\n        \u001b[0m\u001b[0;36mw1_enc\u001b[0m \u001b[0;34m= MPCTensor(\n\t_tensor=CUDALongTensor(tensor([[ 2079,  -485,  -301,  ...,  -552,   187,   749],\n        [ -605,  -614,  1695,  ..., -1870, -1200,  2031],\n        [-2842, -3948,  4000,  ..., -1175, -2232,  3036],\n        ...,\n        [  -62,  4090,  2798,  ...,  -698,  1051,   821],\n        [  673,   880,   288,  ...,  3235,  2738,  2654],\n        [ -978, -1018,  3265,  ...,  -895,  1279,  1624]], device='cuda:0'))\n\tplain_text=HIDDEN\n\tptype=ptype.arithmetic\n)\u001b[0m\u001b[0;34m\n        \u001b[0m\u001b[0;36mw2_enc\u001b[0m \u001b[0;34m= MPCTensor(\n\t_tensor=CUDALongTensor(tensor([[-13016,   6746,  18377,  -8164,   5398,   2889,  -8478,  -2901,  -4755,\n          -8333],\n        [ -5054,   2656, -15241,  -3858,   3131,   3210,   2689,  -2412,  -4148,\n          15190],\n        [ -1778,  -8254,  -6294,  14684,  -4597,    883,  16665,  -1924,  -7850,\n           6934],\n        [  9393,   -525,  -2822,  -1787,  -2645,  -6380,   1607,   9460,  -9659,\n          -1533],\n        [ -9465,  -6815,  -6140,  -6932,  -6713,   8001,   8364, -15875,   2909,\n          -2226],\n        [ 11588,    468,  -4582,   -927, -14429,  22562,   5033,  -4545, -11022,\n          -8378],\n        [ -5811,  -3796,  -2984,  -1867,  -6842,   9680,   9326,  -6766, -20578,\n           7636],\n        [  5716,  -1760,  -3747, -21350,  29776,  -6659,  -8389,  -6582,  -8461,\n          -9669],\n        [  6339,  -8666, -14593,   4400, -16950,  -6587,  11426, -16651,  -3322,\n          -1122],\n        [ -6386, -11550,   3703, -10061,   3341,   7135,  -8358,  12892,   1787,\n            498],\n        [  -351,  13544,   -851, -12631,  -2218,   7201,  -3630,  -3656,  16133,\n         -14849],\n        [  9296,   7943, -12052,  -2570,  -4714,  13470,  -2051,   3399, -12627,\n          -6192],\n        [  3808,   9585,    724,   8681,   2254, -10326,   3275,  -1501,  -9543,\n          -8340],\n        [ -6809,  -3698,  -8292,  12163,   5114,  10720,  -8741,   3029,   3310,\n         -10468],\n        [ -1549,  -5348,  -5688,  15288,  -7120,  15691,  -6959, -11564,   4721,\n         -11180],\n        [  1825,  -2600,  -2320,   9015,  -7528,  -3021,   9596,   8681,  -6244,\n           4018],\n        [-12925,   9092,  -9125,   7717,   5688,   1667, -12503,  -2098,   7646,\n          -9689],\n        [ 17592, -13928,   4362, -16180,  -2417,  -2668,  -1456,  -2150,  14094,\n          -1881],\n        [  4670,  -9341,   6275,   6567,  -4470,   2548, -11534,  -3937,  -4604,\n          -4432],\n        [  3559,   1784,  -4370,  -2333,   3905, -13141,  -5139,   -625,   9014,\n           2541],\n        [ -4116,  10961,  -4713,  -4395,   2775,  -8469,  -5326,   2968,   1713,\n           8506],\n        [ -9004, -10162,  -7282,  -3234,    153, -13068,  23733,  -2318,  -2549,\n          -3135],\n        [  9111,  -4499,  -5484,   1413,   2796,  10080,   7390,  -1601,  12641,\n          -6019],\n        [ -7804, -19196,  -7583,  11367,   6718,  -1975,   1295,  -7514,  -3258,\n           8294],\n        [  1830,  -9197,  -8509,   5523,  -1693,  -4331,  -2357,  -1742,  20041,\n          -8968],\n        [ -8064,   9562,  -1244,   4480,  -2905,  -3355,  -7510,   7396,   2029,\n           5044],\n        [  7125,  -9297,   1946,  10543,   3383,  -8121,   1581, -10224,   2455,\n          -3866],\n        [ -6927,  -9600,  -5293,  -1043,   9046,   8835,   4791, -13304,  -1759,\n          -2857],\n        [ -7923,   3400,  15323,  -2324, -13218,  -4292,   4041,  14150,  -2229,\n          -7469],\n        [  5771,  -2046, -11987,  -7034,  -3227,  -7425,   3960,  23331,  -6277,\n          -9508],\n        [ 11070,  -3834,   2470,  -6991,  10275, -14225,  -5546,   9939,  -7175,\n          -2646],\n        [  8283, -11518,  -1902,  -1934, -19835,  11820,  12132,   4709,   9668,\n          -9713],\n        [ -4737,  -9906,  -7998,  -3374,  -5628,  -2545,    481,   -218,  -9259,\n          -4622],\n        [ -3457,  -9885,   9604,   1499,  11327,   5492,   6102,   2496, -13510,\n          -7514],\n        [   961,  12832,  -7536,   -107,  -3036,   2022,  11484,   8298, -15478,\n         -11634],\n        [   898,    960,   6109,   4395,  -1584,  -4486,  12269,  -7463,   5776,\n          -5293],\n        [  1450,   3703,   5981,  -1726,   2774, -12379,   7873,   -372,   7292,\n           3969],\n        [ -1538,   1896,  -9079,   7695,  -8764,  -6817,  -6345,  12096,  13496,\n           2383],\n        [   711,   1044,  -9159,   -166,   6472,  -3128, -11428,  -3091,   6894,\n          -3459],\n        [  3132,  -8762,   8795,  -5613,    771,  -8905, -18015,  19077, -10661,\n         -10239],\n        [   180,   1640,   4375,  -5312,    139,   6021,   4972,   5347,   4910,\n           1970],\n        [ 12152, -20011,  -8589,   1826,  -6260,  -1153,   4764,   8118,  13077,\n           1550],\n        [ 11575,    702,   6638, -12827,  -1380,  -2156,   7596,   3497,  -5798,\n         -11188],\n        [  6936,   -439,    264,   2610,  -7717,   8363,   1488,   2738,   2922,\n           4943],\n        [  4471,   9028,   6935,  -2993,   -531,   7363,  -8506, -14217,  -7711,\n         -12430],\n        [ -4455,   5358,   3613,    280, -10083, -14633,  -4778,   6800,   4768,\n           -543],\n        [ -7580,  -7247,  -2279,  23989,    206,  -2459,  -6503,   2989,  -5490,\n          -4429],\n        [  4567, -11718,  20040,  11528, -11815,  -3314,  -9999,  -4073, -12505,\n          -5795],\n        [  8025,   1966,   1600,   2202,   3549, -10303,   5114,     62,  -6356,\n           4907],\n        [-10683,  -1649,  21844,   7405, -12820, -15881,   -724,   -178,  -2375,\n          11790],\n        [ -2176,   1342,  -7069,  -8773,  10695,   -113,    -50,  -3522,   2056,\n           9871],\n        [ -3827,  -4660,   3442,   2428,   1903,   5096,  -3519,   5593,   5152,\n          -1113],\n        [  7416,  -6535,   9572,  -8759,  -4844,  -7758,   5216,  -9197,   2067,\n          -2797],\n        [  5746,   4081,  -4140,  -7690,   6329, -10560,   1219,   1983,  10857,\n          -1897],\n        [ -4974,   9755,   4931,   -719,  -1517,   5538,   3872,  -9542,  -1460,\n          -4424],\n        [ -3110,  12957,  -3722,   6595, -16209,  -5676,    697,  -1205,   7782,\n          -2266],\n        [ -5371,   5685,   5067,   5925,   8210,  -7790,   2420,  -4126,  -5762,\n          -5364],\n        [ -8563,  19514, -12363,  -7242,   8070,   3851,  -4333, -10497, -13034,\n          -8316],\n        [  6448,  -4850,   3067, -13254, -11859,  11035,   4115,  -1387,  -9443,\n            302],\n        [ -1120,  -3991,   4547,  -8634,   6834,  -4273,   4915,   3878,   3531,\n         -10842],\n        [ -5443,  14557,   8905,   4746,   5118,   4686, -13883,  -9347,   -483,\n          -2007],\n        [ -3604,   8695, -12202,  -7692, -10057, -17020,  -3399,   9606,   1806,\n          16759],\n        [  4918,  -2320,   -673,  -3605,    617,   -294,   6129,    871,   -376,\n          -3447],\n        [  5956,  14615,  -6010,   9753,   -494,   1184,  -2173,  -6725, -10922,\n          -4711],\n        [  1508,  -6330,   4462, -14553,  -4909,   8049,  -3865,  -5751, -16088,\n           6620],\n        [-11351,    875,   6206,   8116,  -9703,  14540,  -1564, -13378,  10647,\n          -3789],\n        [  -291,  -6735,  -6778,  -3800,  21916,  -2213,  -5715,   7672,  -6387,\n         -13908],\n        [  4237,   -288,   4329,   8354,   4702,  -7422,  -2576,  -8562,   1718,\n            438],\n        [   550,  -3872,   8008,  -1269,  11398,   3838,  -8511,  -3956,   7989,\n         -17397],\n        [ -1239,  -2622,   1391,   5676,   6297,  14433,   1731,   1155,    936,\n           2776],\n        [  7639,  -8858,  14759,  -6216,  -4186,  10114,  -7184,  -9700,  -3938,\n           -151],\n        [ -1848,   9077,  11974,   6030,  -3904,   5513,  -4412,   4599,  -6845,\n          11112],\n        [ -5173,  -5597,  -1660,  -1170,   7152, -12661,  15254,  -9043, -20617,\n          -2211],\n        [  1804,  -3308,   2754,  -5723,   1457, -13130,   4433,  -3874,  -4483,\n          -8300],\n        [ 10500,   4337,  -1375,  -6909,   7133,  -5422,   4714,   8723,  -5167,\n           2936],\n        [ -4115,  -9744, -10414,   3743,   2627, -12667,  -9865,  15672,    614,\n           5499],\n        [   340,  -8527,  -3366,    502,   4673,  -5205,   1128,  -3468,  -2389,\n           6128],\n        [ -6909, -11299,  -5328,    365,  -9321, -18395,  -3280,   6203,  22058,\n         -10439],\n        [  7939,   8694,   8728,   -967,  -3798,    958,   -391,  -2165, -20377,\n          -3748],\n        [   314,  -7644,  -4099,  -5165,   4209,  23020,  12053,  -6408,   -515,\n         -12513],\n        [  6166,  -6818,  13291,   1448,  -3295,    327, -11778,  -8279,  -7101,\n           2186],\n        [ -9755, -10572,  -7383,  -6315,  -3016,  -7904,  12543,    701,   8368,\n          -2134],\n        [-11005,   -363,   4543,  -2320,  10420,  -6235,   1477,  -1121,  13180,\n          -6340],\n        [ -2793,  -9441,  -4427,  -6461,  -5657,  23188,  -1436, -12044,   5190,\n         -10463],\n        [  -365,  -3361,  -4168,   1286,   2336,  -4553,  19293,  -4780,  -4717,\n          -1898],\n        [ -8304,  22608, -12734,   6845,   1778,   4322,  -6623,   3997,   5264,\n          10516],\n        [ -8591,   1834,  -1071,  -2926,   1889,  -4602,   6088, -10530,   9265,\n          11169],\n        [  3736,   5580,   4047,  -8862,  11868,  -3314,  -2849, -12192,   4890,\n          -1290],\n        [ 13267,  -9703,    754,  -3117,  -9242,  -4558,  -6217,   5701,  -1515,\n            660],\n        [  6307,   6755,   7995,   4822,   2400,  -3558,  -6084,  -7074,   8356,\n          -3427],\n        [ -5693,  -5868,   1409,   6330, -13608,  -9126, -10349,  12636, -13804,\n          -5759],\n        [  3657,  -4336,   1393,  -8993,  -9699,  25907,   7794, -14686,  -6051,\n         -11359],\n        [ 25136,   -340,   -797,   3176, -12767,  -7752, -10484, -11775, -11009,\n            -84],\n        [ -5010,  -8018,   8020,  14343,  -7679,  -3995,   2158,  13463,  -9190,\n          -5927],\n        [  5054,  -5239,  -9077,  -3431,   1475,  21663,  10417, -13751,  -9362,\n          -8140],\n        [ -5420, -10791,  -5864,   1290,  -2697,  -1948,  15197,    873, -16122,\n           4393],\n        [ -6183,  -6642,  -1712,    -98,  -4028, -12635, -10370,    361,  15412,\n           5412],\n        [  3506,   -534,  -8278,  -9458,  -8578,  -5600,   -825,   5678,   3213,\n          10816],\n        [  1798,  -5398,  11856,  12150,  -8021, -10109,  -7489,  -2651,  -4001,\n          -3115],\n        [  7927,  -2130,  -7042,  -8623, -11462,  12783,  -7070,   1860,   -282,\n           4774]], device='cuda:0'))\n\tplain_text=HIDDEN\n\tptype=ptype.arithmetic\n)\u001b[0m\n\u001b[1;32m     22\u001b[0m           \u001b[0;31m#print(output_enc.size())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m           \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_enc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_plain_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-34-000e93dcf0c5>\u001b[0m in \u001b[0;36mrun_snn_test_2\u001b[0;34m(inputs=MPCTensor(\n\t_tensor=CUDALongTensor(tensor([[0, 0...0'))\n\tplain_text=HIDDEN\n\tptype=ptype.arithmetic\n), w1_enc=MPCTensor(\n\t_tensor=CUDALongTensor(tensor([[ 207...0'))\n\tplain_text=HIDDEN\n\tptype=ptype.arithmetic\n), w2_enc=MPCTensor(\n\t_tensor=CUDALongTensor(tensor([[-130...0'))\n\tplain_text=HIDDEN\n\tptype=ptype.arithmetic\n))\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mnew_syn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0msyn_enc\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0malpha\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0mmult_1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mnew_mem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbeta\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mmem_enc\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0msyn_enc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mrst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m        \u001b[0;36mnew_mem\u001b[0m \u001b[0;34m= MPCTensor(\n\t_tensor=CUDALongTensor(tensor([      0,       0,       0,       0,       0,  118812, -162876, -102873,\n         -31696,    2349,       0,   -1616,  -18595,       0,   -8746,       0,\n         -49162,  -13278,   -1588,   -9070,  -24486,   -8125,  -52284,       0,\n           3721,    1877,  -54448,       0,       0,   24331,       0,       0,\n              0,       0,       0,       0,   29970,  -16902,       0,   -8639,\n              0,       0,       0,       0,       0,       0,       0,       0,\n         -33716,       0,       0,       0,       0,       0,       0,       0,\n              0,       0,       0,       0,       0,       0,       0,       0,\n              0,       0,       0,       0,       0,       0,       0,       0,\n              0,       0,       0,       0,       0,       0,       0,       0,\n              0,       0,       0,       0,       0,       0,       0,       0,\n              0,       0,       0,       0,       0,       0,       0,       0,\n              0,       0,       0,       0], device='cuda:0'))\n\tplain_text=HIDDEN\n\tptype=ptype.arithmetic\n)\u001b[0m\u001b[0;34m\n        \u001b[0m\u001b[0;36mglobal\u001b[0m \u001b[0;36mbeta\u001b[0m \u001b[0;34m= 0.9048374180359595\u001b[0m\u001b[0;34m\n        \u001b[0m\u001b[0;36mmem_enc\u001b[0m \u001b[0;34m= MPCTensor(\n\t_tensor=CUDALongTensor(tensor([      0,       0,       0,       0,       0,  118812, -162876, -102873,\n         -31696,    2349,       0,   -1616,  -18595,       0,   -8746,       0,\n         -49162,  -13278,   -1588,   -9070,  -24486,   -8125,  -52284,       0,\n           3721,    1877,  -54448,       0,       0,   24331,       0,       0,\n              0,       0,       0,       0,   29970,  -16902,       0,   -8639,\n              0,       0,       0,       0,       0,       0,       0,       0,\n         -33716,       0,       0,       0,       0,       0,       0,       0,\n              0,       0,       0,       0,       0,       0,       0,       0,\n              0,       0,       0,       0,       0,       0,       0,       0,\n              0,       0,       0,       0,       0,       0,       0,       0,\n              0,       0,       0,       0,       0,       0,       0,       0,\n              0,       0,       0,       0,       0,       0,       0,       0,\n              0,       0,       0,       0], device='cuda:0'))\n\tplain_text=HIDDEN\n\tptype=ptype.arithmetic\n)\u001b[0m\u001b[0;34m\n        \u001b[0m\u001b[0;36msyn_enc\u001b[0m \u001b[0;34m= MPCTensor(\n\t_tensor=CUDALongTensor(tensor([     0,      0,      0,      0, -22645,  30514, -82443,    769,  -1701,\n          1472,      0,    258,  -4244,      0,    747,      0,  -6377,  -3641,\n            30,   -216,    995,   1241,  -4732,      0,    688,  -3645,  -3515,\n             0,      0,  -1669,      0,      0,      0,      0,      0,      0,\n          -633,   1990,      0,    163,      0,      0,      0,      0,      0,\n             0,      0,      0,    740,      0,      0,      0,      0,      0,\n             0,      0,      0,      0,      0,      0,      0,      0,      0,\n             0,      0,      0,      0,      0,      0,      0,      0,      0,\n             0,      0,      0,      0,      0,      0,      0,      0,      0,\n             0,      0,      0,      0,      0,      0,      0,      0,      0,\n             0,      0,      0,      0,      0,      0,      0,      0,      0,\n             0], device='cuda:0'))\n\tplain_text=HIDDEN\n\tptype=ptype.arithmetic\n)\u001b[0m\u001b[0;34m\n        \u001b[0m\u001b[0;36mrst\u001b[0m \u001b[0;34m= MPCTensor(\n\t_tensor=CUDALongTensor(tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0], device='cuda:0'))\n\tplain_text=HIDDEN\n\tptype=ptype.arithmetic\n)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mmem_rec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmem_enc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/crypten/cryptensor.py\u001b[0m in \u001b[0;36m__mul__\u001b[0;34m(self=MPCTensor(\n\t_tensor=CUDALongTensor(tensor([     ...0'))\n\tplain_text=HIDDEN\n\tptype=ptype.arithmetic\n), tensor=MPCTensor(\n\t_tensor=CUDALongTensor(tensor([1, 1,...0'))\n\tplain_text=HIDDEN\n\tptype=ptype.arithmetic\n))\u001b[0m\n\u001b[1;32m    499\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__mul__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m         \u001b[0;34m\"\"\"Element-wise multiply with a tensor.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 501\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m        \u001b[0;36mself.mul\u001b[0m \u001b[0;34m= <function CrypTensor._get_autograd_forward_function.<locals>.autograd_forward at 0x7fa05129bcb0>\u001b[0m\u001b[0;34m\n        \u001b[0m\u001b[0;36mtensor\u001b[0m \u001b[0;34m= MPCTensor(\n\t_tensor=CUDALongTensor(tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1], device='cuda:0'))\n\tplain_text=HIDDEN\n\tptype=ptype.arithmetic\n)\u001b[0m\n\u001b[1;32m    502\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m     \u001b[0m__rmul__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m__mul__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/crypten/cryptensor.py\u001b[0m in \u001b[0;36mautograd_forward\u001b[0;34m(*args=(MPCTensor(\n\t_tensor=CUDALongTensor(tensor([1, 1,...0'))\n\tplain_text=HIDDEN\n\tptype=ptype.arithmetic\n),), **kwargs={})\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m        \u001b[0;36mglobal\u001b[0m \u001b[0;36mself.__getattribute__\u001b[0m \u001b[0;34m= \u001b[0;36mundefined\u001b[0m\u001b[0m\u001b[0;34m\n        \u001b[0m\u001b[0;36mglobal\u001b[0m \u001b[0;36mname\u001b[0m \u001b[0;34m= \u001b[0;36mundefined\u001b[0m\u001b[0m\u001b[0;34m\n        \u001b[0m\u001b[0;36margs\u001b[0m \u001b[0;34m= (MPCTensor(\n\t_tensor=CUDALongTensor(tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1], device='cuda:0'))\n\tplain_text=HIDDEN\n\tptype=ptype.arithmetic\n),)\u001b[0m\u001b[0;34m\n        \u001b[0m\u001b[0;36mkwargs\u001b[0m \u001b[0;34m= {}\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m                 \u001b[0;31m# in-place functions are not supported when requires_grad:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/crypten/mpc/mpc.py\u001b[0m in \u001b[0;36mbinary_wrapper_function\u001b[0;34m(self=MPCTensor(\n\t_tensor=CUDALongTensor(tensor([     ...0'))\n\tplain_text=HIDDEN\n\tptype=ptype.arithmetic\n), value=MPCTensor(\n\t_tensor=CUDALongTensor(tensor([1, 1,...0'))\n\tplain_text=HIDDEN\n\tptype=ptype.arithmetic\n), *args=(), **kwargs={})\u001b[0m\n\u001b[1;32m    334\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_add_binary_passthrough_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbinary_wrapper_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshallow_copy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m        \u001b[0;36mresult\u001b[0m \u001b[0;34m= \u001b[0;36mundefined\u001b[0m\u001b[0m\u001b[0;34m\n        \u001b[0m\u001b[0;36mself.shallow_copy\u001b[0m \u001b[0;34m= <bound method MPCTensor.shallow_copy of MPCTensor(\n\t_tensor=CUDALongTensor(tensor([      0,       0,       0,       0,  -22645,  138018, -229818,  -92313,\n         -30380,    3597,       0,   -1204,  -21069,       0,   -7166,       0,\n         -50860,  -15655,   -1406,   -8422,  -21160,   -6110,  -52040,       0,\n           4054,   -1947,  -52781,       0,       0,   20346,       0,       0,\n              0,       0,       0,       0,   26484,  -13303,       0,   -7653,\n              0,       0,       0,       0,       0,       0,       0,       0,\n         -29767,       0,       0,       0,       0,       0,       0,       0,\n              0,       0,       0,       0,       0,       0,       0,       0,\n              0,       0,       0,       0,       0,       0,       0,       0,\n              0,       0,       0,       0,       0,       0,       0,       0,\n              0,       0,       0,       0,       0,       0,       0,       0,\n              0,       0,       0,       0,       0,       0,       0,       0,\n              0,       0,       0,       0], device='cuda:0'))\n\tplain_text=HIDDEN\n\tptype=ptype.arithmetic\n)>\u001b[0m\n\u001b[1;32m    337\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMPCTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/crypten/mpc/mpc.py\u001b[0m in \u001b[0;36mshallow_copy\u001b[0;34m(self=MPCTensor(\n\t_tensor=CUDALongTensor(tensor([     ...0'))\n\tplain_text=HIDDEN\n\tptype=ptype.arithmetic\n))\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;34m\"\"\"Create a shallow copy of the input tensor.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;31m# TODO: Rename this to __copy__()?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMPCTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m        \u001b[0;36mresult\u001b[0m \u001b[0;34m= \u001b[0;36mundefined\u001b[0m\u001b[0m\u001b[0;34m\n        \u001b[0m\u001b[0;36mglobal\u001b[0m \u001b[0;36mMPCTensor\u001b[0m \u001b[0;34m= <class 'crypten.mpc.mpc.MPCTensor'>\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mptype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mptype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/crypten/mpc/mpc.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self=<class 'crypten.mpc.mpc.MPCTensor'> instance, tensor=[], ptype=<ptype.arithmetic: 0>, device=None, *args=(), **kwargs={})\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# call CrypTensor constructor:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m        \u001b[0;36mglobal\u001b[0m \u001b[0;36msuper.__init__\u001b[0m \u001b[0;34m= \u001b[0;36mundefined\u001b[0m\u001b[0m\u001b[0;34m\n        \u001b[0m\u001b[0;36mrequires_grad\u001b[0m \u001b[0;34m= False\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"device\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/crypten/cryptensor.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self=<class 'crypten.mpc.mpc.MPCTensor'> instance, requires_grad=False)\u001b[0m\n\u001b[1;32m    197\u001b[0m         \"\"\"\n\u001b[1;32m    198\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequires_grad\u001b[0m  \u001b[0;31m# whether tensors needs gradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m        \u001b[0;36mself._reset_gradients\u001b[0m \u001b[0;34m= \u001b[0;36mundefined\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__new__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/crypten/debug/__init__.py\u001b[0m in \u001b[0;36mvalidate_attribute\u001b[0;34m(self=<class 'crypten.mpc.mpc.MPCTensor'> instance, name='_reset_gradients')\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mfunction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidation_mode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m        \u001b[0;36mglobal\u001b[0m \u001b[0;36mcfg.debug.validation_mode\u001b[0m \u001b[0;34m= False\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/crypten/config/config.py\u001b[0m in \u001b[0;36m__getattribute__\u001b[0;34m(self=<crypten.config.config.CrypTenConfig object>, name='debug')\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0mkeys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m        \u001b[0;36mresult\u001b[0m \u001b[0;34m= \u001b[0;36mundefined\u001b[0m\u001b[0m\u001b[0;34m\n        \u001b[0m\u001b[0;36mglobal\u001b[0m \u001b[0;36mgetattr\u001b[0m \u001b[0;34m= \u001b[0;36mundefined\u001b[0m\u001b[0m\u001b[0;34m\n        \u001b[0m\u001b[0;36mself.config\u001b[0m \u001b[0;34m= {'communicator': {'verbose': False}, 'debug': {'debug_mode': False, 'validation_mode': False}, 'encoder': {'precision_bits': 16}, 'functions': {'max_method': 'log_reduction', 'exp_iterations': 8, 'reciprocal_method': 'NR', 'reciprocal_nr_iters': 10, 'reciprocal_log_iters': 1, 'reciprocal_all_pos': False, 'reciprocal_initial': None, 'sqrt_nr_iters': 3, 'sqrt_nr_initial': None, 'sigmoid_tanh_method': 'reciprocal', 'sigmoid_tanh_terms': 32, 'log_iterations': 2, 'log_exp_iterations': 8, 'log_order': 8, 'trig_iterations': 10, 'erf_iterations': 8}, 'mpc': {'active_security': False, 'provider': 'TFP', 'protocol': 'beaver'}}\u001b[0m\u001b[0;34m\n        \u001b[0m\u001b[0;36mkeys\u001b[0m \u001b[0;34m= ['debug']\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self={'communicator': {'verbose': False}, 'debug': {'... False, 'provider': 'TFP', 'protocol': 'beaver'}}, key='debug')\u001b[0m\n\u001b[1;32m    349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 351\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_DEFAULT_MARKER_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m        \u001b[0;36mself._get_impl\u001b[0m \u001b[0;34m= <bound method DictConfig._get_impl of {'communicator': {'verbose': False}, 'debug': {'debug_mode': False, 'validation_mode': False}, 'encoder': {'precision_bits': 16}, 'functions': {'max_method': 'log_reduction', 'exp_iterations': 8, 'reciprocal_method': 'NR', 'reciprocal_nr_iters': 10, 'reciprocal_log_iters': 1, 'reciprocal_all_pos': False, 'reciprocal_initial': None, 'sqrt_nr_iters': 3, 'sqrt_nr_initial': None, 'sigmoid_tanh_method': 'reciprocal', 'sigmoid_tanh_terms': 32, 'log_iterations': 2, 'log_exp_iterations': 8, 'log_order': 8, 'trig_iterations': 10, 'erf_iterations': 8}, 'mpc': {'active_security': False, 'provider': 'TFP', 'protocol': 'beaver'}}>\u001b[0m\u001b[0;34m\n        \u001b[0m\u001b[0;36mkey\u001b[0m \u001b[0;34m= 'debug'\u001b[0m\u001b[0;34m\n        \u001b[0m\u001b[0;36mglobal\u001b[0m \u001b[0;36mdefault_value\u001b[0m \u001b[0;34m= \u001b[0;36mundefined\u001b[0m\u001b[0m\u001b[0;34m\n        \u001b[0m\u001b[0;36mglobal\u001b[0m \u001b[0;36m_DEFAULT_MARKER_\u001b[0m \u001b[0;34m= _DEFAULT_MARKER_\u001b[0m\n\u001b[1;32m    352\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mConfigKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m             self._format_and_raise(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py\u001b[0m in \u001b[0;36m_get_impl\u001b[0;34m(self={'communicator': {'verbose': False}, 'debug': {'... False, 'provider': 'TFP', 'protocol': 'beaver'}}, key='debug', default_value=_DEFAULT_MARKER_)\u001b[0m\n\u001b[1;32m    444\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m         return self._resolve_with_default(\n\u001b[0;32m--> 446\u001b[0;31m             \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdefault_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m        \u001b[0;36mkey\u001b[0m \u001b[0;34m= 'debug'\u001b[0m\u001b[0;34m\n        \u001b[0m\u001b[0;36mglobal\u001b[0m \u001b[0;36mvalue\u001b[0m \u001b[0;34m= \u001b[0;36mundefined\u001b[0m\u001b[0m\u001b[0;34m\n        \u001b[0m\u001b[0;36mnode\u001b[0m \u001b[0;34m= {'debug_mode': False, 'validation_mode': False}\u001b[0m\u001b[0;34m\n        \u001b[0m\u001b[0;36mdefault_value\u001b[0m \u001b[0;34m= _DEFAULT_MARKER_\u001b[0m\n\u001b[1;32m    447\u001b[0m         )\n\u001b[1;32m    448\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/omegaconf/basecontainer.py\u001b[0m in \u001b[0;36m_resolve_with_default\u001b[0;34m(self={'communicator': {'verbose': False}, 'debug': {'... False, 'provider': 'TFP', 'protocol': 'beaver'}}, key='debug', value={'debug_mode': False, 'validation_mode': False}, default_value=_DEFAULT_MARKER_)\u001b[0m\n\u001b[1;32m     70\u001b[0m         )\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_get_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresolved_node\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m        \u001b[0;36mglobal\u001b[0m \u001b[0;36m_get_value\u001b[0m \u001b[0;34m= <function _get_value at 0x7fa0bccd8200>\u001b[0m\u001b[0;34m\n        \u001b[0m\u001b[0;36mresolved_node\u001b[0m \u001b[0;34m= {'debug_mode': False, 'validation_mode': False}\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__str__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/omegaconf/_utils.py\u001b[0m in \u001b[0;36m_get_value\u001b[0;34m(value={'debug_mode': False, 'validation_mode': False})\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_get_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 675\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mContainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m        \u001b[0;36mglobal\u001b[0m \u001b[0;36mbase\u001b[0m \u001b[0;34m= \u001b[0;36mundefined\u001b[0m\u001b[0m\u001b[0;34m\n        \u001b[0m\u001b[0;36mContainer\u001b[0m \u001b[0;34m= \u001b[0;36mundefined\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mnodes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mValueNode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_handle_fromlist\u001b[0;34m(module=<module 'omegaconf.base' from '/usr/local/lib/python3.7/dist-packages/omegaconf/base.py'>, fromlist=('Container',), import_=<built-in function __import__>, recursive=False)\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "SNN_MNIST_v2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c1af491a50af444e990136a7d433e299": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_ef169c8205ca43f7a980740aaeaec91f",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_88f360063c054ce0a18b52756a7f043f",
              "IPY_MODEL_50a306e6c6f54ad6b3f84720c87b4acb",
              "IPY_MODEL_0938757ea56245578e57596f302c987c"
            ]
          }
        },
        "ef169c8205ca43f7a980740aaeaec91f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "88f360063c054ce0a18b52756a7f043f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_f8749b243d1444098b07a3808077d330",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_310506f8301445f5a210b0f424c6edf5"
          }
        },
        "50a306e6c6f54ad6b3f84720c87b4acb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_b535410024c44ce2b8a2ece3d438ce1a",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 26421880,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 26421880,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1efe7c0083c8404fae7e8cfa7c2bc91a"
          }
        },
        "0938757ea56245578e57596f302c987c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_77152e81d840428fbced1f06201e31d3",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 26422272/? [00:00&lt;00:00, 70034959.14it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ab84025db8dc4a3782ef17d08837db69"
          }
        },
        "f8749b243d1444098b07a3808077d330": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "310506f8301445f5a210b0f424c6edf5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b535410024c44ce2b8a2ece3d438ce1a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "1efe7c0083c8404fae7e8cfa7c2bc91a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "77152e81d840428fbced1f06201e31d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ab84025db8dc4a3782ef17d08837db69": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8ec76c02c4c541ad926284b242cb1f53": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_c9abdb45182a4e4b80863063184d8af8",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_0b527fd41a8c4616abc45b7c74e8f254",
              "IPY_MODEL_78000ba7458d48feb0ae10c14538c254",
              "IPY_MODEL_d2df82e0e9214eb8b9c04638c4646056"
            ]
          }
        },
        "c9abdb45182a4e4b80863063184d8af8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0b527fd41a8c4616abc45b7c74e8f254": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_0199c48fa1664b0cb6fa2b26f98b69a0",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f2e1bf8a83e44c8d99c3735352b8fbb2"
          }
        },
        "78000ba7458d48feb0ae10c14538c254": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_4e778bcfa5864afea26d47d9ca5bf04e",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 29515,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 29515,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_945ce3154bd34af9960dd45497a24fbf"
          }
        },
        "d2df82e0e9214eb8b9c04638c4646056": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_34fc6d19dc074279999661c6b4591550",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 29696/? [00:00&lt;00:00, 9304.58it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_97c352b2fc9e4c639b8d78ffe9562be9"
          }
        },
        "0199c48fa1664b0cb6fa2b26f98b69a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f2e1bf8a83e44c8d99c3735352b8fbb2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4e778bcfa5864afea26d47d9ca5bf04e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "945ce3154bd34af9960dd45497a24fbf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "34fc6d19dc074279999661c6b4591550": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "97c352b2fc9e4c639b8d78ffe9562be9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "451bfe0cecaa4a6e8ad23703c822da1e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_fc4a095441244221abed03f62caae2e9",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_1916c590ac4b47a6919da7f1a7ccfafe",
              "IPY_MODEL_ef849420fd3f41dbaf590b3b896972a6",
              "IPY_MODEL_eb20a00931ac44c09cfd8c5d397acc98"
            ]
          }
        },
        "fc4a095441244221abed03f62caae2e9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1916c590ac4b47a6919da7f1a7ccfafe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_32d71e3b76d549acbcb133052e3b6be6",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c70689e31d2f46b5a919593f40a241d4"
          }
        },
        "ef849420fd3f41dbaf590b3b896972a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_8eac527f5aa046cda47a015defbe013f",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 4422102,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 4422102,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_53fe5ba4dcaf4e049bacb00c45d81755"
          }
        },
        "eb20a00931ac44c09cfd8c5d397acc98": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_06c88ed49f7c4b93a2cdde8276b6bfb5",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 4422656/? [00:00&lt;00:00, 6163079.34it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d56c6b41342443f0afc7b51958da6517"
          }
        },
        "32d71e3b76d549acbcb133052e3b6be6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c70689e31d2f46b5a919593f40a241d4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8eac527f5aa046cda47a015defbe013f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "53fe5ba4dcaf4e049bacb00c45d81755": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "06c88ed49f7c4b93a2cdde8276b6bfb5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d56c6b41342443f0afc7b51958da6517": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d0d3323e671b476ea508b4d222e01525": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_050ce66cf5a04cf19fbbd36a1896ef60",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_b0aa144ec4b34c6782fa0b6e1899a3a9",
              "IPY_MODEL_4f3385bf7971455a8ac06da2fc2b34b2",
              "IPY_MODEL_b868784422434bdda11cff15f6e8488f"
            ]
          }
        },
        "050ce66cf5a04cf19fbbd36a1896ef60": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b0aa144ec4b34c6782fa0b6e1899a3a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_d5640285a5c94de8ad2939efa862b194",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_17a69ec2609f467996c2ed322fda5f3e"
          }
        },
        "4f3385bf7971455a8ac06da2fc2b34b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_2782948a188e47d9a514ec8b543c6913",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 5148,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 5148,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f577aa0130bb476d85933223c36c40ff"
          }
        },
        "b868784422434bdda11cff15f6e8488f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_8d85e4992a9c4e218f481b93555fe125",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 6144/? [00:00&lt;00:00, 62610.53it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6c2fe6e05c1c4433a3c7b7904e18606f"
          }
        },
        "d5640285a5c94de8ad2939efa862b194": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "17a69ec2609f467996c2ed322fda5f3e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2782948a188e47d9a514ec8b543c6913": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f577aa0130bb476d85933223c36c40ff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8d85e4992a9c4e218f481b93555fe125": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6c2fe6e05c1c4433a3c7b7904e18606f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}